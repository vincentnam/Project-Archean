{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import webrtcvad\n",
    "import wave\n",
    "import speech_recognition as sr           # package SpeechRecognition\n",
    "import datetime\n",
    "import strsimpy\n",
    "import xml.etree.ElementTree as ET\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import os\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import spleeter\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading signal\n",
    "def load_signal(file, samplerate=16000):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        string file : chemin du fichier audio\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "    Returns:\n",
    "        bytes binary_data : signal sous forme binaire\n",
    "        numpy.ndarray signal : signal\n",
    "        int fe : fréquence d'échantillonage\n",
    "    \"\"\"\n",
    "    \n",
    "    signal, fe = librosa.load(file, sr=samplerate, mono=True)\n",
    "    \n",
    "    w = wave.open(file, \"rb\")\n",
    "    binary_data = w.readframes(w.getnframes())\n",
    "    \n",
    "    return binary_data, signal, fe\n",
    "\n",
    "# speech detection using webrtcvad\n",
    "def get_speech_intervals(bin_signal, samplerate=16000, agg=3, smooth='rolling_mean', affichage=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        bytes bin_signal : signal sous forme binaire\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "        int agg : paramètre de la fonction vad (1/2/3)\n",
    "        str smooth : méthode de lissage à utiliser\n",
    "        bool affichage : booléen : afficher ou non un graphique représentant le résultat\n",
    "    Returns:\n",
    "        Series speech : signal booléen indiquant où se trouve la parole dans le signal d'entré\n",
    "    \"\"\"\n",
    "    \n",
    "    if smooth is None : smooth = 'rolling_mean'\n",
    "    \n",
    "    recomposed_signal = np.frombuffer(bin_signal, dtype=np.int16)\n",
    "\n",
    "    vad = webrtcvad.Vad()\n",
    "    vad.set_mode(mode=agg)\n",
    "\n",
    "    millisec = 10\n",
    "    fenetre = int(samplerate * millisec * 2 / 1000)\n",
    "\n",
    "    fen_speech = [vad.is_speech(bin_signal[m:m+fenetre], samplerate) \n",
    "                  for m in range(0,len(bin_signal),fenetre)\n",
    "                 if len(bin_signal[m:m+fenetre]) == fenetre]\n",
    "\n",
    "    widened_speech = np.array([[s] * int(fenetre/2) for s in fen_speech]).ravel()\n",
    "    speech = np.full(len(recomposed_signal), widened_speech[-1])\n",
    "    speech[0:len(widened_speech)] = widened_speech\n",
    "\n",
    "    # adoucissement\n",
    "    if smooth == 'closing':\n",
    "        smoothed_speech = ndimage.binary_closing(input=speech, structure=np.array([1]*10000)).astype(bool)\n",
    "    elif smooth == 'rolling_mean' :\n",
    "        smoothed_speech = pd.Series(speech).rolling(window=int(samplerate/5), min_periods=2, center=True).mean() > 0.2\n",
    "    \n",
    "    if affichage :\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(np.arange(len(smoothed_speech))/samplerate, smoothed_speech*max(recomposed_signal), label=\"speech\")\n",
    "        plt.plot(np.arange(len(recomposed_signal))/samplerate, recomposed_signal, label=\"signal\")\n",
    "        plt.xlabel(\"sec\")\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        \n",
    "    return pd.Series(smoothed_speech)\n",
    "\n",
    "# SNR using speech detection (bad on recomposed signal from binary)\n",
    "def get_SNR(signal, speech, samplerate=16000):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        numpy.ndarray signal : signal audio\n",
    "        Series speech : signal booléen indiquant où se trouve la parole dans le signal d'entré\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "    Returns:\n",
    "        float SNR : donne le rapport de l'energie du signal parlé sur l'énergie totale\n",
    "    \"\"\"\n",
    "    s = pd.Series(signal)\n",
    "    energy_total = (s**2).rolling(window=samplerate, min_periods=2, center=True).mean()\n",
    "\n",
    "    energy_speech = (s[speech]**2).rolling(window=samplerate, min_periods=2, center=True).mean()\n",
    "    energy_notSpeech = (s[~speech]**2).rolling(window=samplerate, min_periods=2, center=True).mean()\n",
    "\n",
    "    SNR = energy_speech.mean() / (energy_speech.mean() + energy_notSpeech.mean())\n",
    "    \n",
    "    return SNR\n",
    "\n",
    "# a partir des timestamps des sous-titres, creer un signal booléen représantant la parole\n",
    "def get_filtre_paroles(ts, dilation_ms=100):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        list ts : liste des phrases, timestamps et durée des fichier xml\n",
    "        int dilation_ms : ajout de quelques millisecondes avant et après les timestamps\n",
    "    Returns:\n",
    "        numpy.ndarray paroles : signal booléen indiquant où se trouve la parole dans le signal d'entré\n",
    "    \"\"\"\n",
    "    # creation du filtre paroles\n",
    "    ts_longueur = int(np.sum(ts[-1][0])*1000)\n",
    "    paroles = np.zeros(ts_longueur)\n",
    "    for i,v in ts :\n",
    "        a,d = int(i[0]*1000), int(i[1]*1000)\n",
    "        paroles[a:a+d] = 1\n",
    "    \n",
    "    paroles = ndimage.binary_dilation(input=paroles, structure=np.array([1]*dilation_ms)).astype(bool)\n",
    "    return paroles\n",
    "\n",
    "# filtre à la main pour estimer quand les sous-titres commencent\n",
    "def suppression_bords(m, t):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        m : array d'une certaine longeur représentant les décallages possibles des sous-titres\n",
    "        t : decallage à la main de t pas\n",
    "    Returns:\n",
    "        Series : représente un score pour chaque décallage\n",
    "    \"\"\"\n",
    "    \n",
    "    droite = np.zeros([len(m)+2*t])\n",
    "    droite[0:t], droite[-t:], droite[t:-t] = 4,4,1\n",
    "    droite = droite / droite.sum()\n",
    "    m2 = pd.Series(droite).rolling(window=2*t+1, center=True).mean()[t:-t]\n",
    "    m2.index = m.index\n",
    "    return m2/2\n",
    "\n",
    "# a partir d'un signal audio et d'un signal booléen représentant la parole,\n",
    "# calcul d'aires du signal parlé et non parlé en fonction d'un décallage des sous-titres\n",
    "# pour chaque décallage retourne une métrique basé sur ces calculs d'aires\n",
    "def get_decalage(paroles, signal, pas=100, affichage=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        paroles : signal booléen indiquant parlé / non parlé dans les sous-titres\n",
    "        numpy.ndarray signal : signal\n",
    "        int pas : on décalle au fur et à mésure les sous titres de pas millisecondes\n",
    "        bool affichage : afficher ou non un graphique représentant le résultat\n",
    "    Returns:\n",
    "        Series metric : représente un score pour chaque décallage\n",
    "    \"\"\"\n",
    "    \n",
    "    intervals = range(0, len(signal) - len(paroles), pas)\n",
    "    aires_0 = np.zeros([len(intervals)])\n",
    "    aires_1 = np.zeros([len(intervals)])\n",
    "\n",
    "    for i,debut in enumerate(intervals):\n",
    "        aire_0 = signal[debut:debut+len(paroles)] * (paroles == 0)\n",
    "        aire_1 = signal[debut:debut+len(paroles)] * (paroles == 1)\n",
    "        aires_0[i] = abs(aire_0).sum()\n",
    "        aires_1[i] = abs(aire_1).sum()\n",
    "    \n",
    "    if len(aires_0) > 0 : decalage = np.argmin(aires_0 - aires_1)*pas\n",
    "    else : decalage = 0\n",
    "    \n",
    "    aire_0 = signal[decalage:decalage+len(paroles)] * (paroles == 0)[:len(signal)]\n",
    "    aire_1 = signal[decalage:decalage+len(paroles)] * (paroles == 1)[:len(signal)]\n",
    "\n",
    "    resultat = np.zeros(len(signal))\n",
    "    resultat[decalage : decalage + len(paroles)] = paroles[0:len(paroles)]\n",
    "    \n",
    "    metric = (aires_0-aires_1)\n",
    "    metric = metric / abs(metric.sum())\n",
    "    metric = pd.Series(metric, index=np.arange(0,len(metric))*pas)\n",
    "    if len(metric) == 0 : metric = pd.Series(np.zeros(1))\n",
    "    \n",
    "    if affichage:\n",
    "        plt.figure(figsize=(14,2))\n",
    "        plt.title(str(decalage))\n",
    "        plt.plot(resultat, label=\"paroles\")\n",
    "        plt.plot(signal)\n",
    "        plt.legend(loc=2)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(14,2))\n",
    "        plt.plot(metric, label=\"aires\")\n",
    "        plt.legend(loc=2)\n",
    "        plt.show()\n",
    "        \n",
    "    return metric\n",
    "\n",
    "# return text sentence from audio\n",
    "def get_recognition(audiofile, adjust_noise=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str audiofile : chemin du fichier audio\n",
    "        bool adjust_noise : utiliser ou non adjust_for_ambient_noise\n",
    "    Returns:\n",
    "        float conf : indice de confiance de google pour ce qui est reconnu\n",
    "        str text : texte reconnu\n",
    "    \"\"\"\n",
    "    r  = sr.Recognizer()\n",
    "    demo = sr.AudioFile(audiofile)\n",
    "    with demo as source:\n",
    "        if adjust_noise: r.adjust_for_ambient_noise(source)\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        recon = r.recognize_google(audio, language='fr-FR', show_all=True)\n",
    "        if len(recon) == 0 :\n",
    "            text = ''\n",
    "            conf = 0\n",
    "        else :\n",
    "            text = recon['alternative'][0]['transcript']\n",
    "            conf = recon['alternative'][0]['confidence']\n",
    "    except LookupError:\n",
    "        print(\"LookupError : Could not understand audio\")\n",
    "        text = ''\n",
    "        conf = 0\n",
    "    \n",
    "    return conf, text\n",
    "\n",
    "# return sentences, when they start in seconds and their length in seconds\n",
    "def get_timed_sentences(xmlfile):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str xmlfile : chemin du fichier xml\n",
    "    Returns:\n",
    "        list sentences : liste de valeurs (debut (sec), duree (sec), phrase (texte))\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "\n",
    "    sentences = [([e.attrib['value'] for e in sent if e.tag=='time'],\n",
    "                  \" \".join([w.text.strip() for w in sent if w.text is not None]))\n",
    "                 for sent in tree.getroot()]\n",
    "\n",
    "    t0 = datetime.datetime.strptime(sentences[0][0][0], '%H:%M:%S,%f')\n",
    "    for i,(t,s) in enumerate(sentences):\n",
    "        t1 = datetime.datetime.strptime(t[0], '%H:%M:%S,%f')\n",
    "        t2 = datetime.datetime.strptime(t[1], '%H:%M:%S,%f')\n",
    "        sentences[i] = (((t1 - t0).total_seconds() , (t2 - t1).total_seconds()), s)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# split the speech signal (0/1) into several signal (1)\n",
    "def split_speech(speech, samplerate=16000, sec_before=1, sec_after=0.5):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        Series speech : chemin du fichier xml\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "        float sec_before : ajouter x secondes avant chaque début de parlé\n",
    "        float sec_after : ajouter x secondes après chaque fin de parlé\n",
    "    Returns:\n",
    "        DataFrame df_intervals : dataframe qui pour chaque ligne contient 5 colonnes:\n",
    "            add_noise : temps de bruit à ajouter (ou non)\n",
    "            start_signal : début de la séquence parlé (avec ajout ou non de temps avant)\n",
    "            start_speech : début de la parole\n",
    "            end_speech : fin de la parole\n",
    "            end_signal : fin de la séquence parlé (avec ajout ou non de temps après)\n",
    "    \"\"\"\n",
    "    f = np.array([1,-1])\n",
    "    r = np.convolve(speech, f, 'same')\n",
    "    starts, ends = np.where(r == 1)[0].tolist(), np.where(r == -1)[0].tolist()\n",
    "    if len(ends) == 0 : ends = [len(speech)]\n",
    "    if ends[0] < starts[0] : starts = [0] + starts\n",
    "    if len(starts) > len(ends) : ends = ends + [len(speech)]\n",
    "\n",
    "    speech_intervals = np.array([(0,\n",
    "                                  starts[i] - int(samplerate*sec_before),\n",
    "                                  starts[i],\n",
    "                                  ends[i], ends[i] + int(samplerate*sec_after))\n",
    "                                 for i in range(len(starts))])\n",
    "\n",
    "    diffs = [(v[2] - speech_intervals[i][-2] - int(samplerate*sec_before)) for i,v in enumerate(speech_intervals[1:])]\n",
    "    diffs = [0 if speech_intervals[0,1] > 0 else -speech_intervals[0,1]] + [0 if v > 0 else abs(v) for v in diffs]\n",
    "\n",
    "    speech_intervals[:,0] = diffs\n",
    "    speech_intervals[:,1] += speech_intervals[:,0]\n",
    "    speech_intervals[speech_intervals < 0] = 0\n",
    "\n",
    "    df_intervals = pd.DataFrame(speech_intervals,\n",
    "                                columns=['add_noise','start_signal','start_speech','end_speech','end_signal'])\n",
    "    return df_intervals\n",
    "\n",
    "# return bool serie where the background noise is\n",
    "def get_background(signal, speech, samplerate, affichage=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        numpy.ndarray signal : signal audio\n",
    "        Series speech : signal booléen\n",
    "        samplerate : fréquence d'échantilonnage\n",
    "        affichage : affichage ou non d'un graphique représentant le résultat\n",
    "    Returns:\n",
    "        Series background : signal booléen indiquant ou est le signal de fond\n",
    "    \"\"\"\n",
    "    background = speech.rolling(window=int(samplerate/4), center=True).mean() < 1/10\n",
    "    \n",
    "    if affichage:\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(signal, label=\"signal\")\n",
    "        plt.plot(speech * max(signal) / 2, label=\"speech\")\n",
    "        plt.plot(background * max(signal), label=\"noise\")\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        \n",
    "    return background\n",
    "\n",
    "# make a noise from signal and background\n",
    "def make_noise(signal, background, samplerate, lenght=3):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        numpy.ndarray signal : signal audio\n",
    "        Series background : signal booléen\n",
    "        int samplerate : fréquence d'échantilonnage\n",
    "        int lenght : temps en secondes du bruit voulu\n",
    "    Returns:\n",
    "        numpy.ndarray noise : signal du bruit\n",
    "    \"\"\"\n",
    "    if background.sum() < samplerate/10 :\n",
    "        noise = signal[:len(signal) - len(signal)%samplerate].reshape(samplerate, int(len(signal)/samplerate)).mean(axis=1)\n",
    "    else :\n",
    "        noise = np.array([signal[background][m:m+int(lenght*samplerate)]\n",
    "                          for m in range(len(signal[background][::int(lenght*samplerate)]))]).mean(axis=0)\n",
    "        if len(noise) < lenght*samplerate:\n",
    "            noise = np.array(list(noise) * (int(lenght*samplerate / len(noise))+1))[0:int(lenght*samplerate)]\n",
    "    return noise\n",
    "\n",
    "# recherche la valeur en dB de coupure entre silence et bruit\n",
    "# en fonction d'un nombre de coupure du signal voulu\n",
    "def get_dB_treshold(dB, n_win_wanted=11, affichage=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        numpy.ndarray dB : array des dB d'un signal\n",
    "        int n_win_wanted : nombre de séparation de dB recherché\n",
    "        bool affichage : représenter ou non les résultats\n",
    "    Returns:\n",
    "        int treshold : valeur du dB qui sépare silence et non silence\n",
    "    \"\"\"\n",
    "    dBs = list(range(50,100))\n",
    "    res = pd.Series(0, index=dBs)\n",
    "\n",
    "    for v_dB in dBs:\n",
    "        d = np.array(dB) + v_dB\n",
    "        r = (d > 0)\n",
    "\n",
    "        f = np.array([1,1]) / 2\n",
    "        c = (np.convolve(r,f,'same') < 0.8)\n",
    "\n",
    "        f2 = np.array([-1,1])\n",
    "        c2 = np.convolve(c,f2,'same')\n",
    "        res[v_dB] = sum(c2==max(c2))\n",
    "\n",
    "\n",
    "    treshold = min(list(res.items()), key=lambda x: abs(x[1]-n_win_wanted))[0]\n",
    "\n",
    "    d = np.array(dB) + treshold\n",
    "    r = (d > 0)\n",
    "    f = np.array([1,1]) / 2\n",
    "    c = (np.convolve(r,f,'same') < 0.8)\n",
    "    f2 = np.array([-1,1])\n",
    "    c2 = np.convolve(c,f2,'same')\n",
    "\n",
    "    if affichage :\n",
    "        plt.figure(figsize=(14,3))\n",
    "        plt.title(\"treshold : \" + str(treshold))\n",
    "        plt.plot(d, alpha=0.8)\n",
    "        plt.plot(c2 * max(d), alpha=0.8)\n",
    "        plt.show()\n",
    "    return treshold\n",
    "\n",
    "# segemente l'audio suivant plusieurs méthodes\n",
    "def get_chunks(audiofile, samplerate=16000, method='subtitles', smooth='closing', speech=None, subtitles=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str audiofile : chemin du fichier audio\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "        str method : méthode de segmentation de l'audio\n",
    "        str smooth : méthode de lissage à utiliser dans get_speech_intervals\n",
    "        Series speech : signal booléen de la parole calculé par vad\n",
    "        numpy.ndarray subtitles : signal booléen de la parole à partir des sous-titres\n",
    "    Returns:\n",
    "        list listenable_chunks : liste de segments audio\n",
    "    \"\"\"\n",
    "    song = AudioSegment.from_wav(audiofile)\n",
    "    \n",
    "    if method == 'silence': \n",
    "        # split track where silence is quieter than the threshold in dBFS for x milliseconds\n",
    "        fen = 50\n",
    "        n_win_wanted = round(len(song) / 1000 / 3) # on vise des chunk de 3 secondes en moyenne\n",
    "        dB = np.array([song[i:i+fen].dBFS for i in range(0,len(song),fen)])\n",
    "        treshold = get_dB_treshold(dB, n_win_wanted=n_win_wanted, affichage=False)\n",
    "        chunks = split_on_silence(song, min_silence_len=100, silence_thresh=-treshold)\n",
    "    \n",
    "    elif method == 'speech':\n",
    "        # split track based on speech detection\n",
    "        bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "        if speech is None :\n",
    "            speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, smooth=smooth, affichage=False)\n",
    "        df_intervals = split_speech(speech, sec_before=0.8, sec_after=0.3)\n",
    "        chunks_sep = [(int(df_intervals.loc[i,'start_speech']/samplerate*1000),\n",
    "                       int(df_intervals.loc[i,'end_speech']/samplerate*1000))\n",
    "                      for i in range(len(df_intervals))]\n",
    "        chunks = [song[cs[0]:cs[1]] for cs in chunks_sep]\n",
    "        \n",
    "    elif method == 'stable_cuts':\n",
    "        # split every x secondes\n",
    "        bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "        df_intervals = pd.DataFrame(columns=['add_noise','start_signal','start_speech','end_speech','end_signal'])\n",
    "        cuts = pd.Series(signal)[::samplerate*5].index\n",
    "        for i,ci in enumerate(cuts):\n",
    "            df_intervals.loc[i,'add_noise'] = samplerate\n",
    "            df_intervals.loc[i,'start_signal'] = ci\n",
    "            df_intervals.loc[i,'start_speech'] = ci\n",
    "            df_intervals.loc[i,'end_speech'] = min(len(signal), ci+samplerate*10)\n",
    "            df_intervals.loc[i,'end_signal'] = min(len(signal), ci+samplerate*10)\n",
    "        chunks_sep = [(int(df_intervals.loc[i,'start_speech']/samplerate*1000),\n",
    "                       int(df_intervals.loc[i,'end_speech']/samplerate*1000))\n",
    "                      for i in range(len(df_intervals))]\n",
    "        chunks = [song[cs[0]:cs[1]] for cs in chunks_sep]\n",
    "        \n",
    "    elif method == 'subtitles':\n",
    "        # use subtitles timestamp to split the song, with help from speech detection\n",
    "        bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "        if speech is None :\n",
    "            speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, smooth=smooth, affichage=False)\n",
    "        \n",
    "        # resample audio to look like the song variable\n",
    "        signal_rs = scipy.signal.resample(np.array(signal), round(len(np.array(signal)) / samplerate * 1000))\n",
    "        speech_rs = scipy.signal.resample(np.array(speech), round(len(np.array(speech)) / samplerate * 1000)) > 0.5\n",
    "        \n",
    "        # subtitles length do not match audio length, find where to start:\n",
    "        pas = 100\n",
    "        m0 = get_decalage(subtitles, signal_rs, pas=pas, affichage=False)\n",
    "        m1 = get_decalage(subtitles, speech_rs, pas=pas, affichage=False)\n",
    "        m2 = suppression_bords(m0, t=5)\n",
    "        decalage = np.argmin(np.array(m1+m2)) * pas\n",
    "        \n",
    "        filtre = np.zeros(len(song))\n",
    "        filtre[decalage : decalage + len(subtitles)] = subtitles[0:len(song)]\n",
    "        \n",
    "        # split the single audio input into several speech audio output\n",
    "        df_intervals = split_speech(filtre, samplerate=1000, sec_before=0.5, sec_after=0.5)\n",
    "        chunks_sep = [(int(df_intervals.loc[i,'start_signal']),\n",
    "                       int(df_intervals.loc[i,'end_signal']))\n",
    "                      for i in range(len(df_intervals))]\n",
    "        chunks = [song[cs[0]:cs[1]] for cs in chunks_sep]\n",
    "    \n",
    "    listenable_chunks = [c for c in chunks if len(c) > 300]\n",
    "    return listenable_chunks\n",
    "\n",
    "# write and open a generated noise from a audio file\n",
    "def make_write_get_noise(audiofile, samplerate=16000, duration=500, smooth='closing', speech=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str audiofile : chemin du fichier audio\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "        int duration : durée du bruit en millisecondes\n",
    "        str smooth : méthode de lissage à utiliser dans get_speech_intervals\n",
    "        str speech : signal booléen de la parole calculé par vad\n",
    "    Returns:\n",
    "        AudioSegment noise : segment audio\n",
    "    \"\"\"\n",
    "    bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "    if speech is None :\n",
    "        speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, \n",
    "                                      smooth=smooth, affichage=False)\n",
    "    background = get_background(signal, speech, fe, affichage=False)\n",
    "    noise = make_noise(signal, background, fe, lenght=duration/1000)\n",
    "    filename = './audio_chunks/noise.wav'\n",
    "    sf.write(filename, noise, samplerate=fe, subtype='PCM_16')\n",
    "    noise = AudioSegment.from_wav(filename)\n",
    "    return noise\n",
    "\n",
    "# text from audio file using google recognizer\n",
    "def recon(audiofile, samplerate=16000, method='speech', smooth='closing', speech=None, subtitles=None, noise=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str audiofile : chemin du fichier audio\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "        str method : méthode de segmentation de l'audio\n",
    "        str smooth : méthode de lissage à utiliser dans get_speech_intervals\n",
    "        Series speech : signal booléen de la parole calculé par vad\n",
    "        numpy.ndarray subtitles : signal booléen de la parole depuis les sous-titres\n",
    "        bool noise : ajouter ou non du bruit\n",
    "    Returns:\n",
    "        float overall_conf : moyenne de la confiance que google a dans la reconnaissance de tous les chunks\n",
    "        str overall_text : texte représentant l'ensemble de ce qui est reconnu dans tous les chunks\n",
    "    \"\"\"\n",
    "    # create chunks\n",
    "    listenable_chunks = get_chunks(audiofile, samplerate=samplerate,\n",
    "                                   method=method, smooth=smooth, speech=speech, subtitles=subtitles)\n",
    "    \n",
    "    # noise and silence creation\n",
    "    chunk_noise  = make_write_get_noise(audiofile, samplerate=samplerate, duration=500, smooth=smooth, speech=speech)\n",
    "    chunk_silent = AudioSegment.silent(duration=500)\n",
    "\n",
    "    # create a directory to store the audio chunks.\n",
    "    try: os.mkdir('./audio_chunks') \n",
    "    except(FileExistsError): pass\n",
    "    \n",
    "    overall_conf = []\n",
    "    overall_text = ''\n",
    "    for i,chunk in enumerate(listenable_chunks):\n",
    "        \n",
    "        # add 500 milliseconds of silence or noise and raise the volume by 16 dB\n",
    "        if noise : audio_chunk = chunk_noise +  (chunk + 16) + chunk_noise\n",
    "        else :     audio_chunk = chunk_silent + (chunk + 16) + chunk_silent\n",
    "        \n",
    "        # save the newly created chunk\n",
    "        filename = './audio_chunks/chunk'+str(i)+'.wav'\n",
    "        audio_chunk.export(filename, bitrate ='192k', format =\"wav\") \n",
    "\n",
    "        # recognize the chunk\n",
    "        r = sr.Recognizer()\n",
    "        with sr.AudioFile(filename) as source: \n",
    "            audio_listened = r.record(source) \n",
    "        try:\n",
    "            rec = r.recognize_google(audio_listened, language='fr-FR', show_all=True)\n",
    "            if len(rec) == 0 : \n",
    "                conf = 0\n",
    "                text = ''\n",
    "            else :\n",
    "                if 'confidence' in rec['alternative'][0]: conf = rec['alternative'][0]['confidence']\n",
    "                else : conf = np.nan\n",
    "                text = rec['alternative'][0]['transcript']\n",
    "            \n",
    "            overall_text += ' ' + text\n",
    "            overall_conf.append(conf)\n",
    "        except sr.UnknownValueError: print(\"-- Could not understand audio\") \n",
    "        except sr.RequestError as e: print(\"-- Could not request results. check your internet connection\")\n",
    "    \n",
    "    overall_text = \" \".join(overall_text.split())\n",
    "    overall_conf = np.array(overall_conf)[np.array(overall_conf) != 0].mean()\n",
    "    \n",
    "    return overall_conf, overall_text\n",
    "\n",
    "# get features from a scene\n",
    "def extract_features(scene, method='subtitles', smooth='closing', noise=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str scene : nom de la scène (ex : '100_1')\n",
    "        str method : méthode de segmentation de l'audio\n",
    "        str smooth : méthode de lissage à utiliser dans get_speech_intervals\n",
    "        bool noise : ajouter ou non du bruit\n",
    "    Returns:\n",
    "        float SR : proportion qui est parlée dans la scène\n",
    "        float SNR : rapport signal sur bruit\n",
    "        float VBR : rapport vocal on background\n",
    "        float conf : indice de confiance de google\n",
    "        str text : texte reconnu par google\n",
    "        float score : score de reconnaissance basé sur la distance de Levenshtein\n",
    "        str sent : texte à reconnaitre\n",
    "    \"\"\"\n",
    "    spleet(scene)\n",
    "    audiofile = spleeter_output_dir + scene + audio_extension.split('.')[-2] + '/vocals_resample.wav'\n",
    "    xmlfile   = dir_texte + scene + texte_extension\n",
    "    \n",
    "    # load data\n",
    "    bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "    \n",
    "    # speech intervals à partir de la librairie webrtcvad\n",
    "    speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, smooth=smooth, affichage=False)\n",
    "    SR = speech.sum()/len(speech)\n",
    "    \n",
    "    # SNR (from speech data)\n",
    "    SNR = get_SNR(signal, speech, samplerate=fe)\n",
    "    \n",
    "    # VBR (vocal background ratio from speeted file)\n",
    "    vocal_file = spleeter_output_dir + scene + audio_extension.split('.')[-2] + '/vocals.wav'\n",
    "    backg_file = spleeter_output_dir + scene + audio_extension.split('.')[-2] + '/accompaniment.wav'\n",
    "    signal_vocal, fe_vocal = librosa.load(vocal_file, sr=samplerate, mono=True)\n",
    "    signal_backg, fe_backg = librosa.load(backg_file, sr=samplerate, mono=True)\n",
    "    VBR = (signal_vocal**2).sum() / ((signal_vocal**2).sum() + (signal_backg**2).sum())\n",
    "    \n",
    "    # xml to text\n",
    "    sentences = get_timed_sentences(xmlfile)\n",
    "    sent_ponct = \" \".join([s[1] for s in sentences])\n",
    "    sent = \" \".join(\"\".join([x if x.isalpha() else \" \" for x in sent_ponct]).split())\n",
    "    \n",
    "    # speech intervals à partir des sous titres\n",
    "    subtitles = get_filtre_paroles(sentences, dilation_ms=300)\n",
    "    \n",
    "    # audio to text\n",
    "    conf, text = recon(audiofile, samplerate=fe, method=method, smooth=smooth,\n",
    "                       speech=speech, subtitles=subtitles, noise=noise)\n",
    "    \n",
    "    # distance between text_audio and text_xml and score\n",
    "    levenshtein = strsimpy.Levenshtein()\n",
    "    distance = levenshtein.distance(text.lower(), sent.lower())\n",
    "    score = distance / len(sent)\n",
    "    \n",
    "    return SR, SNR, VBR, conf, text, score, sent\n",
    "\n",
    "# generate the features dataframe\n",
    "def make_audio_features(scenes, df=None, samplerate=16000, method='silence', smooth='rolling_mean', noise=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str scenes : liste de scènes (ex : ['100_1','101_7'])\n",
    "        DataFrame df : dataframe des features (déja calculé)\n",
    "        int samplerate : fréquence d'échantillonage\n",
    "        str method : méthode de segmentation de l'audio\n",
    "        str smooth : méthode de lissage à utiliser dans get_speech_intervals\n",
    "        bool noise : ajouter ou non du bruit\n",
    "    Returns:\n",
    "        DataFrame df : dataframe des features\n",
    "    \"\"\"\n",
    "    if df is None : df = pd.DataFrame(columns=['SCENE','SR','SNR','VBR','CONF','RECON','SCORE','XML'])\n",
    "    for scene in scenes:\n",
    "        if scene not in list(df['SCENE']):\n",
    "            SR, SNR, VBR, conf, text, score, sent = extract_features(scene, method=method, smooth=smooth, noise=noise)\n",
    "            \n",
    "            row = pd.Series([scene,SR,SNR,VBR,conf,text,score,sent],\n",
    "                            index = df.columns)\n",
    "\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            print(scene, score)\n",
    "    return df\n",
    "\n",
    "# using spleeter library, split a audio file in 2 : vocal and background\n",
    "def spleet(scene):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        str scene : nom de la scène (ex : '100_1')\n",
    "    Returns:\n",
    "        create 3 audio files : 'vocals.wav', 'vocals_resample.wav' and 'accompaniment.wav' using spleeter\n",
    "    \"\"\"\n",
    "    name = scene + audio_extension.split('.')[-2]\n",
    "    rep = spleeter_output_dir + name + '/'\n",
    "\n",
    "    audiofile = dir_audio + scene + audio_extension\n",
    "    audio_dirs  = [f for f in os.listdir(spleeter_output_dir)\n",
    "                   if os.path.isdir(os.path.join(spleeter_output_dir, f))]\n",
    "\n",
    "    if name not in audio_dirs :\n",
    "        print(\"speeting\", scene, \"...\")\n",
    "        subprocess.run(['spleeter','separate','-i',audiofile,'-o',spleeter_output_dir])\n",
    "\n",
    "    if 'vocals_resample.wav' not in os.listdir(rep):\n",
    "        audiofile = rep + 'vocals.wav'\n",
    "        signal, fe = librosa.load(audiofile, sr=samplerate, mono=True)\n",
    "        filename = rep + 'vocals_resample.wav'\n",
    "        sf.write(filename, signal, samplerate=fe, subtype='PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
