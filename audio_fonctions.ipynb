{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading signal\n",
    "def load_signal(file, samplerate=16000):\n",
    "    signal, fe = librosa.load(file, sr=samplerate, mono=True)\n",
    "    \n",
    "    w = wave.open(file, \"rb\")\n",
    "    binary_data = w.readframes(w.getnframes())\n",
    "    \n",
    "    return binary_data, signal, fe\n",
    "\n",
    "# speech detection using webrtcvad\n",
    "def get_speech_intervals(bin_signal, samplerate=16000, agg=3, smooth='closing', affichage=False):\n",
    "    recomposed_signal = np.frombuffer(bin_signal, dtype=np.int16)\n",
    "\n",
    "    vad = webrtcvad.Vad()\n",
    "    vad.set_mode(mode=agg)\n",
    "\n",
    "    millisec = 10\n",
    "    fenetre = int(samplerate * millisec * 2 / 1000)\n",
    "\n",
    "    fen_speech = [vad.is_speech(bin_signal[m:m+fenetre], samplerate) \n",
    "                  for m in range(0,len(bin_signal),fenetre)\n",
    "                 if len(bin_signal[m:m+fenetre]) == fenetre]\n",
    "\n",
    "    widened_speech = np.array([[s] * int(fenetre/2) for s in fen_speech]).ravel()\n",
    "    speech = np.full(len(recomposed_signal), widened_speech[-1])\n",
    "    speech[0:len(widened_speech)] = widened_speech\n",
    "\n",
    "    # adoucissement\n",
    "    if smooth == 'closing':\n",
    "        smoothed_speech = ndimage.binary_closing(input=speech, structure=np.array([1]*10000)).astype(bool)\n",
    "    elif smooth == 'rolling_mean' :\n",
    "        smoothed_speech = pd.Series(speech).rolling(window=int(samplerate/5), min_periods=2, center=True).mean() > 0.2\n",
    "    \n",
    "    \n",
    "    if affichage :\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(np.arange(len(smoothed_speech))/samplerate, smoothed_speech*max(recomposed_signal), label=\"speech\")\n",
    "        plt.plot(np.arange(len(recomposed_signal))/samplerate, recomposed_signal, label=\"signal\")\n",
    "        plt.xlabel(\"sec\")\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        \n",
    "    return pd.Series(smoothed_speech)\n",
    "\n",
    "# SNR using speech detection (bad on recomposed signal from binary)\n",
    "def get_SNR(signal, speech, samplerate=16000):\n",
    "    s = pd.Series(signal)\n",
    "    energy_total = (s**2).rolling(window=samplerate, min_periods=2, center=True).mean()\n",
    "\n",
    "    energy_speech = (s[speech]**2).rolling(window=samplerate, min_periods=2, center=True).mean()\n",
    "    energy_notSpeech = (s[~speech]**2).rolling(window=samplerate, min_periods=2, center=True).mean()\n",
    "\n",
    "    SNR = energy_speech.mean() / (energy_speech.mean() + energy_notSpeech.mean())\n",
    "    \n",
    "    return SNR\n",
    "\n",
    "def get_filtre_paroles(ts, dilation_ms=300):\n",
    "    # creation du filtre paroles\n",
    "    ts_longueur = int(np.sum(ts[-1][0])*1000)\n",
    "    paroles = np.zeros(ts_longueur)\n",
    "    for i,v in ts :\n",
    "        a,d = int(i[0]*1000), int(i[1]*1000)\n",
    "        paroles[a:a+d] = 1\n",
    "    \n",
    "    dilation_ms = 300\n",
    "    paroles = ndimage.binary_dilation(input=paroles, structure=np.array([1]*dilation_ms)).astype(bool)\n",
    "    return paroles\n",
    "\n",
    "def suppression_bords(m,t):\n",
    "    droite = np.zeros([len(m)+2*t])\n",
    "    droite[0:t], droite[-t:], droite[t:-t] = 4,4,1\n",
    "    droite = droite / droite.sum()\n",
    "    m2 = pd.Series(droite).rolling(window=2*t+1, center=True).mean()[t:-t]\n",
    "    m2.index = m.index\n",
    "    return m2/2\n",
    "\n",
    "def get_decalage(paroles, signal, pas=100, affichage=False):\n",
    "    \n",
    "    duree = round(len(signal) / samplerate * 1000)\n",
    "    intervals = range(0, duree - len(paroles), pas)\n",
    "    aires_0 = np.zeros([len(intervals)])\n",
    "    aires_1 = np.zeros([len(intervals)])\n",
    "\n",
    "    for i,debut in enumerate(intervals):\n",
    "        aire_0 = signal[debut:debut+len(paroles)] * (paroles == 0)\n",
    "        aire_1 = signal[debut:debut+len(paroles)] * (paroles == 1)\n",
    "        aires_0[i] = abs(aire_0).sum()\n",
    "        aires_1[i] = abs(aire_1).sum()\n",
    "\n",
    "    decalage = np.argmin(aires_0 - aires_1)*pas\n",
    "    aire_0 = signal[decalage:decalage+len(paroles)] * (paroles == 0)\n",
    "    aire_1 = signal[decalage:decalage+len(paroles)] * (paroles == 1)\n",
    "    \n",
    "    resultat = np.zeros(duree)\n",
    "    resultat[decalage : decalage + len(paroles)] = paroles\n",
    "    \n",
    "    metric = (aires_0-aires_1)\n",
    "    metric = metric / abs(metric.sum())\n",
    "    metric = pd.Series(metric, index=np.arange(0,len(metric))*pas)\n",
    "    \n",
    "    if affichage:\n",
    "        plt.figure(figsize=(14,2))\n",
    "        plt.title(\"        \"+str(decalage))\n",
    "        plt.plot(resultat, label=\"paroles\")\n",
    "        plt.plot(index_signal, signal)\n",
    "        plt.legend(loc=2)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(14,2))\n",
    "        plt.plot(metric, label=\"aires\")\n",
    "        plt.legend(loc=2)\n",
    "        plt.show()\n",
    "        \n",
    "    return metric\n",
    "\n",
    "# return text sentence from audio\n",
    "def get_recognition(audiofile, adjust_noise=False):\n",
    "    r  = sr.Recognizer()\n",
    "    demo = sr.AudioFile(audiofile)\n",
    "    with demo as source:\n",
    "        if adjust_noise: r.adjust_for_ambient_noise(source)\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        recon = r.recognize_google(audio, language='fr-FR', show_all=True)\n",
    "        if len(recon) == 0 :\n",
    "            text = ''\n",
    "            conf = 0\n",
    "        else :\n",
    "            text = recon['alternative'][0]['transcript']\n",
    "            conf = recon['alternative'][0]['confidence']\n",
    "    except LookupError:\n",
    "        print(\"LookupError : Could not understand audio\")\n",
    "        text = ''\n",
    "        conf = 0\n",
    "    \n",
    "    return conf, text\n",
    "\n",
    "# return sentences, when they start in seconds and their length in seconds\n",
    "def get_timed_sentences(xmlfile):\n",
    "    tree = ET.parse(xmlfile)\n",
    "\n",
    "    sentences = [([e.attrib['value'] for e in sent if e.tag=='time'],\n",
    "                  \" \".join([w.text.strip() for w in sent if w.text is not None]))\n",
    "                 for sent in tree.getroot()]\n",
    "\n",
    "    t0 = datetime.datetime.strptime(sentences[0][0][0], '%H:%M:%S,%f')\n",
    "    for i,(t,s) in enumerate(sentences):\n",
    "        t1 = datetime.datetime.strptime(t[0], '%H:%M:%S,%f')\n",
    "        t2 = datetime.datetime.strptime(t[1], '%H:%M:%S,%f')\n",
    "        sentences[i] = (((t1 - t0).total_seconds() , (t2 - t1).total_seconds()),s)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# split tthe speech signal (0/1) into several signal (1)\n",
    "def split_speech(speech, samplerate=16000, sec_before=1, sec_after=0.5):\n",
    "    f = np.array([1,-1])\n",
    "    r = np.convolve(speech, f, 'same')\n",
    "    starts, ends = np.where(r == 1)[0].tolist(), np.where(r == -1)[0].tolist()\n",
    "    if ends[0] < starts[0] : starts = [0] + starts\n",
    "    if len(starts) > len(ends) : ends = ends + [len(speech)]\n",
    "\n",
    "    speech_intervals = np.array([(0,\n",
    "                                  starts[i] - int(samplerate*sec_before),\n",
    "                                  starts[i],\n",
    "                                  ends[i], ends[i] + int(samplerate*sec_after))\n",
    "                                 for i in range(len(starts))])\n",
    "\n",
    "    diffs = [(v[2] - speech_intervals[i][-2] - int(samplerate*sec_before)) for i,v in enumerate(speech_intervals[1:])]\n",
    "    diffs = [0 if speech_intervals[0,1] > 0 else -speech_intervals[0,1]] + [0 if v > 0 else abs(v) for v in diffs]\n",
    "\n",
    "    speech_intervals[:,0] = diffs\n",
    "    speech_intervals[:,1] += speech_intervals[:,0]\n",
    "    speech_intervals[speech_intervals < 0] = 0\n",
    "\n",
    "    df_intervals = pd.DataFrame(speech_intervals,\n",
    "                                columns=['add_noise','start_signal','start_speech','end_speech','end_signal'])\n",
    "    return df_intervals\n",
    "\n",
    "# return bool signal where the background noise is\n",
    "def get_background(signal, speech, samplerate, affichage=False):\n",
    "    background = speech.rolling(window=int(samplerate/4), center=True).mean() < 1/10\n",
    "    \n",
    "    if affichage:\n",
    "        plt.figure(figsize=(14,4))\n",
    "        plt.plot(signal, label=\"signal\")\n",
    "        plt.plot(speech * max(signal) / 2, label=\"speech\")\n",
    "        plt.plot(background * max(signal), label=\"noise\")\n",
    "        plt.legend(loc=4)\n",
    "        plt.show()\n",
    "        \n",
    "    return background\n",
    "\n",
    "# make noise from signal and background\n",
    "def make_noise(signal, background, samplerate, lenght=3):\n",
    "    if background.sum() < samplerate/10 :\n",
    "        noise = signal[:len(signal) - len(signal)%samplerate].reshape(samplerate, int(len(signal)/samplerate)).mean(axis=1)\n",
    "    else :\n",
    "        noise = np.array([signal[background][m:m+int(lenght*samplerate)]\n",
    "                          for m in range(len(signal[background][::int(lenght*samplerate)]))]).mean(axis=0)\n",
    "        if len(noise) < lenght*samplerate:\n",
    "            noise = np.array(list(noise) * (int(lenght*samplerate / len(noise))+1))[0:int(lenght*samplerate)]\n",
    "    return noise\n",
    "\n",
    "def get_dB_treshold(dB, n_win_wanted=11, affichage=False):\n",
    "    dBs = list(range(50,100))\n",
    "    res = pd.Series(0, index=dBs)\n",
    "\n",
    "    for v_dB in dBs:\n",
    "        d = np.array(dB) + v_dB\n",
    "        r = (d > 0)\n",
    "\n",
    "        taille = 1\n",
    "        f = np.array([1,1]) / 2\n",
    "        c = (np.convolve(r,f,'same') < 0.8)\n",
    "\n",
    "        f2 = np.array([-1,1])\n",
    "        c2 = np.convolve(c,f2,'same')\n",
    "        res[v_dB] = sum(c2==max(c2))\n",
    "\n",
    "        if affichage:\n",
    "            plt.figure(figsize=(14,3))\n",
    "            plt.plot(d, alpha=0.5)\n",
    "            plt.plot(c2 * max(d), alpha=0.5)\n",
    "            plt.show()\n",
    "    \n",
    "    treshold = min(list(res.items()), key=lambda x: abs(x[1]-n_win_wanted))[0]\n",
    "    return treshold\n",
    "\n",
    "def get_chunks(audiofile, samplerate=16000, method='speech', smooth='closing', speech=None, subtitles=None):\n",
    "    song = AudioSegment.from_wav(audiofile)\n",
    "    \n",
    "    if method == 'silence': \n",
    "        # split track where silence is quieter than the threshold in dBFS for x milliseconds\n",
    "        fen = 300\n",
    "        dB = [song[i:i+fen].dBFS for i in range(0,len(song),fen)]\n",
    "        treshold = get_dB_treshold(dB, n_win_wanted=11, affichage=False)\n",
    "        chunks = split_on_silence(song, min_silence_len=100, silence_thresh=-treshold)\n",
    "    \n",
    "    elif method == 'speech':\n",
    "        # split track based on speech detection\n",
    "        bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "        if speech is None :\n",
    "            speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, smooth=smooth, affichage=False)\n",
    "        df_intervals = split_speech(speech, sec_before=0.8, sec_after=0.3)\n",
    "        chunks_sep = [(int(df_intervals.loc[i,'start_speech']/samplerate*1000),\n",
    "                       int(df_intervals.loc[i,'end_speech']/samplerate*1000))\n",
    "                      for i in range(len(df_intervals))]\n",
    "        chunks = [song[cs[0]:cs[1]] for cs in chunks_sep]\n",
    "        \n",
    "    elif method == 'stable_cuts':\n",
    "        bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "        df_intervals = pd.DataFrame(columns=['add_noise','start_signal','start_speech','end_speech','end_signal'])\n",
    "        cuts = pd.Series(signal)[::samplerate*5].index\n",
    "        for i,ci in enumerate(cuts):\n",
    "            df_intervals.loc[i,'add_noise'] = samplerate\n",
    "            df_intervals.loc[i,'start_signal'] = ci\n",
    "            df_intervals.loc[i,'start_speech'] = ci\n",
    "            df_intervals.loc[i,'end_speech'] = min(len(signal), ci+samplerate*10)\n",
    "            df_intervals.loc[i,'end_signal'] = min(len(signal), ci+samplerate*10)\n",
    "        chunks_sep = [(int(df_intervals.loc[i,'start_speech']/samplerate*1000),\n",
    "                       int(df_intervals.loc[i,'end_speech']/samplerate*1000))\n",
    "                      for i in range(len(df_intervals))]\n",
    "        chunks = [song[cs[0]:cs[1]] for cs in chunks_sep]\n",
    "        \n",
    "    elif method == 'subtitles':\n",
    "        bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "        pas = 100\n",
    "        m0 = get_decalage(subtitles, signal, pas=pas, affichage=False)\n",
    "        m1 = get_decalage(subtitles, np.array(speech), pas=pas, affichage=False)\n",
    "        m2 = suppression_bords(m0, t=5)\n",
    "        decalage = np.argmin(np.array(m1+m2)) * pas\n",
    "        \n",
    "        filtre = np.zeros(len(song))\n",
    "        filtre[decalage : decalage + len(subtitles)] = subtitles\n",
    "        \n",
    "        df_intervals = split_speech(filtre, samplerate=1000, sec_before=0.5, sec_after=0.5)\n",
    "        chunks_sep = [(int(df_intervals.loc[i,'start_signal']),\n",
    "                       int(df_intervals.loc[i,'end_signal']))\n",
    "                      for i in range(len(df_intervals))]\n",
    "        chunks = [song[cs[0]:cs[1]] for cs in chunks_sep]\n",
    "    \n",
    "    listenable_chunks = [c for c in chunks if len(c) > 300]\n",
    "    return listenable_chunks\n",
    "\n",
    "def make_write_get_noise(audiofile, samplerate=16000, duration=500, smooth='closing', speech=None):\n",
    "    bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "    if speech is None :\n",
    "        speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, smooth=smooth, affichage=False)\n",
    "    background = get_background(signal, speech, fe, affichage=False)\n",
    "    noise = make_noise(signal, background, fe, lenght=duration/1000)\n",
    "    filename = './audio_chunks/noise.wav'\n",
    "    sf.write(filename, noise, samplerate=fe, subtype='PCM_16')\n",
    "    noise = AudioSegment.from_wav(filename)\n",
    "    return noise\n",
    "\n",
    "# text from audio file using google recognizer\n",
    "def recon(audiofile, method='speech', smooth='closing', speech=None, subtitles=None):\n",
    "    #song = AudioSegment.from_wav(audiofile) \n",
    "    \n",
    "    # create chunks\n",
    "    listenable_chunks = get_chunks(audiofile, samplerate=samplerate,\n",
    "                                   method=method, smooth=smooth, speech=speech, subtitles=subtitles)\n",
    "    \n",
    "    # noise and silence creation\n",
    "    chunk_noise  = make_write_get_noise(audiofile, samplerate=16000, duration=500, smooth=smooth, speech=speech)\n",
    "    chunk_silent = AudioSegment.silent(duration=500)\n",
    "\n",
    "    # create a directory to store the audio chunks.\n",
    "    try: os.mkdir('./audio_chunks') \n",
    "    except(FileExistsError): pass\n",
    "    \n",
    "    overall_conf = []\n",
    "    overall_text = ''\n",
    "    for i,chunk in enumerate(listenable_chunks):\n",
    "        \n",
    "        # add 500 milliseconds silence and raise the volume by 16 dB\n",
    "        audio_chunk = chunk_silent + (chunk + 16) + chunk_silent\n",
    "        # or add 500 milliseconds of noise and raise the volume by 16 dB\n",
    "        audio_chunk = chunk_noise + (chunk + 16) + chunk_noise\n",
    "        \n",
    "        # save the newly created chunk\n",
    "        filename = './audio_chunks/chunk'+str(i)+'.wav'\n",
    "        audio_chunk.export(filename, bitrate ='192k', format =\"wav\") \n",
    "\n",
    "        # recognize the chunk\n",
    "        r = sr.Recognizer()\n",
    "        with sr.AudioFile(filename) as source: \n",
    "            audio_listened = r.record(source) \n",
    "        try:\n",
    "            rec = r.recognize_google(audio_listened, language='fr-FR', show_all=True)\n",
    "            if len(rec) == 0 : \n",
    "                conf = 0\n",
    "                text = ''\n",
    "            else :\n",
    "                if 'confidence' in rec['alternative'][0]: conf = rec['alternative'][0]['confidence']\n",
    "                else : conf = np.nan\n",
    "                text = rec['alternative'][0]['transcript']\n",
    "            \n",
    "            overall_text += ' ' + text\n",
    "            overall_conf.append(conf)\n",
    "        except sr.UnknownValueError: print(\"-- Could not understand audio\") \n",
    "        except sr.RequestError as e: print(\"-- Could not request results. check your internet connection\")\n",
    "    \n",
    "    overall_text = \" \".join(overall_text.split())\n",
    "    overall_conf = np.array(overall_conf)[np.array(overall_conf) != 0].mean()\n",
    "    \n",
    "    return overall_conf, overall_text\n",
    "\n",
    "# get features from a scene\n",
    "def extract_features(scene, method='speech', smooth='closing'):\n",
    "    audiofile = dir_audio + scene + audio_extension\n",
    "    xmlfile   = dir_texte + scene + texte_extension\n",
    "    \n",
    "    # load data\n",
    "    bin_signal, signal, fe = load_signal(audiofile, samplerate=samplerate)\n",
    "\n",
    "    # speech intervals à partir de la librairie webrtcvad\n",
    "    speech = get_speech_intervals(bin_signal, samplerate=fe, agg=3, smooth=smooth, affichage=False)\n",
    "    SR = speech.sum()/len(speech)\n",
    "\n",
    "    # SNR (from speech data)\n",
    "    SNR = get_SNR(signal, speech, samplerate=fe)\n",
    "    \n",
    "    # xml to text\n",
    "    sentences = get_timed_sentences(xmlfile)\n",
    "    sent_ponct = \" \".join([s[1] for s in sentences])\n",
    "    sent = \" \".join(\"\".join([x if x.isalpha() else \" \" for x in sent_ponct]).split())\n",
    "    \n",
    "    # speech intervals à partir des sous titres\n",
    "    subtitles = get_filtre_paroles(sentences, dilation_ms=300)\n",
    "    \n",
    "    # audio to text\n",
    "    conf, text = recon(audiofile, method=method, smooth=smooth, speech=speech, subtitles=subtitles)\n",
    "    \n",
    "    # distance between text_audio and text_xml and score\n",
    "    levenshtein = strsimpy.Levenshtein()\n",
    "    distance = levenshtein.distance(text.lower(), sent.lower())\n",
    "    score = distance / len(sent)\n",
    "    \n",
    "    return SR, SNR, conf, text, score, sent\n",
    "\n",
    "def make_audio_features(scenes, df=None, samplerate=16000, method='subtitles', smooth='closing'):\n",
    "    if df is None : df = pd.DataFrame(columns=['SCENE','SR','SNR','CONF','RECON','SCORE','XML'])\n",
    "    for scene in scenes:\n",
    "        if scene not in list(df['SCENE']):\n",
    "            SR, SNR, conf, text, score, sent = extract_features(scene, method=method, smooth=smooth)\n",
    "            \n",
    "            row = pd.Series([scene,SR,SNR,conf,text,score,sent],\n",
    "                            index = df.columns)\n",
    "\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            print(scene, score)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "106_2 0.7231222385861561  method='subtitles', smooth='rolling_mean'  \n",
    "106_2 0.7614138438880707  method='subtitles', smooth='closing'  \n",
    "106_2 0.7614138438880707  method='speech', smooth='rolling_mean'  \n",
    "106_2 0.7378497790868925  method='speech', smooth='closing'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
