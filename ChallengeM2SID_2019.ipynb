{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChallengeM2SID_2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% \n"
        },
        "id": "TJB_Qi1KFPAy",
        "colab_type": "code",
        "outputId": "64706aed-3373-4dd8-8566-fe4553c5d69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "# OS setup\n",
        "!rm -rf challenge-m2-sid/\n",
        "!cat /etc/os-release\n",
        "!apt-get install -qq bc tree sox\n",
        "\n",
        "# Liaison avec les données\n",
        "#!git clone \"https://etudiantsid:etudiantsidPW;@gitlab.com/jeromefarinas/challenge-m2-sid.git\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.3 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 132684 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../2-libmagic-mgc_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../3-libmagic1_1%3a5.32-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Selecting previously unselected package bc.\n",
            "Preparing to unpack .../4-bc_1.07.1-2_amd64.deb ...\n",
            "Unpacking bc (1.07.1-2) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../5-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../6-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../7-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../8-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package tree.\n",
            "Preparing to unpack .../9-tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.3) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.3) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up bc (1.07.1-2) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9Rq-iFYpFPA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "1d6e4ba0-acb3-4cf9-b020-8e92e538c97d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile\n",
        "import scipy.signal\n",
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import GaussianNoise,BatchNormalization, Conv1D\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re  \n",
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "from google.colab import files\n",
        "import datetime\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn import preprocessing\n",
        "import sklearn.preprocessing\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "o9kIqLh4FPA4",
        "colab_type": "text"
      },
      "source": [
        "# Label preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CZRnow5RFPA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Return a part of the dataset with only 1 medium (text, audio, text + audio...)\n",
        "def get_medium(medium, df):\n",
        "  \"\"\"\n",
        "  # Return a subset of informations limited to a communication medium \n",
        "  # (audio : 100 , text : 001, audio and video : 110, audio and text : 101 \n",
        "  # audio, video and text : 111)\n",
        "  Parameters:\n",
        "      :param medium: ID for a medium \n",
        "      :param df: dataset containing \"code_doc\" columns containing \n",
        "      xx_x_medium_x as an document ID\n",
        "      \n",
        "      :type medium: string\n",
        "      :type df: DataFrame (pandas)\n",
        "  \n",
        "  Returns:\n",
        "      medium: the part of the dataset with only the choosen medium\n",
        "      type : DataFrame (pandas)\n",
        "      \n",
        "  :Example:\n",
        "\n",
        "      >>> print(get_medium(\"100\",csv_file))\n",
        " Unnamed: 0      code_doc  il08_09  ...  la09_10  cg13_14  mb00_12\n",
        "5              6    57_6_100_1       -1  ...     -1.0     -1.0       76\n",
        "55            56   147_1_100_1       -1  ...     -1.0     -1.0       64\n",
        "135          136   210_3_100_1       70  ...     -1.0     -1.0       -1\n",
        "        ... \n",
        "  \"\"\"\n",
        "  return (df[df[\"code_doc\"].map(lambda x : x[len(x)-5:-2]==medium)])  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# Return the list of label for each document\n",
        "def ret_max_docid(medium, only_commented):\n",
        "  '''\n",
        "  Choose the medium on which return the list of label for each document\n",
        "  Medium is a string : sequence of 3 bits : audio-video-texte sequence\n",
        "  only_commented : Dataframe of each annotated extract (not only extracts\n",
        "  ending with a \"1\")\n",
        "  Mean of multiple label is used when there are differents labels for one\n",
        "  document.\n",
        "  Parameters:\n",
        "      :param medium: ID for a medium \n",
        "      :param df: dataset containing \"code_doc\" columns containing \n",
        "      xx_x_medium_x as an document ID\n",
        "      \n",
        "      :type medium: string\n",
        "      :type df: DataFrame (pandas)\n",
        "  \n",
        "  Returns:\n",
        "      list_labels : couple list of each (document id, label) \n",
        "      type : list (of couple)\n",
        "  :Example:\n",
        "\n",
        "      >>>ret_max_docid(\"101\", only_commented)\n",
        "        [('57_6_101_0', 87.0),\n",
        "         ('88_11_101_1', 34.0),\n",
        "         ('51_5_101_1', 65.0),\n",
        "          ...\n",
        "  '''\n",
        "  # Get the list of annotated extracts for a medium\n",
        "  medium = get_medium(medium, only_commented)\n",
        "  # Replace all -1 by a NaN value  \n",
        "  medium = medium.replace(-1.,np.NaN)\n",
        "  # Return the list of couple (doc_id, evaluation max of complexity)\n",
        "  return [(row[1],row[2:].mean()) for index,row in medium.iterrows() ]\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JXYcjqiPFPA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Annotation file reading\n",
        "\n",
        "def get_dataset(csv_file):\n",
        "  \"\"\"\n",
        "  Get only the commented row in the annoted csv file. The last digit is \n",
        "  here to know if a row is empty or not but some labels are forgotten. This \n",
        "  function return only row that contains something different from -1.\n",
        "  Parameters:\n",
        "      :param csv_file: DataFrame with all label for each document \n",
        "      :type csv_file : DataFrame (pandas)\n",
        "      \n",
        "  Returns:\n",
        "      dataset: The sub part of the annotation file with only commented \n",
        "      documents\n",
        "      type : DataFrame Pandas\n",
        "      \n",
        "  :Example:\n",
        "\n",
        "      >>> get_dataset(updated_csv)\n",
        "        \\t\tcode_doc\til08_09\tvg04_05\tfd03_04\tla09_10\tcg13_14\tja05_06\tfj11_12\tec20_11\tmb00_12\n",
        "      5\t6\t57_6_100_1\t-1\t-1\t-1\t-1.0\t-1.0\t-1.0\t100.0\t-1.0\t76\n",
        "      6\t7\t57_6_110_1\t100\t100\t-1\t-1.0\t-1.0\t-1.0\t-1.0\t-1.0\t-1\n",
        "      7\t8\t57_6_111_1\t-1\t-1\t-1\t88.0\t-1.0\t-1.0\t-1.0\t-1.0\t-1\n",
        "\n",
        "  \"\"\"\n",
        "  # 2 first columns are index and code_id\n",
        "  names = csv_file.columns[2:]\n",
        "  dataset=[]\n",
        "  for index,row in csv_file.iterrows():\n",
        "    if any(row[names]!=-1):\n",
        "      dataset.append(row)\n",
        "  return pd.DataFrame(dataset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmeNiUTo-hN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute a standard normalisation for the labels.\n",
        "def normalisation_annot(df):\n",
        "  \"\"\"\n",
        "  Compute a standard normalisation with mean and Standard deviation on label\n",
        "  to remove bias and make label comparables. Return the normalised distribution\n",
        "  with mean = 0 and std = 1, the max and the min of the distribution for each\n",
        "  annotator to make it available to get back the value of the label.\n",
        "  Parameters:\n",
        "      :param df: Label dataframe on wich perform the normalisation\n",
        "      :type df: DataFrame (pandas)\n",
        "      \n",
        "  Returns:\n",
        "      norm_df,max_list,min_list: tuple containing the normalised DataFrame, the\n",
        "      list of max for each annotator and the list of min for each annotator\n",
        "      type : (DataFrame (pandas), list,list)\n",
        "      \n",
        "  Other itema to note:\n",
        "      - Don't forget to remove -1 in the dataset, unless the normalisation\n",
        "      will be biased  \n",
        "\n",
        "  \"\"\"\n",
        "  name = df.columns[2:]\n",
        "  # Work on a copy of the DF\n",
        "  ret_df = df[df[name]!=-1]\n",
        "  max_list = []\n",
        "  min_list = []\n",
        "  for i, annot in enumerate(ret_df[name]):\n",
        "      ret_df[annot]= (ret_df[annot] - ret_df[annot].mean()) / ret_df[annot].std()\n",
        "      max_list.append(ret_df[annot].max())\n",
        "      min_list.append(ret_df[annot].min())\n",
        "  for index in df.index :\n",
        "    ret_df[\"code_doc\"][index] = df[\"code_doc\"][index]\n",
        "  return  ret_df, np.array(max_list), np.array(min_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "y8kWpXyDFPBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "\n",
        "\n",
        "dataset = get_dataset(updated_csv)\n",
        "#dataset = dataset.replace(50,np.NaN)\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JDwPeo0xFPBG",
        "colab_type": "code",
        "outputId": "e4692dd9-aaa8-408a-ae36-7b399c674d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "norm_dataset,data_max_list,data_min_list"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(      Unnamed: 0     code_doc   il08_09  ...   fj11_12   ec20_11   mb00_12\n",
              " 5            NaN   57_6_100_1       NaN  ...  1.359498       NaN  1.511797\n",
              " 6            NaN   57_6_110_1  2.355661  ...       NaN       NaN       NaN\n",
              " 7            NaN   57_6_111_1       NaN  ...       NaN       NaN       NaN\n",
              " 8            NaN   57_6_101_1       NaN  ...       NaN       NaN       NaN\n",
              " 9            NaN   57_6_001_1       NaN  ...       NaN  1.607416       NaN\n",
              " ...          ...          ...       ...  ...       ...       ...       ...\n",
              " 1430         NaN  256_1_100_1       NaN  ...       NaN  0.664170       NaN\n",
              " 1431         NaN  256_1_110_1       NaN  ... -0.192199       NaN  0.694386\n",
              " 1432         NaN  256_1_111_1       NaN  ...       NaN       NaN       NaN\n",
              " 1433         NaN  256_1_101_1       NaN  ...       NaN       NaN       NaN\n",
              " 1434         NaN  256_1_001_1 -0.308542  ...       NaN       NaN       NaN\n",
              " \n",
              " [241 rows x 11 columns],\n",
              " array([2.35566073, 1.00357724, 2.51622781, 1.59344352, 2.3850719 ,\n",
              "        1.85812355, 1.35949831, 1.60741643, 1.82967853]),\n",
              " array([-2.45148778, -2.59301938, -2.01415897, -3.2541503 , -1.68491953,\n",
              "        -1.45184789, -1.80855007, -2.33706812, -1.75784549]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Uk8YKxlIFPBI",
        "colab_type": "code",
        "outputId": "9bf81731-1c2a-456b-cca0-ccc2f88cc469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "norm_dataset.describe()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>il08_09</th>\n",
              "      <th>vg04_05</th>\n",
              "      <th>fd03_04</th>\n",
              "      <th>la09_10</th>\n",
              "      <th>cg13_14</th>\n",
              "      <th>ja05_06</th>\n",
              "      <th>fj11_12</th>\n",
              "      <th>ec20_11</th>\n",
              "      <th>mb00_12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.500000e+01</td>\n",
              "      <td>5.500000e+01</td>\n",
              "      <td>5.500000e+01</td>\n",
              "      <td>5.500000e+01</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>5.300000e+01</td>\n",
              "      <td>5.500000e+01</td>\n",
              "      <td>5.500000e+01</td>\n",
              "      <td>5.500000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.018587e-17</td>\n",
              "      <td>-7.872491e-17</td>\n",
              "      <td>9.891078e-17</td>\n",
              "      <td>1.402918e-16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.021196e-17</td>\n",
              "      <td>1.564405e-17</td>\n",
              "      <td>2.018587e-18</td>\n",
              "      <td>-6.257621e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.451488e+00</td>\n",
              "      <td>-2.593019e+00</td>\n",
              "      <td>-2.014159e+00</td>\n",
              "      <td>-3.254150e+00</td>\n",
              "      <td>-1.684920</td>\n",
              "      <td>-1.451848e+00</td>\n",
              "      <td>-1.808550e+00</td>\n",
              "      <td>-2.337068e+00</td>\n",
              "      <td>-1.757845e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-5.402119e-01</td>\n",
              "      <td>-2.442216e-01</td>\n",
              "      <td>-8.006625e-01</td>\n",
              "      <td>-4.476486e-01</td>\n",
              "      <td>-0.771780</td>\n",
              "      <td>-9.535726e-01</td>\n",
              "      <td>-9.518839e-01</td>\n",
              "      <td>-5.363252e-01</td>\n",
              "      <td>-8.269057e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.895480e-02</td>\n",
              "      <td>2.695779e-01</td>\n",
              "      <td>8.923489e-02</td>\n",
              "      <td>1.264085e-01</td>\n",
              "      <td>-0.171718</td>\n",
              "      <td>7.856901e-02</td>\n",
              "      <td>1.633984e-01</td>\n",
              "      <td>2.354218e-01</td>\n",
              "      <td>-3.220102e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6.470959e-01</td>\n",
              "      <td>7.099775e-01</td>\n",
              "      <td>7.094664e-01</td>\n",
              "      <td>7.961419e-01</td>\n",
              "      <td>0.715332</td>\n",
              "      <td>8.259819e-01</td>\n",
              "      <td>1.003901e+00</td>\n",
              "      <td>7.284824e-01</td>\n",
              "      <td>8.079154e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2.355661e+00</td>\n",
              "      <td>1.003577e+00</td>\n",
              "      <td>2.516228e+00</td>\n",
              "      <td>1.593444e+00</td>\n",
              "      <td>2.385072</td>\n",
              "      <td>1.858124e+00</td>\n",
              "      <td>1.359498e+00</td>\n",
              "      <td>1.607416e+00</td>\n",
              "      <td>1.829679e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0       il08_09  ...       ec20_11       mb00_12\n",
              "count         0.0  5.500000e+01  ...  5.500000e+01  5.500000e+01\n",
              "mean          NaN -2.018587e-17  ...  2.018587e-18 -6.257621e-17\n",
              "std           NaN  1.000000e+00  ...  1.000000e+00  1.000000e+00\n",
              "min           NaN -2.451488e+00  ... -2.337068e+00 -1.757845e+00\n",
              "25%           NaN -5.402119e-01  ... -5.363252e-01 -8.269057e-01\n",
              "50%           NaN -1.895480e-02  ...  2.354218e-01 -3.220102e-02\n",
              "75%           NaN  6.470959e-01  ...  7.284824e-01  8.079154e-01\n",
              "max           NaN  2.355661e+00  ...  1.607416e+00  1.829679e+00\n",
              "\n",
              "[8 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qXtM3urlFPBK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "c8QhiewFFPBL",
        "colab_type": "code",
        "outputId": "7a3f0dec-d61c-4301-d05f-ff800a173f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "for annot in norm_dataset[norm_dataset.columns[2:]]:\n",
        "  norm_dataset[annot].hist()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAIcAAJFgANAoAhwAAgUAQ4Agfo/TCHAUDqov+YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjzZfdHFFuXn",
        "colab_type": "text"
      },
      "source": [
        "# Text features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EtW3d69yFPBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "\n",
        "dataset = get_dataset(get_medium(\"001\",updated_csv))\n",
        "norm_dataset,data_max, data_min = normalisation_annot(dataset)\n",
        "for annot in norm_dataset[norm_dataset.columns[2:]]:\n",
        "  norm_dataset[annot].hist()\n",
        "print(norm_dataset.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QnT-VRfeLpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m spacy download fr_core_news_md\n",
        "# Run this, then restart kernel before running rest of the notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXPU1GuryU-9",
        "colab_type": "text"
      },
      "source": [
        "### 1 - Data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBuAomOXeQnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re  \n",
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "import datetime\n",
        "from getpass import getpass\n",
        "import spacy\n",
        "import gensim\n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "import warnings\n",
        "import scipy\n",
        "import matplotlib as mlp\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "warnings.filterwarnings(\"ignore\",category = UserWarning)\n",
        "warnings.filterwarnings(\"ignore\",category = RuntimeWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYiV33FmeSz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete file before downloading data\n",
        "!rm -rf challenge-m2-sid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUA0DMuweZN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OS setup\n",
        "!cat /etc/os-release\n",
        "!apt-get install -qq bc tree sox\n",
        "\n",
        "# Data loading\n",
        "!git clone \"https://etudiantsid:etudiantsidPW;@gitlab.com/jeromefarinas/challenge-m2-sid.git\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuyScQ_xeaW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete file before downloading data\n",
        "!rm -rf Project-Archean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADb0Msdmec-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data from GitHub\n",
        "user = getpass('BitBucket user')\n",
        "password = getpass('BitBucket password')\n",
        "os.environ['GITHUB_AUTH'] = user + ':' + password\n",
        "\n",
        "!git clone https://$GITHUB_AUTH@github.com/vincentnam/Project-Archean.git\n",
        "!cd Project-Archean && git checkout Texte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln92uRP6egw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_media_type(annot, noteurs, media='audio'):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        DataFrame annot : csv des annotations (modifié)\n",
        "        list noteurs : liste des annotateurs\n",
        "        str media : nom du média ('audio'/'video'/'texte')\n",
        "    Returns:\n",
        "        DataFrame df_m  : csv qui pour chaque scene contenant le media donne le nombre et la moyenne des notes\n",
        "        DataFrame df_mo : csv qui pour chaque scene contenant uniquement le media, donne le nombre et la moyenne des notes\n",
        "    \"\"\"\n",
        "    \n",
        "    les_medias = set(['audio','video','texte'])\n",
        "    les_medias.difference(set([media]))\n",
        "    \n",
        "    filtres = {}\n",
        "    filtres['isRated'] = annot['isRated'] == 1\n",
        "    filtres[media]     = annot[media] == 1\n",
        "    f = pd.DataFrame(filtres).apply(sum, axis=1) == len(filtres)\n",
        "    df_media = annot.loc[f,:].copy()\n",
        "    \n",
        "    for m in les_medias.difference(set([media])):\n",
        "        filtres[m] = annot[m] == 0\n",
        "    f = pd.DataFrame(filtres).apply(sum, axis=1) == len(filtres)\n",
        "    df_media_only = annot.loc[f,:].copy()\n",
        "    \n",
        "    if len(df_media) > 0:\n",
        "        df_media['moyenne'] = df_media[noteurs].apply(np.nanmean, axis=1)\n",
        "        df_m = df_media[['doc','moyenne']].groupby(['doc']).agg(['count','mean']).droplevel(level=0, axis=1)\n",
        "        df_m = df_m.sort_values('doc').reset_index()\n",
        "        print(media,\" : \",      len(set(df_media['doc'])), sep=\"\")\n",
        "    else : df_m = None\n",
        "    \n",
        "    if len(df_media_only) > 0:\n",
        "        df_media_only['moyenne'] = df_media_only[noteurs].apply(np.nanmean, axis=1)\n",
        "        df_mo = df_media_only[['doc','moyenne']].groupby(['doc']).agg(['count','mean']).droplevel(level=0, axis=1)\n",
        "        df_mo = df_mo.sort_values('doc').reset_index()\n",
        "        print(media,\"_only : \", len(set(df_media_only['doc'])), sep=\"\")\n",
        "    else : df_mo = None\n",
        "    \n",
        "    return df_m, df_mo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4fJG660ehio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "label_normalised_origin = pd.read_csv('/content/Project-Archean/label_normalised_moncoucou.csv', sep=\",\", index_col=0, header=0)\n",
        "label_normalised_origin = label_normalised_origin.drop('\\t',axis=1)\n",
        "label_normalised, noteurs_normalised  = transform_annotation(label_normalised_origin)\n",
        "graded = label_normalised[(label_normalised['isRated'] == 1)].copy()\n",
        "\n",
        "text, text_only = get_media_type(graded, noteurs_normalised, media='texte')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7YVkIaye9L6",
        "colab_type": "text"
      },
      "source": [
        "### 2 -  Function that compares features with target\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dep0TVrMejGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Compares feature with target\n",
        "\n",
        "def compar_anno(dic, DF_cible):\n",
        "  \"\"\"\n",
        "    Plots a chart of feature as ordinate, and target as abscissa and prints \n",
        "    correlation between target and feature\n",
        "\n",
        "    Parameters:\n",
        "        :param dic: Dictionnary with documents as keys and a feature as value\n",
        "        :param DF_cible: The target values\n",
        "        :type dic: Dictionnary\n",
        "        :type DF_cible: DataFrame\n",
        "\n",
        "    Returns:\n",
        "         /\n",
        "  \"\"\"\n",
        "  DF_dic = pd.DataFrame.from_dict(dic, orient='index', columns=['dic'])\n",
        "  DF_dic['doc']  = DF_dic.index\n",
        "  DF_dic['doc'] = DF_dic['doc'].apply(lambda x : x[:-4])\n",
        "  DF_dic = DF_dic.set_index('doc')\n",
        "  DF_cible = DF_cible.set_index('doc')\n",
        "  index_cible = list(DF_cible.index)\n",
        "  DF_dic = DF_dic.loc[index_cible]\n",
        "  DF_total = DF_cible.join(DF_dic)\n",
        "  plt.scatter(DF_total['mean'], DF_total['dic'])\n",
        "  plt.xlabel('Target')\n",
        "  plt.ylabel('Feature')\n",
        "  plt.title('Correlation between feature and target')\n",
        "  plt.show()\n",
        "  corr = scipy.stats.spearmanr(DF_total['mean'], DF_total['dic'])[0]\n",
        "  print('Correlation between feature and target : ' + str(corr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHR2jU9vzQGJ",
        "colab_type": "text"
      },
      "source": [
        "### 3 - Getting clean words, sentences, lemmas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnB0JtWZengi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting a list of the documents text\n",
        "\n",
        "path_text = 'challenge-m2-sid/corpus/text/'\n",
        "List_txt = os.listdir(path_text)\n",
        "List_txt.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mdi9xoTgv4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Gets the sentences of each documents as strings\n",
        "\n",
        "def get_sentences(List_txt, path_text):\n",
        "  \"\"\"\n",
        "    Gets the sentences of each documents as strings by reading xml documents\n",
        "\n",
        "    Parameters:\n",
        "        :param List_txt: List of textual documents (names)\n",
        "        :param path_text: path to find the textual documents\n",
        "        :type List_txt: list\n",
        "        :type path_text: string\n",
        "    \n",
        "    Returns:\n",
        "        dic_docs: Dictionnary with documents as keys, and sentences of documents\n",
        "                 as values\n",
        "        type : Dictionnary\n",
        "  \"\"\"\n",
        "  dic_docs = {}\n",
        "  for doc in List_txt:\n",
        "    tree = ET.parse(path_text + doc)\n",
        "    root = tree.getroot()\n",
        "    dic_docs[doc] = []\n",
        "    for s in root:\n",
        "      sentence = ''\n",
        "      for w in s:\n",
        "        word = w.text\n",
        "        if (word is not None):\n",
        "          sentence = sentence + word\n",
        "      dic_docs[doc].append(sentence)\n",
        "  return(dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CU5-6O7q3k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Gets clean sentences\n",
        "\n",
        "def clean_sentences(dic_docs):  \n",
        "  \"\"\"\n",
        "    Deletes characters as punctuation, except '-'\n",
        "\n",
        "    Parameters:\n",
        "        :param dic_docs: Dictionnary out of 'get_sentences' function\n",
        "        :type dic_docs: Dictionnary\n",
        "        \n",
        "    Returns:\n",
        "        dic_docs: Dictionnary with documents as keys, and sentences of documents\n",
        "                 as values\n",
        "        type : Dictionnary\n",
        "  \"\"\"\n",
        "  for key in dic_docs.keys() : \n",
        "    list_new = []\n",
        "    for sentence in dic_docs[key]:\n",
        "      sentence = sentence.replace(\"'\", ' ').replace(\"’\", ' ')\n",
        "      sentence = re.sub(\"([^\\s\\w\\-])\", '',sentence)\n",
        "      list_new.append(sentence)\n",
        "    dic_docs[key] = list_new\n",
        "  return(dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvcRbvexwluh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Gets clean words\n",
        "\n",
        "def get_clean_words(dic_docs, mode = 'All'):\n",
        "  \"\"\"\n",
        "    Deletes words as blancs, or one letter words, or None\n",
        "    \n",
        "    Parameters:\n",
        "        :param dic_docs: Dictionnary out of 'clean_sentences' function\n",
        "        :param mode: mode of execution, getting clean words as sentences \n",
        "                     or list of words\n",
        "        :type dic_docs: Dictionnary\n",
        "        :type mode: string\n",
        "\n",
        "    Returns:\n",
        "        dic_docs: Dictionnary with documents as keys, and clean words as values\n",
        "        type : Dictionnary\n",
        "  \"\"\"\n",
        "  if mode == 'All':\n",
        "    for key in dic_docs.keys() : \n",
        "      list_words = []\n",
        "      for sentence in dic_docs[key]:\n",
        "        for word in sentence.split():\n",
        "            w = word.replace(' ', '')\n",
        "            if len(w) != 0:\n",
        "              list_words.append(w.lower())\n",
        "      dic_docs[key] = list_words\n",
        "  if mode == 'Sentences':\n",
        "    for key in dic_docs.keys() : \n",
        "        list_words = []\n",
        "        for sentence in dic_docs[key]:\n",
        "          list_words_sent = []\n",
        "          for word in sentence.split():\n",
        "              w = word.replace(' ', '')\n",
        "              if len(w) > 1:\n",
        "                list_words_sent.append(w.lower())\n",
        "          list_words.append(list_words_sent)\n",
        "        dic_docs[key] = list_words\n",
        "  return(dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qFaa2xt7LR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download French language modele, and french StopWords\n",
        "\n",
        "nlp = spacy.load('fr_core_news_md')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXZYSjs7Ogj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gets lemmas of each word\n",
        "\n",
        "def get_lemmatize(dic_docs):\n",
        "  \"\"\"\n",
        "    Lowers words and replace them by their lemma if they are longer than\n",
        "    1 letter, for each word of each document\n",
        "\n",
        "    Parameters:\n",
        "        :param dic_docs: Dictionnary out of 'get_clean_words' \n",
        "                         function (sentences mode)\n",
        "        :type dic_docs: Dictionnary\n",
        "    \n",
        "    Returns:\n",
        "        dic_lemma: Dictionnary with documents as keys, and lemmatize sentences \n",
        "                   as values\n",
        "        type : Dictionnary\n",
        "  \"\"\"\n",
        "\n",
        "  stop_words = set(stopwords.words('french')) \n",
        "  dic_lemma = {}\n",
        "  i = 1\n",
        "  N = len(dic_docs)\n",
        "  for doc in dic_docs.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    list_doc = []\n",
        "    for sentence in dic_docs[doc]:\n",
        "      list_sent = []\n",
        "      filtered_sentence = [w.lower() for w in sentence if w not in stop_words] \n",
        "      sentence_clean = ' '.join(w for w in filtered_sentence)\n",
        "      sentence_nlp = nlp(sentence_clean)\n",
        "      for token in sentence_nlp:\n",
        "        if len(token.lemma_) > 1 :\n",
        "          list_sent.append(token.lemma_)\n",
        "      list_doc.append(list_sent)\n",
        "    dic_lemma[doc] = list_doc\n",
        "    i = i + 1\n",
        "  return(dic_lemma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45_Zfg7WiDSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_docs = get_clean_words(clean_sentences(get_sentences(List_txt, path_text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKHnHmVHCF1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Clean words for 226_6 : ')\n",
        "print(dic_docs['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwAIgs3n-lkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_lemma = get_lemmatize(get_clean_words(clean_sentences(get_sentences(List_txt, path_text)), 'Sentences'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3JgZjMCOlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Lemmas for 226_6 : ')\n",
        "print(dic_lemma['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFaXktLP3Ca1",
        "colab_type": "text"
      },
      "source": [
        "### Feature 1 : number of low frequencie words per video "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hcNgsHHiY4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Read excel file\n",
        "\n",
        "def read_excel(file):\n",
        "  \"\"\"\n",
        "    Read the excel file, computes a normalized frequency, keeps only some of\n",
        "    the columns (word, lemma, pos_tag, frequency, number of syllable)\n",
        "    \n",
        "    Parameters:\n",
        "        :param file: Path leading to the Excel file\n",
        "        :type file: string\n",
        "    \n",
        "    Returns:\n",
        "        df_lex:  DataFrame of excel file, minus some columns\n",
        "        type : DataFrame\n",
        "  \"\"\"\n",
        "  dfs = pd.ExcelFile(file)\n",
        "  sh = dfs.sheet_names[0]\n",
        "  df_lex = dfs.parse('Sheet1')\n",
        "  df_lex = df_lex[['ortho', 'lemme', 'cgram', 'freqfilms2', 'nbsyll']]\n",
        "  serie = df_lex['freqfilms2']\n",
        "  normalized_serie=(serie)/max(serie)\n",
        "  df_lex['freqfilms2_norm'] = normalized_serie\n",
        "  return(df_lex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH1p2o6uiy_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lex = read_excel('Project-Archean/Lexique-query.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcH1yHkO8bwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pba9u33B7Yve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Computes complexity of a document\n",
        "\n",
        "def get_complexity_doc(doc, df_lex, dic_docs):\n",
        "  \"\"\"\n",
        "    Computes complexity of a document, by getting ratio of the number of words\n",
        "    with a small frequency on total number of words\n",
        "    \n",
        "    Parameters:\n",
        "        :param doc: Name of a document\n",
        "        :param df_lex: Dataframe out of 'read_excel' function\n",
        "        :param dic_docs: Dictionnary out of 'get_clean_words'\n",
        "                         function (All words mode)\n",
        "        :type doc: string\n",
        "        :type df_lex: Dataframe\n",
        "        :type dic_docs: Dictionnary\n",
        "    \n",
        "    Returns:\n",
        "        cplxty: Complexity of the document \n",
        "        type : float\n",
        "  \"\"\"\n",
        "  cplxty = 0\n",
        "  list_words = list(set(list(dic_docs[doc])))\n",
        "  for word in list_words:\n",
        "    try : \n",
        "      freq = max(df_lex[df_lex['ortho']==word]['freqlemfilms2_norm'])\n",
        "    except :\n",
        "      if len(word) > 3:\n",
        "        freq = 0\n",
        "      else : \n",
        "        freq = 1\n",
        "    if freq < 0.0001:\n",
        "      cplxty = cplxty + 1\n",
        "  cplxty = cplxty/(len(list_words))\n",
        "  return(cplxty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_WQRJpCj2fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Gets all the complexities\n",
        "\n",
        "def get_all_cplx(df_lex, dic_docs):\n",
        "  \"\"\"\n",
        "    Gets all the complexities by calling 'get_complexity_doc' function\n",
        "    \n",
        "    Parameters:\n",
        "        :param df_lex: Dataframe out of 'read_excel' function\n",
        "        :param dic_docs: Dictionnary out of 'get_clean_words'\n",
        "                         function (All words mode)\n",
        "        :type df_lex: Dataframe\n",
        "        :type dic_docs: Dictionnary\n",
        "    \n",
        "    Returns:\n",
        "        cplxty: Dictionnary with documents as keys, and complexity as values\n",
        "        type : Dictionnary\n",
        "  \"\"\"\n",
        "  dic_cplx = {}\n",
        "  i = 1\n",
        "  N = len(dic_docs.keys())\n",
        "  for doc in dic_docs.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    dic_cplx[doc] = get_complexity_doc(doc, df_lex, dic_docs)\n",
        "    i = i + 1\n",
        "  return(dic_cplx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlRalZfTjvj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_cplx = get_all_cplx(df_lex, dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkdDeV_kR1JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Lexical complexity for 226_6 : ')\n",
        "print(dic_cplx['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5G43IT5qSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_cplx, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNAby9i09oN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_docs_sent = clean_sentences(get_sentences(List_txt, path_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSgyxSc23jIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Gets the duration of each document\n",
        "\n",
        "def get_len_video(List_txt, path_text):\n",
        "  \"\"\"\n",
        "  Gets the duration of each document by time codes (last minus first), in second\n",
        "\n",
        "    Parameters:\n",
        "        :param List_txt: List of textual documents (names)\n",
        "        :param path_text: path to find the textual documents\n",
        "        :type List_txt: list\n",
        "        :type path_text: string\n",
        "\n",
        "    Returns:\n",
        "        dic_doc_len_video: Dictionnary with documents as keys, and time of \n",
        "                           documents as values\n",
        "        type : Dictionnary\n",
        "  \"\"\"\n",
        "  dic_doc_len_video = {}\n",
        "  for doc in List_txt:\n",
        "    tree = ET.parse(path_text + doc)\n",
        "    root = tree.getroot()\n",
        "    ma = int(max([root[i].attrib['id'] for i in range(len(root))]))\n",
        "    start = root[0][0].attrib['value'][:8]\n",
        "    end = root[ma-1][-1].attrib['value'][:8]\n",
        "    format_ = '%H:%M:%S'\n",
        "    startDateTime = datetime.datetime.strptime(start, format_)\n",
        "    endDateTime = datetime.datetime.strptime(end, format_)\n",
        "    diff = endDateTime - startDateTime\n",
        "    time_s = diff.total_seconds()\n",
        "    dic_doc_len_video[doc] = time_s\n",
        "  return(dic_doc_len_video)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqCTnRzweFFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Gets the mean length of sentences and the number of sentences per minute\n",
        "\n",
        "def get_length_sentences_nb(dic_docs_sent, dic_doc_len_video):\n",
        "  \"\"\"\n",
        "    Computes the mean length of sentences for each document (AVG), \n",
        "    and the number of sentences per minute for each document, thanks to the\n",
        "    duration of subtitles\n",
        "\n",
        "    Parameters:\n",
        "        :param dic_docs_sent: Dictionnary out of 'clean_sentences' function\n",
        "        :param dic_doc_len_video: Dictionnary out of 'get_len_video' function \n",
        "        :type dic_docs_sent: Dictionnary\n",
        "        :type dic_doc_len_video: Dictionnary\n",
        "    \n",
        "    Returns:\n",
        "        dic_len_sentence: Dictionnary with documents as keys, and mean sentences\n",
        "                          length as values \n",
        "        dic_nb_sentence : Dictionnary with documents as keys, and number of \n",
        "                          sentence per minute as values \n",
        "        type dic_len_sentence: Dictionnary\n",
        "        type dic_nb_sentence: Dictionnary\n",
        "  \"\"\"\n",
        "  dic_len_sentence = {}\n",
        "  dic_nb_sentence =  {}\n",
        "  for doc in dic_docs_sent.keys():\n",
        "    time = dic_doc_len_video[doc]\n",
        "    nb_s = len(dic_docs_sent[doc])\n",
        "    s_per_min = 60*nb_s/time\n",
        "    mean_len_s = np.mean([len(s.split()) for s in dic_docs_sent[doc]])\n",
        "    dic_len_sentence[doc] = mean_len_s\n",
        "    dic_nb_sentence[doc] = s_per_min\n",
        "  return(dic_len_sentence, dic_nb_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkgK9wgVDPLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_doc_len_video = get_len_video(List_txt, path_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9vKEYetSBOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Lenght of video (in s) for 226_6 : ')\n",
        "print(dic_doc_len_video['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpxSNs_L5DkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_len_sentence, dic_nb_sentence = get_length_sentences_nb(dic_docs_sent,\n",
        "                                                            dic_doc_len_video)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-2qjiK9SKVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Average length of sentences for 226_6 : ')\n",
        "print(dic_len_sentence['226_6.xml'])\n",
        "print('Number of sentences per minute for 226_6 : ')\n",
        "print(dic_nb_sentence['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkVbvHW95uEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_len_sentence, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uw8S1En5zKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_nb_sentence, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTMLtBiK-58E",
        "colab_type": "text"
      },
      "source": [
        "### Feature 4 : word repetition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVS4fTpvLfQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Computes the rate of different words\n",
        "\n",
        "def get_repetition_ratio_doc(doc, dic_docs):\n",
        "  \"\"\"\n",
        "      Computes the ratio between number of unique words and total number of\n",
        "      words\n",
        "\n",
        "      Parameters:\n",
        "          :param doc: Name of a document\n",
        "          :param dic_docs: Dictionnary out of 'get_clean_words' function \n",
        "          :type doc: string\n",
        "          :type dic_docs: Dictionnary\n",
        "\n",
        "      Returns:\n",
        "          rep: ratio of different words on total  number of words\n",
        "          type : float\n",
        "    \"\"\"\n",
        "  list_words_dif = list(set(dic_docs[doc]))\n",
        "  list_words = dic_docs[doc]\n",
        "  rep = len(list_words_dif)/len(list_words)\n",
        "  return(rep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGuhlx9Mm4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#  Gets all repetition rates\n",
        "\n",
        "def get_all_rep (dic_docs):\n",
        "  \"\"\"\n",
        "      Gets all repetition rates by calling 'get_repetition_ratio_doc' function\n",
        "\n",
        "      Parameters:\n",
        "          :param dic_docs: Dictionnary out of 'get_clean_words' function \n",
        "          :type dic_docs: Dictionnary\n",
        "\n",
        "      Returns:\n",
        "          dic_repetition: Dictionnary with documents as keys, and repetition\n",
        "                          rate as values\n",
        "          type : Dictionnary\n",
        "    \"\"\"\n",
        "  dic_repetition = {}\n",
        "  for doc in dic_docs.keys():\n",
        "    dic_repetition[doc] = get_repetition_ratio_doc(doc, dic_docs)\n",
        "  return(dic_repetition)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHA_Cg02M07F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_repetition = get_all_rep (dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJKxGmCHTRfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Repetition ratio for 226_6 : ')\n",
        "print(dic_repetition['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZquP2iO53xY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_repetition, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE7clVBBO8PB",
        "colab_type": "text"
      },
      "source": [
        "### Feature 5 : Number of syllable for 100 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g90mW8leTyrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Computes the number of syllable for 100 words \n",
        "\n",
        "def nb_syll_100 (dic_docs, df_lex):\n",
        "  \"\"\"\n",
        "      Computes the number of syllable for 100 words for each document, thanks to \n",
        "      length of document (number of words), and number of syllable.\n",
        "\n",
        "      Parameters:\n",
        "          :param dic_docs: Dictionnary out of 'get_clean_words' function \n",
        "          :param df_lex: DataFrame out of 'read_excel' function\n",
        "          :type dic_docs: Dictionnary\n",
        "          :type df_lex: DataFrame\n",
        "          \n",
        "      Returns:\n",
        "          dic_syll_per_100: Dictionnary with documents as keys, and number of \n",
        "                            syllable for 100 words as values\n",
        "          type : Dictionnary\n",
        "    \"\"\"\n",
        "  dic_syll_per_100 = {}\n",
        "  i = 1\n",
        "  N = len(dic_docs)\n",
        "  m = np.mean(df_lex['nbsyll'])\n",
        "  list_words = list(set(list(df_lex['ortho'])))\n",
        "  for doc in dic_docs.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    syll = 0\n",
        "    nb_word = len(dic_docs[doc])\n",
        "    syll = sum([int(max(df_lex[df_lex['ortho']==w]['nbsyll'])) for w in dic_docs[doc] if w in list_words])\n",
        "    syll = syll + sum([m for w in dic_docs[doc] if w not in list_words])\n",
        "    ratio = 100 * syll / nb_word \n",
        "    dic_syll_per_100[doc] = ratio\n",
        "    i = i + 1\n",
        "  return(dic_syll_per_100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9ClY2y3WmMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_syll_per_100 = nb_syll_100 (dic_docs, df_lex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BedmWESlTfh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Number of syllable for 100 words for 226_6 : ')\n",
        "print(dic_syll_per_100['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qelOT6DK58IS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_syll_per_100, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJ7QwFiorOA",
        "colab_type": "text"
      },
      "source": [
        "### Feature 6 : Dispersion of words belonging to the 4 main topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Z-tsfCosxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Computes a list of words belonging to 4 topic per document\n",
        "\n",
        "def get_topics (dic_lemma):\n",
        "  \"\"\"\n",
        "      Computes the 4 main topics for each documents, with LDA model, and then\n",
        "       gets for each topic a list of the words that are part of this topic\n",
        "\n",
        "      Parameters:\n",
        "          :param dic_lemma: Dictionnary out of 'get_lemmatize' function \n",
        "          :type dic_lemma: Dictionnary\n",
        "\n",
        "      Returns:\n",
        "          dic_topics: Dictionnary with documents as keys, and one dictionnary \n",
        "                      for each topic as value. Each dictionnary has number of \n",
        "                      topic as key and list of words that are part of this topic\n",
        "                      as value\n",
        "          type : Dictionnary\n",
        "    \"\"\"\n",
        "  dic_topics = {}\n",
        "  i=1\n",
        "  N = len(dic_lemma)\n",
        "  for doc in dic_lemma.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    dictionary = gensim.corpora.Dictionary(dic_lemma[doc])\n",
        "    bow_corpus = [dictionary.doc2bow(s) for s in dic_lemma[doc]]\n",
        "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 4, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)\n",
        "    dic_topics_doc = {}\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "      topics = topic.split('\"')\n",
        "      list_topic_i = []\n",
        "      for j in range(1, len(topics), 2):\n",
        "        list_topic_i.append(topics[j])\n",
        "      dic_topics_doc[idx] = list_topic_i\n",
        "    dic_topics[doc] = dic_topics_doc\n",
        "    i = i + 1\n",
        "  return(dic_topics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsRjjz7CzRxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_topics = get_topics (dic_lemma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hgpXHgITqdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('4 main topics for 226_6 : ')\n",
        "print(dic_topics['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxhnPaIJ4vUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Computes the over-dispersed topic rate\n",
        "\n",
        "def topic_in_time(dic_topics, dic_lemma, test, list_doc = []):\n",
        "  \"\"\"\n",
        "      Computes the VMR rate (variance/mean) for each topic of each document and \n",
        "      gets the ratio of topics with VMR > 1 on number of topics if test is False\n",
        "      OR plots the dispersion of topics for 4 representative documents if \n",
        "      test is True\n",
        "\n",
        "      Parameters:\n",
        "          :param dic_topics: Dictionnary out of 'get_topics' function \n",
        "          :param dic_lemma: Dictionnary out of 'get_lemmatize' function\n",
        "          :param test: Boolean value, to know if it has to compute all documents\n",
        "                       or example ones\n",
        "          :param list_doc: list of documents for plot, only used if test is True\n",
        "          :type dic_topics: Dictionnary\n",
        "          :type dic_lemma: Dictionnary\n",
        "          :type test: Boolean\n",
        "          :type test: list\n",
        "\n",
        "      Returns:\n",
        "        if test is False :\n",
        "          dic_syll_per_100: Dictionnary with documents as keys, and ratio of \n",
        "                            over-dispersed topic on total number of topics\n",
        "                            as values\n",
        "          type : Dictionnary\n",
        "        if test is True :\n",
        "          /\n",
        "    \"\"\"\n",
        "\n",
        "  if test is False :\n",
        "    dic_time_topic = {}\n",
        "    for doc in dic_topics.keys():\n",
        "      lemmas = dic_lemma[doc]\n",
        "      cpt_disp = 0\n",
        "      for topic in dic_topics[doc].keys():\n",
        "        index = []\n",
        "        for w in dic_topics[doc][topic]:\n",
        "          index.extend([i for i, n in enumerate(lemmas) if w in n])\n",
        "        VMR = np.var(index)/np.mean(index)\n",
        "        if VMR > 1:\n",
        "          cpt_disp = cpt_disp + 1\n",
        "      dic_time_topic[doc] = cpt_disp/4\n",
        "    return(dic_time_topic) \n",
        "\n",
        "  else : \n",
        "    colors = ['blue', 'green', 'red', 'yellow']\n",
        "    for doc in list_doc:\n",
        "      lemmas = dic_lemma[doc]\n",
        "      plt.figure()\n",
        "      for topic in dic_topics[doc].keys():\n",
        "        index = []\n",
        "        for w in dic_topics[doc][topic]:\n",
        "          index.extend([i for i, n in enumerate(lemmas) if w in n])\n",
        "        plt.scatter(index, [i for i in range(len(index))],\n",
        "                    label = 'topic ' + str(topic),\n",
        "                    color = colors[int(topic)])\n",
        "      plt.legend()\n",
        "      plt.title('Dispersion of topics in document ' + doc)\n",
        "      plt.xlabel('Position of word in document')\n",
        "      plt.ylabel('Does not matter')\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkcpAc0E_OQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_time_topic = topic_in_time(dic_topics, dic_lemma, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OxTzdbvT2aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Dispersion rate of topics for 226_6 : ')\n",
        "print(dic_time_topic['226_6.xml'])\n",
        "topic_in_time(dic_topics, dic_lemma, True, ['226_6.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63drTZ9_8iz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of the 4 level of dispersion\n",
        "# Here we can see 4 documents labelized from 0.25 to 1 (in this order)\n",
        "\n",
        "topic_in_time(dic_topics, dic_lemma, True, ['113_11.xml', '184_15.xml', '160_9.xml', '124_13.xml'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQWyY_w6K6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_time_topic, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-Foc-BqJk3",
        "colab_type": "text"
      },
      "source": [
        "## 4 - Features agregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bNPYhIJ3Nno",
        "colab": {}
      },
      "source": [
        "\n",
        "# Aggregates all features\n",
        "\n",
        "def create_DF_agreg(dic_nb_sentence,\n",
        "                    dic_len_sentence,\n",
        "                    dic_cplx,\n",
        "                    dic_syll_per_100,\n",
        "                    dic_repetition,\n",
        "                    dic_time_topic):\n",
        "  \"\"\"\n",
        "      Aggregates all features for every documents in one DataFrame\n",
        "\n",
        "      Parameters:\n",
        "          :param dic_nb_sentence: Dictionnary out of 'dic_nb_sentence' function\n",
        "          :param dic_len_sentence: Dictionnary out of 'dic_len_sentence' \n",
        "                                  function\n",
        "          :param dic_cplx: Dictionnary out of 'get_all_cplx' function \n",
        "          :param dic_syll_per_100: Dictionnary out of 'nb_syll_100' function \n",
        "          :param dic_repetition:  Dictionnary out of 'dic_repetition' function \n",
        "          :param dic_time_topic: Dictionnary out of 'topic_in_time' function  \n",
        "          :type dic_nb_sentence: Dictionnary\n",
        "          :type dic_len_sentence: Dictionnary\n",
        "          :type dic_cplx: Dictionnary\n",
        "          :type dic_syll_per_100: Dictionnary\n",
        "          :type dic_repetition: Dictionnary\n",
        "          :type dic_time_topic: Dictionnary\n",
        "          \n",
        "      Returns:\n",
        "          DF: DataFrame with column for documents, and others for features\n",
        "          type : DataFrame\n",
        "    \"\"\"\n",
        "  col = ['doc',\n",
        "         'nb_sentence',\n",
        "         'len_sentence',\n",
        "         'cplx_words',\n",
        "         'syll_100',\n",
        "         'different_words',\n",
        "         'topic']\n",
        "  list_DF = []\n",
        "  for doc in dic_nb_sentence.keys():\n",
        "    list_DF_doc = [doc[:-4],\n",
        "                   dic_nb_sentence[doc],\n",
        "                   dic_len_sentence[doc],\n",
        "                   dic_cplx[doc],\n",
        "                   dic_syll_per_100[doc],\n",
        "                   dic_repetition[doc],\n",
        "                   dic_time_topic[doc]]\n",
        "    list_DF.append(list_DF_doc)\n",
        "  DF = pd.DataFrame(list_DF, columns=col)\n",
        "  return(DF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k9-NVtsuXLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DF_aggreg = create_DF_agreg(dic_nb_sentence,\n",
        "                    dic_len_sentence,\n",
        "                    dic_cplx,\n",
        "                    dic_syll_per_100,\n",
        "                    dic_repetition,\n",
        "                    dic_time_topic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8EfGxpHUImo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example on video 226_6 : Cyrano de Bergerac\n",
        "\n",
        "print('Features for 226_6 : ')\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(DF_aggreg[DF_aggreg['doc'] == '226_6'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qka8vfAubTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DF_aggreg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ipNpzonH7EQ3",
        "colab": {}
      },
      "source": [
        "def get_X_y_model(DF_aggreg, DF_cible):\n",
        "  DF_aggreg = DF_aggreg.set_index('doc')\n",
        "  DF_cible = DF_cible.set_index('doc')\n",
        "  index_cible = list(DF_cible.index)\n",
        "  DF_aggreg = DF_aggreg.loc[index_cible]\n",
        "  DF_total = DF_cible.join(DF_aggreg)\n",
        "  X = DF_total[['nb_sentence',\n",
        "          'len_sentence',\n",
        "          'cplx_words',\n",
        "          'syll_100',\n",
        "          'different_words',\n",
        "          'topic']]\n",
        "\n",
        "  y = DF_total['mean']\n",
        "  return(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3emRt6mH_XlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X,y = get_X_y_model(DF_aggreg, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlHLV-cF7lIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
        "scaler.fit(X)\n",
        "DF_scaled = pd.DataFrame(scaler.transform(X),\n",
        "                         index = X.index, columns = list(X))\n",
        "DF_scaled['mean'] = y\n",
        "\n",
        "X_N = DF_scaled[['nb_sentence',\n",
        "         'len_sentence',\n",
        "         'cplx_words',\n",
        "         'syll_100',\n",
        "         'different_words',\n",
        "         'topic']]\n",
        "y_N = DF_scaled['mean']\n",
        "reg = Lasso(alpha = 0.07)\n",
        "reg.fit(X_N, y_N)\n",
        "pred = reg.predict(X_N)\n",
        "coef = pd.Series(reg.coef_, index = X_N.columns)\n",
        "imp_coef = coef.sort_values()\n",
        "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
        "plt.scatter(y_N, pred)\n",
        "plt.xlim(-2,2)\n",
        "plt.ylim(-2,2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwlOM49EZ_nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corr = DF_scaled.corr()\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
        "fig.colorbar(cax)\n",
        "ticks = np.arange(0,len(DF_scaled.columns),1)\n",
        "ax.set_xticks(ticks)\n",
        "plt.xticks(rotation=90)\n",
        "ax.set_yticks(ticks)\n",
        "ax.set_xticklabels(DF_scaled.columns)\n",
        "ax.set_yticklabels(DF_scaled.columns)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eB5GArI5XN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp_coef = coef.sort_values()\n",
        "import matplotlib\n",
        "imp_coef.plot(kind = \"barh\")\n",
        "plt.title(\"Feature importance using Lasso Model\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHRyl7ydFzFc",
        "colab_type": "text"
      },
      "source": [
        "# Audio features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N_Z1AZOF1FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyHYm4KiF2KD",
        "colab_type": "text"
      },
      "source": [
        "# Video features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc43AdbFF3T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnx9UQULF3fH",
        "colab_type": "text"
      },
      "source": [
        "# Agregation models\n",
        "The models works on csv file from features extractions (some features take lot of time to be computed. To make it possible, we'll work directly with csv results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk02dN5ruBGJ",
        "colab_type": "code",
        "outputId": "f76df436-8c28-442f-a5f0-bb21b89cab8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "# Create an aggregation DataFrame of features\n",
        "def create_features_dataframe(text_path ,audio_path, video_path ):\n",
        "    \"\"\"\n",
        "  # Create an aggregation DataFrame of each medium features to create a dataset\n",
        "  # for the learning model. Each medium is merge on document column.\n",
        "  Parameters:\n",
        "      :param text_path: path to the csv containing calculated text features \n",
        "      :param audio_path: path to the csv containing calculated audio features\n",
        "      :param video_path: path to the csv containing calculated video features\n",
        "      :type text_path: string\n",
        "      :type audio_path: string\n",
        "      :type video_path: string\n",
        "  \n",
        "  Returns:\n",
        "      audio_video_text: DataFrame with all the features concatenate in the same\n",
        "      object\n",
        "      type : DataFrame (pandas)\n",
        "  Other itema to note:\n",
        "    - All medium are merge together. It may be more accurate for the problem\n",
        "    merge text features only with text documents, audio text features only with\n",
        "    audio documents, etc...\n",
        "  :Example:\n",
        "\n",
        "      >>> print(create_features_dataframe(\"Text_Features_6.csv\",\n",
        "      \"./silence_rolling_mean_new.csv\",\"feat_break.csv\"))\n",
        "\tcode_doc\tSR\tSNR\tVBR\tCONF\tRecognition score\tenv_br_per_min\tscene_br_per_min\tnb_sentence\tlen_sentence\tcplx_words\tsyll_100\tdifferent_words\ttopic\n",
        "0\t100_1\t0.506749\t0.975847\t0.937432\t0.891960\t46.659483\t11.789474\t15.157895\t54.545455\t10.350000\t0.753968\t138.665032\t0.608696\t0.75\n",
        "1\t107_7\t0.875052\t0.992874\t0.996106\tNaN\t17.618720\t1.441441\t2.882883\t44.000000\t14.681818\t0.789474\t137.962893\t0.470588\t1.00\n",
        "        ... \n",
        "  \"\"\"\n",
        "  text_feat = pd.read_csv(text_path)\n",
        "  # Audio csv is build with special separator and encoding\n",
        "  audio_feat = pd.read_csv(audio_path, sep='§', engine='python', index_col=0, encoding='utf-8') \n",
        "  audio_feat[\"SCORE\"] = (1-audio_feat[\"SCORE\"])*100\n",
        "  video_feat= pd.read_csv(\"feat_break.csv\")\n",
        "  audio_video = pd.merge(audio_feat,video_feat,left_on=\"SCENE\",right_on=\"Unnamed: 0\")\n",
        "  audio_video = audio_video.drop([\"Unnamed: 0\"], axis=1)\n",
        "  audio_video = audio_video.rename(columns={\"SCENE\":\"code_doc\", \"SCORE\": \"Recognition score\"})\n",
        "  audio_video = audio_video.drop([\"RECON\",\"XML\"],axis=1)\n",
        "  audio_video_text = pd.merge(audio_video,text_feat,left_on=\"code_doc\",right_on=\"doc\")\n",
        "  audio_video_text = audio_video_text.drop([\"Unnamed: 0\",\"doc\"],axis=1)\n",
        "  return audio_video_text\n",
        "\n",
        "\n",
        "features = create_features_dataframe(\"Text_Features_6.csv\",\"./silence_rolling_mean_new.csv\",\"feat_break.csv\")\n",
        "features"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_doc</th>\n",
              "      <th>SR</th>\n",
              "      <th>SNR</th>\n",
              "      <th>VBR</th>\n",
              "      <th>CONF</th>\n",
              "      <th>Recognition score</th>\n",
              "      <th>env_br_per_min</th>\n",
              "      <th>scene_br_per_min</th>\n",
              "      <th>nb_sentence</th>\n",
              "      <th>len_sentence</th>\n",
              "      <th>cplx_words</th>\n",
              "      <th>syll_100</th>\n",
              "      <th>different_words</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100_1</td>\n",
              "      <td>0.506749</td>\n",
              "      <td>0.975847</td>\n",
              "      <td>0.937432</td>\n",
              "      <td>0.891960</td>\n",
              "      <td>46.659483</td>\n",
              "      <td>11.789474</td>\n",
              "      <td>15.157895</td>\n",
              "      <td>54.545455</td>\n",
              "      <td>10.350000</td>\n",
              "      <td>0.753968</td>\n",
              "      <td>138.665032</td>\n",
              "      <td>0.608696</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>107_7</td>\n",
              "      <td>0.875052</td>\n",
              "      <td>0.992874</td>\n",
              "      <td>0.996106</td>\n",
              "      <td>NaN</td>\n",
              "      <td>17.618720</td>\n",
              "      <td>1.441441</td>\n",
              "      <td>2.882883</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>14.681818</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>137.962893</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>108_3</td>\n",
              "      <td>0.610753</td>\n",
              "      <td>0.950284</td>\n",
              "      <td>0.992284</td>\n",
              "      <td>0.892878</td>\n",
              "      <td>46.724891</td>\n",
              "      <td>15.678392</td>\n",
              "      <td>12.060302</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>7.333333</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>145.640040</td>\n",
              "      <td>0.696970</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>128_5</td>\n",
              "      <td>0.636332</td>\n",
              "      <td>0.998066</td>\n",
              "      <td>0.989983</td>\n",
              "      <td>NaN</td>\n",
              "      <td>52.879581</td>\n",
              "      <td>2.926829</td>\n",
              "      <td>1.951220</td>\n",
              "      <td>29.142857</td>\n",
              "      <td>9.117647</td>\n",
              "      <td>0.767857</td>\n",
              "      <td>129.190227</td>\n",
              "      <td>0.722581</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13_2</td>\n",
              "      <td>0.784851</td>\n",
              "      <td>0.995663</td>\n",
              "      <td>0.932309</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.962963</td>\n",
              "      <td>3.720930</td>\n",
              "      <td>3.720930</td>\n",
              "      <td>23.076923</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>0.802632</td>\n",
              "      <td>145.396761</td>\n",
              "      <td>0.767677</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>96_6</td>\n",
              "      <td>0.761384</td>\n",
              "      <td>0.900388</td>\n",
              "      <td>0.724127</td>\n",
              "      <td>0.909442</td>\n",
              "      <td>10.033445</td>\n",
              "      <td>8.470588</td>\n",
              "      <td>4.235294</td>\n",
              "      <td>24.827586</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>0.765957</td>\n",
              "      <td>177.835900</td>\n",
              "      <td>0.746032</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>97_6</td>\n",
              "      <td>0.745310</td>\n",
              "      <td>0.962507</td>\n",
              "      <td>0.881788</td>\n",
              "      <td>0.896575</td>\n",
              "      <td>65.192582</td>\n",
              "      <td>2.440678</td>\n",
              "      <td>2.440678</td>\n",
              "      <td>34.285714</td>\n",
              "      <td>6.950000</td>\n",
              "      <td>0.734694</td>\n",
              "      <td>152.829136</td>\n",
              "      <td>0.705036</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>98_13</td>\n",
              "      <td>0.661133</td>\n",
              "      <td>0.971335</td>\n",
              "      <td>0.997959</td>\n",
              "      <td>0.908270</td>\n",
              "      <td>37.795276</td>\n",
              "      <td>27.692308</td>\n",
              "      <td>27.692308</td>\n",
              "      <td>26.400000</td>\n",
              "      <td>8.818182</td>\n",
              "      <td>0.743243</td>\n",
              "      <td>154.891601</td>\n",
              "      <td>0.762887</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>99_11</td>\n",
              "      <td>0.689642</td>\n",
              "      <td>0.995921</td>\n",
              "      <td>0.999603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>54.858300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.330097</td>\n",
              "      <td>20.869565</td>\n",
              "      <td>12.750000</td>\n",
              "      <td>0.760563</td>\n",
              "      <td>133.037197</td>\n",
              "      <td>0.696078</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>9_1</td>\n",
              "      <td>0.554712</td>\n",
              "      <td>0.987751</td>\n",
              "      <td>0.953031</td>\n",
              "      <td>NaN</td>\n",
              "      <td>29.634002</td>\n",
              "      <td>12.590164</td>\n",
              "      <td>11.016393</td>\n",
              "      <td>52.500000</td>\n",
              "      <td>8.857143</td>\n",
              "      <td>0.762376</td>\n",
              "      <td>129.376739</td>\n",
              "      <td>0.543011</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    code_doc        SR       SNR  ...    syll_100  different_words  topic\n",
              "0      100_1  0.506749  0.975847  ...  138.665032         0.608696   0.75\n",
              "1      107_7  0.875052  0.992874  ...  137.962893         0.470588   1.00\n",
              "2      108_3  0.610753  0.950284  ...  145.640040         0.696970   0.75\n",
              "3      128_5  0.636332  0.998066  ...  129.190227         0.722581   0.75\n",
              "4       13_2  0.784851  0.995663  ...  145.396761         0.767677   0.50\n",
              "..       ...       ...       ...  ...         ...              ...    ...\n",
              "295     96_6  0.761384  0.900388  ...  177.835900         0.746032   0.75\n",
              "296     97_6  0.745310  0.962507  ...  152.829136         0.705036   1.00\n",
              "297    98_13  0.661133  0.971335  ...  154.891601         0.762887   0.75\n",
              "298    99_11  0.689642  0.995921  ...  133.037197         0.696078   1.00\n",
              "299      9_1  0.554712  0.987751  ...  129.376739         0.543011   0.75\n",
              "\n",
              "[300 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7KuxC3ShGp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbdjqKLu6Ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "bcb71b34-80eb-494a-b4ac-1a4a6220797b"
      },
      "source": [
        "# Create a training dataset with features and labels\n",
        "def create_model_dataset(labels,features):\n",
        "  \"\"\"\n",
        "  # Create an aggregation of features (x set) and labels (y set) to provide \n",
        "  # data for learning model. Label are the last columns of the dataframe and\n",
        "  # keep the code_doc for each tuple.\n",
        "  Parameters:\n",
        "      :param labels: DataFrame with annotation of each annotator for each\n",
        "      document \n",
        "      :param features: DataFrame with all features, created with \n",
        "       create_features_dataframe(..) function.\n",
        "      :type labels: DataFrame (pandas) \n",
        "      :type features: DataFrame (pandas)\n",
        "  \n",
        "  Returns:\n",
        "      model_dataset: Dataframe with the features and the labels in the same\n",
        "      document. Labels are last column and first column is the code_doc\n",
        "      type : DataFrame (pandas)\n",
        "      \n",
        "  :Example:\n",
        "\n",
        "      >>> print(create_model_dataset(...,...))\n",
        " code_doc        SR       SNR  ...  different_words  topic    labels\n",
        "0      100_1  0.506749  0.975847  ...         0.608696   0.75 -0.187638\n",
        "1      100_1  0.506749  0.975847  ...         0.608696   0.75  0.414244\n",
        "        ... \n",
        "  \"\"\"\n",
        "  dataset = labels\n",
        "  dataset[\"labels\"] = dataset[dataset.columns[2:]].mean(axis=1)\n",
        "  labels = pd.merge(dataset[\"code_doc\"],dataset[\"labels\"], right_index=True, left_index=True)\n",
        "  labels[\"code_doc\"] = labels[\"code_doc\"].map(lambda x : x[:-6])\n",
        "  model_dataset = pd.merge(features, labels, on=\"code_doc\")\n",
        "  return model_dataset\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "features = create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\")\n",
        "\n",
        "\n",
        "\n",
        "#print(create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "print(model_dataset)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    code_doc        SR       SNR  ...  different_words  topic    labels\n",
            "0      100_1  0.506749  0.975847  ...         0.608696   0.75 -0.187638\n",
            "1      100_1  0.506749  0.975847  ...         0.608696   0.75  0.414244\n",
            "2      100_1  0.506749  0.975847  ...         0.608696   0.75 -0.269893\n",
            "3      100_1  0.506749  0.975847  ...         0.608696   0.75  0.663152\n",
            "4      107_7  0.875052  0.992874  ...         0.470588   1.00  0.542397\n",
            "..       ...       ...       ...  ...              ...    ...       ...\n",
            "236     89_2  0.663904  0.999360  ...         0.765957   0.50 -0.633253\n",
            "237     89_2  0.663904  0.999360  ...         0.765957   0.50 -0.653581\n",
            "238     89_2  0.663904  0.999360  ...         0.765957   0.50  0.454524\n",
            "239     89_2  0.663904  0.999360  ...         0.765957   0.50  0.154798\n",
            "240     89_2  0.663904  0.999360  ...         0.765957   0.50  0.491719\n",
            "\n",
            "[241 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT_9tLvfC2xO",
        "colab_type": "code",
        "outputId": "537af062-ddf2-4921-f27a-333f902bbb66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "\n",
        "dataset = get_dataset(updated_csv)\n",
        "dataset[\"code_doc\"] = dataset[\"code_doc\"].map(lambda x :x[:-6] )\n",
        "dataset"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>code_doc</th>\n",
              "      <th>il08_09</th>\n",
              "      <th>vg04_05</th>\n",
              "      <th>fd03_04</th>\n",
              "      <th>la09_10</th>\n",
              "      <th>cg13_14</th>\n",
              "      <th>ja05_06</th>\n",
              "      <th>fj11_12</th>\n",
              "      <th>ec20_11</th>\n",
              "      <th>mb00_12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>57_6</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>57_6</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>57_6</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>88.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>57_6</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>57_6</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>77</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1430</th>\n",
              "      <td>1431</td>\n",
              "      <td>256_1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>1432</td>\n",
              "      <td>256_1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>67.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1432</th>\n",
              "      <td>1433</td>\n",
              "      <td>256_1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433</th>\n",
              "      <td>1434</td>\n",
              "      <td>256_1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>50</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>1435</td>\n",
              "      <td>256_1</td>\n",
              "      <td>54</td>\n",
              "      <td>80</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>241 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 code_doc  il08_09  vg04_05  ...  ja05_06  fj11_12  ec20_11  mb00_12\n",
              "5              6     57_6       -1       -1  ...     -1.0    100.0     -1.0       76\n",
              "6              7     57_6      100      100  ...     -1.0     -1.0     -1.0       -1\n",
              "7              8     57_6       -1       -1  ...     -1.0     -1.0     -1.0       -1\n",
              "8              9     57_6       -1       -1  ...     -1.0     -1.0     -1.0       -1\n",
              "9             10     57_6       -1       -1  ...    100.0     -1.0    100.0       -1\n",
              "...          ...      ...      ...      ...  ...      ...      ...      ...      ...\n",
              "1430        1431    256_1       -1       -1  ...     -1.0     -1.0     78.0       -1\n",
              "1431        1432    256_1       -1       -1  ...     -1.0     52.0     -1.0       58\n",
              "1432        1433    256_1       -1       -1  ...     50.0     -1.0     -1.0       -1\n",
              "1433        1434    256_1       -1       -1  ...     -1.0     -1.0     -1.0       -1\n",
              "1434        1435    256_1       54       80  ...     -1.0     -1.0     -1.0       -1\n",
              "\n",
              "[241 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Om3WgOzF5nR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unnormalise datas to make it human readable\n",
        "def un_norm(in_values, maxi,mini):\n",
        "  \"\"\"\n",
        "  # Change the range of in_values from normalised values ([-3,3]) to the\n",
        "  # initial range of values ([0,100]) to make it human readable and \n",
        "  # understandable\n",
        "  Parameters:\n",
        "      :param in_values : list of values to un-normalise\n",
        "      :param maxi: maximum value of the distribution of normalised data to \n",
        "      compare to\n",
        "      :param mini: minimum value of the distribution of normalised data to \n",
        "      compare to  \n",
        "      :type labels: iterable (list, Dataframe, array...) \n",
        "      :type maxi: float\n",
        "      :type mini: float\n",
        "  \n",
        "  Returns:\n",
        "      values: list of values un-normalised (with the initial range)\n",
        "      type : numpy array\n",
        "      \n",
        "  :Example:\n",
        "\n",
        "      >>> print(un_norm([0.6066652923822403],data_max, data_min))\n",
        "      69.19081818170463\n",
        "  \"\"\"\n",
        "  values = np.array([])\n",
        "  for i,value in enumerate(in_values):\n",
        "    values = np.append(values,100*(value - mini)/(maxi - mini))\n",
        "  return values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2DtVamKPbFV",
        "colab_type": "text"
      },
      "source": [
        "### Kfold validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npjje2ZfPeUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validation function for a model\n",
        "def kfold_valid(model,model_dataset, data_max, data_min, verbose=0 ):\n",
        "   \"\"\"\n",
        "  # Valid model with kfold validation. Take an scikit-learn model with\n",
        "  # fit() and score() method. Score used are R² score from scikit-learn. The \n",
        "  # best score is 1 and can go to -inf. If R²=0, the model is constant and \n",
        "  # always predict the y labels\n",
        "  Parameters:\n",
        "      :param model: learning model from scikit-learn or compatible  \n",
        "      :param model_data: DataFrame containing x and y set ; created with\n",
        "      create_model_dataset(labels,features) function\n",
        "      :param data_max: maximum of the distribution after normalisation ;  \n",
        "      :param data_min: minimum of the distribution after normalisation \n",
        "      :param verbose (default:0): if verbose = 1, print for each model \n",
        "      prediction and ground truth \n",
        "      \n",
        "      :type model: learning model from scikit-learn or compatible  \n",
        "      :type model_data: DataFrame (pandas)\n",
        "      :type data_max: float  \n",
        "      :type data_min: float \n",
        "      :type verbose (default:0): int [0,1]\n",
        "      \n",
        "  Returns:\n",
        "      mean_score : mean score over kfold validation with 10 folds \n",
        "      type : float\n",
        "      \n",
        "  :Example:\n",
        "\n",
        "      >>> model = linear_model.Lasso(alpha=0.1)\n",
        "      >>> lasso_score = kfold_valid(model, model_dataset,data_max,data_min)\n",
        "    \n",
        "    (44, 7) : x_set shape\n",
        "  Final score : -0.24054111866533903\n",
        "  Final score : -0.07246287170463583\n",
        "  Final score : -0.4313794791716872\n",
        "  Final score : 0.17992142432998948\n",
        "  Final score : -0.02666638704382618\n",
        "  Final score : 0.003121865088597131\n",
        "  Final score : -0.9381668682410977\n",
        "  Final score : -0.14343816199947357\n",
        "  Final score : -1.030613322481495\n",
        "  Final score : -0.1075240882471391\n",
        "  Score moyen : -0.28077490081361073\n",
        "  \"\"\"\n",
        "\n",
        "  df_x = model_dataset[model_dataset.columns[1:len(model_dataset.columns)-1]].to_numpy()\n",
        "  #print(df_x)\n",
        "  df_y = model_dataset[model_dataset.columns[len(model_dataset.columns)-1]].to_numpy()\n",
        "  #print(df_y)\n",
        "  nb_split=10\n",
        "  print(df_x.shape)\n",
        "  kf = KFold(n_splits = nb_split, shuffle = True, random_state = 0)\n",
        "  aux = 0\n",
        "  for train_index, test_index in kf.split(df_y):\n",
        "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    \n",
        "    model.fit(df_x[train_index], df_y[train_index])\n",
        " print(get_medium(\"100\",csv_file))\n",
        " Unnamed: 0      code_doc  il08_09  ...  la09_10  cg13_14  mb00_12\n",
        "5              6    57_6_100_1       -1  ...     -1.0     -1.0       76\n",
        "55            56   147_1_100_1       -1  ...     -1.0     -1.0       64\n",
        "135          136   210_3_100_1       70  ...     -1.0     -1.0       -1\n",
        "        ... \n",
        "    score = model.score(df_x[test_index], df_y[test_index])\n",
        "    aux += score\n",
        "    \n",
        "    print(\"Final score : \" +str(score) )\n",
        "    predict = model.predict(df_x[test_index])\n",
        "    if verbose==1:\n",
        "      print(\"Pred = \" + str(un_norm(predict,data_max,data_min)) )\n",
        "      print(\"Ground truth = \" + str(un_norm(df_y[test_index],data_max,data_min)))\n",
        "  print(\"Score moyen : \" + str(float(aux/nb_split)))\n",
        "  return float(aux/nb_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z16ekrqAKmVW",
        "colab_type": "text"
      },
      "source": [
        "## Machine learning models with KFolds (10 folds) \n",
        "* Lasso regression (scikit-learn)\n",
        "* SGD Regressor (scikit-learn)\n",
        "* Gradient Boosting regressor (scikit-learn)\n",
        "* MLP regressor (scikit-learn)\n",
        "* Decision trees (scikit-learn)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoDLvcAJPHFi",
        "colab_type": "text"
      },
      "source": [
        "## Lasso Regression (scikit-learn) : only text medium\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10zaiOOlJwqf",
        "colab_type": "code",
        "outputId": "21869da7-307b-4db2-aa74-0d2f9e4c55dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "text_feat = pd.read_csv(\"Text_Features_6.csv\")\n",
        "text_label = get_medium(\"001\",updated_csv)\n",
        "text_feat = text_feat.rename(columns={\"doc\":\"code_doc\"})\n",
        "# Get label dataset\n",
        "dataset = get_medium(\"001\",get_dataset(updated_csv))\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,text_feat )\n",
        "# Remove Na row : unless error are raised  \n",
        "#print(model_dataset)\n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "\n",
        "model = linear_model.Lasso(alpha=0.1)\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44, 7)\n",
            "Final score : -0.24054111866533903\n",
            "Final score : -0.07246287170463583\n",
            "Final score : -0.4313794791716872\n",
            "Final score : 0.17992142432998948\n",
            "Final score : -0.02666638704382618\n",
            "Final score : 0.003121865088597131\n",
            "Final score : -0.9381668682410977\n",
            "Final score : -0.14343816199947357\n",
            "Final score : -1.030613322481495\n",
            "Final score : -0.1075240882471391\n",
            "Score moyen : -0.28077490081361073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJDIVMExJq3j",
        "colab_type": "text"
      },
      "source": [
        "## Lasso Regression (scikit-learn) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akzr3iATLHZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "# Get label dataset\n",
        "dataset = get_dataset(updated_csv)\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Transform code_doc to match on merge\n",
        "\n",
        "#print(norm_dataset[norm_dataset.columns[2:]].mean(axis=1))\n",
        "#print(create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Remove Na row : unless error are raised  \n",
        "#print(model_dataset)\n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "\n",
        "model = linear_model.Lasso(alpha=0.1)\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c8rGPRRB2zY",
        "colab_type": "text"
      },
      "source": [
        "## SGD Regressor (Text only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKOpFuBk3NNG",
        "colab_type": "code",
        "outputId": "ac9b7640-f4b9-4bac-a0fb-832e6e2a7912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "text_feat = pd.read_csv(\"Text_Features_6.csv\")\n",
        "text_label = get_medium(\"001\",updated_csv)\n",
        "text_feat = text_feat.rename(columns={\"doc\":\"code_doc\"})\n",
        "# Get label dataset\n",
        "dataset = get_medium(\"001\",get_dataset(updated_csv))\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,text_feat )\n",
        "# Remove Na row : unless error are raised  \n",
        "#print(model_dataset)\n",
        "model_dataset = model_dataset.dropna()\n",
        "model=  linear_model.SGDRegressor(max_iter=1000, tol=1e-3, penalty=\"elasticnet\")\n",
        "\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44, 7)\n",
            "Final score : -1.2061966375088907e+34\n",
            "Final score : -9.996074498976965e+34\n",
            "Final score : -1.29432960952284e+32\n",
            "Final score : -4.566923078942728e+34\n",
            "Final score : -2.1288997240242112e+33\n",
            "Final score : -1.258291012947659e+33\n",
            "Final score : -1.2113769733223273e+34\n",
            "Final score : -3.910165985820252e+32\n",
            "Final score : -4.370287640602759e+35\n",
            "Final score : -2.3706475616353005e+31\n",
            "Score moyen : -6.107658227199075e+34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRHYLcnf36Eb",
        "colab_type": "text"
      },
      "source": [
        "## SGD Regressor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aziSaE6U3-EZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c2238d17-0219-400b-9787-2a0b46759421"
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "# Get label dataset\n",
        "dataset = get_dataset(updated_csv)\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Transform code_doc to match on merge\n",
        "#norm_dataset[\"code_doc\"] = norm_dataset[\"code_doc\"].map(lambda x : x[:-6] )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Remove Na row : unless error are raised  \n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "\n",
        "model=  linear_model.SGDRegressor(max_iter=1000, tol=1e-3, penalty=\"elasticnet\")\n",
        "\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(169, 13)\n",
            "Final score : -2.6981625996974104e+26\n",
            "Final score : -4.8514090612365614e+26\n",
            "Final score : -1.213045802008836e+27\n",
            "Final score : -1.3411039490215816e+26\n",
            "Final score : -4.363164544099793e+26\n",
            "Final score : -3.7373799925012115e+26\n",
            "Final score : -3.191883079021392e+25\n",
            "Final score : -6.669819659348315e+25\n",
            "Final score : -1.5691864850870733e+27\n",
            "Final score : -1.9916030362616165e+27\n",
            "Score moyen : -6.571574365396878e+26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzXwwBbE4dxg",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Boosting Regressor (Text only)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z94eTI6sCIAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3bec1fbb-951c-43d6-89ed-aa0ba1799261"
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "text_feat = pd.read_csv(\"Text_Features_6.csv\")\n",
        "text_label = get_medium(\"001\",updated_csv)\n",
        "text_feat = text_feat.rename(columns={\"doc\":\"code_doc\"})\n",
        "# Get label dataset\n",
        "dataset = get_medium(\"001\",get_dataset(updated_csv))\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,text_feat )\n",
        "# Remove Na row : unless error are raised  \n",
        "#print(model_dataset)\n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "model=  GradientBoostingRegressor(loss=\"ls\",learning_rate=0.5,n_estimators=10000)\n",
        "\n",
        "\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44, 7)\n",
            "Final score : 0.5348840178440348\n",
            "Final score : -1.665013543581367\n",
            "Final score : -0.25438407235618077\n",
            "Final score : 0.02823005276987678\n",
            "Final score : 0.12525067765945086\n",
            "Final score : -1.7221673116843714\n",
            "Final score : -2.4733867018960027\n",
            "Final score : -0.5662483546551393\n",
            "Final score : -2.579763659419364\n",
            "Final score : 0.10673551497058975\n",
            "Score moyen : -0.8465863380348473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmUG0Ps7CE1r",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVGJFBcP4dm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "75258a8c-2ba3-4dbd-c1d6-36e719b7c079"
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "# Get label dataset\n",
        "dataset = get_dataset(updated_csv)\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Transform code_doc to match on merge\n",
        "#norm_dataset[\"code_doc\"] = norm_dataset[\"code_doc\"].map(lambda x : x[:-6] )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Remove Na row : unless error are raised  \n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "model=  GradientBoostingRegressor(loss=\"ls\",learning_rate=0.5,n_estimators=10000)\n",
        "\n",
        "\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min,verbose=0)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(169, 13)\n",
            "Final score : 0.6287865457428865\n",
            "Final score : 0.4232271040885648\n",
            "Final score : -0.1730475998874299\n",
            "Final score : 0.22857455135043347\n",
            "Final score : 0.3562818861547171\n",
            "Final score : 0.021027446889456236\n",
            "Final score : 0.11143485256255437\n",
            "Final score : 0.6803629457126272\n",
            "Final score : -1.2512385852593293\n",
            "Final score : -1.3867069048405325\n",
            "Score moyen : -0.03612977574860517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLWKFdzf7RF7",
        "colab_type": "text"
      },
      "source": [
        "## MLP regressor (scikit-learn)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixPYe_857Uko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "70f8cd35-6e01-4d74-a7f9-86a5648368f6"
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "# Get label dataset\n",
        "dataset = get_dataset(updated_csv)\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Transform code_doc to match on merge\n",
        "#norm_dataset[\"code_doc\"] = norm_dataset[\"code_doc\"].map(lambda x : x[:-6] )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Remove Na row : unless error are raised  \n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "model= MLPRegressor()\n",
        "\n",
        "\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(169, 13)\n",
            "Final score : -32.23967481739436\n",
            "Final score : -49.812814632929836\n",
            "Final score : -0.23182362430731263\n",
            "Final score : -0.4273418728389183\n",
            "Final score : -13.2731415250687\n",
            "Final score : -68.65503568874357\n",
            "Final score : -43.85909855379072\n",
            "Final score : -9.737828102269285\n",
            "Final score : -0.46392703697684845\n",
            "Final score : -1.0156454599901457\n",
            "Score moyen : -21.97163313143097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_fnEF6071qV",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBZG_-zQ73Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "# Get label dataset\n",
        "dataset = get_dataset(updated_csv)\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Transform code_doc to match on merge\n",
        "#norm_dataset[\"code_doc\"] = norm_dataset[\"code_doc\"].map(lambda x : x[:-6] )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Remove Na row : unless error are raised  \n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "model=tree.DecisionTreeRegressor()\n",
        "\n",
        "\n",
        "lasso_score = kfold_valid(model, model_dataset,data_max,data_min,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg4mrF18LG6W",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning : Neural network \n",
        "\n",
        "\n",
        "Deep learning model with Keras over Tensorflow ( KFold with 10 folds as it is a small neural network with low number of samples ) :\n",
        "* Dense multilayer neural network with dropout, regularization, early stopping on validation ( optimizer : Adam, loss : MSE, metric : MAE)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLYXSZvlEApL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_network(nb_features):\n",
        "  model = Sequential()\n",
        " # model.add(Conv1D(4, int(nb_features[0]/2),input_shape=(nb_features[1], nb_features[2]), strides=1, padding='valid', dilation_rate=1, activation=None, \n",
        "  #                              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', data_format=\"channels_first\",\n",
        "   #                             kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
        "  model.add(Dense(8, input_shape=(nb_features,)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "  #model.add(GaussianNoise(0.1))\n",
        "  model.add(Dense(4, activation='relu',kernel_initializer='normal'))#,kernel_regularizer=regularizers.l2(0.01)))\n",
        "  \n",
        "  #model.add(Dense(nb_features, activation='relu'))#,kernel_regularizer=regularizers.l2(0.01)))\n",
        "  #model.add(Dropout(0.25))\n",
        "  #model.add(Dense(4, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
        "  #model.add(Dropout(0.25))\n",
        "  #model.add(Dense(2, activation='relu',kernel_regularizer=regularizers.l2(0.01) ))\n",
        "  #model.add(Dropout(0.25))\n",
        "  #model.add(Flatten())\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "  adam = Adam(lr=0.0001)\n",
        "  model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def kfold_valid_keras(model,model_dataset, data_max, data_min, verbose=0,nb_epoch = 100 ):\n",
        "  \"\"\"\n",
        "  # Valid model with kfold validation for keras model. Take an compatible keras\n",
        "  # model with fit() and evaluate() function. Score used are score from \n",
        "  # evaluate. It depends on the loss function  \n",
        "  Parameters:\n",
        "      :param model: learning model from scikit-learn or compatible  \n",
        "      :param model_data: DataFrame containing x and y set ; created with\n",
        "      create_model_dataset(labels,features) function\n",
        "      :param data_max: maximum of the distribution after normalisation ;  \n",
        "      :param data_min: minimum of the distribution after normalisation \n",
        "      :param verbose (default:0): if verbose = 1, print for each model \n",
        "      prediction and ground truth \n",
        "      \n",
        "      :type model: learning model from scikit-learn or compatible  \n",
        "      :type model_data: DataFrame (pandas)\n",
        "      :type data_max: float  \n",
        "      :type data_min: float \n",
        "      :type verbose (default:0): int [0,1]\n",
        "      \n",
        "  Returns:\n",
        "      mean_score : mean score over kfold validation with 10 folds \n",
        "      type : float\n",
        "      \n",
        "  :Example:\n",
        "\n",
        "      >>> model=get_network(len(model_dataset.columns[1:-1]))\n",
        "      >>> mean_score = kfold_valid_keras(model, model_dataset,data_max,data_min)\n",
        "    (44, 7) : x_set shape\n",
        "    ....\n",
        "    Epoch 00011: early stopping\n",
        "    4/4 [==============================] - 0s 257us/step\n",
        "    Final score : [0.41310545802116394, 0.62394779920578]\n",
        "    Epoch 00014: early stopping\n",
        "    4/4 [==============================] - 0s 188us/step\n",
        "    Final score : [1.2279276847839355, 0.9716480374336243]\n",
        "    Score moyen : 0.7768264234066009\n",
        "    83.01624438964666\n",
        "  \"\"\"\n",
        "\n",
        "  df_x = model_dataset[model_dataset.columns[1:len(model_dataset.columns)-1]].to_numpy()\n",
        "  #print(df_x)\n",
        "  df_y = model_dataset[model_dataset.columns[len(model_dataset.columns)-1]].to_numpy()\n",
        "  #print(df_y)\n",
        "  nb_split=10\n",
        "  print(df_x.shape)\n",
        "  kf = KFold(n_splits = nb_split, shuffle = True, random_state = 0)\n",
        "  aux = 0\n",
        "  for train_index, test_index in kf.split(df_y):\n",
        "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    \n",
        "    model.fit(df_x[train_index], df_y[train_index], validation_data=(df_x[test_index], df_y[test_index]),epochs=300\n",
        "            , verbose = 0, callbacks=[EarlyStopping(verbose=1,patience = 10)])\n",
        "    score = model.evaluate(df_x[test_index], df_y[test_index])\n",
        "    aux += score[0]\n",
        "    \n",
        "    print(\"Final score : \" +str(score) )\n",
        "    predict = model.predict(df_x[test_index])\n",
        "    if verbose==1:\n",
        "      print(\"Pred = \" + str(un_norm(predict,data_max,data_min)) )\n",
        "      print(\"Ground truth = \" + str(un_norm(df_y[test_index],data_max,data_min)))\n",
        "  print(\"Score moyen : \" + str(float(aux/nb_split)))\n",
        "  return float(aux/nb_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdpHhYjgEDcV",
        "colab_type": "text"
      },
      "source": [
        "### Dense neural network (Only on text dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdwpR4s5EFuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "fe139c25-fe5d-4fd2-de0c-f2d2f0ee7042"
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "text_feat = pd.read_csv(\"Text_Features_6.csv\")\n",
        "text_label = get_medium(\"001\",updated_csv)\n",
        "text_feat = text_feat.rename(columns={\"doc\":\"code_doc\"})\n",
        "# Get label dataset\n",
        "dataset = get_medium(\"001\",get_dataset(updated_csv))\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,text_feat )\n",
        "# Remove Na row : unless error are raised  \n",
        "#print(model_dataset)\n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "model= get_network(len(model_dataset.columns[1:-1]))\n",
        "\n",
        "\n",
        "mean_score = kfold_valid_keras(model, model_dataset,data_max,data_min)\n",
        "mean_score = 100*(mean_score - data_min)/(data_max - data_min)\n",
        "print(mean_score)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44, 7)\n",
            "Epoch 00011: early stopping\n",
            "5/5 [==============================] - 0s 172us/step\n",
            "Final score : [0.5298326015472412, 0.6647107005119324]\n",
            "Epoch 00011: early stopping\n",
            "5/5 [==============================] - 0s 212us/step\n",
            "Final score : [0.6498591303825378, 0.6705976128578186]\n",
            "Epoch 00014: early stopping\n",
            "5/5 [==============================] - 0s 186us/step\n",
            "Final score : [0.8552005887031555, 0.8309553861618042]\n",
            "Epoch 00011: early stopping\n",
            "5/5 [==============================] - 0s 202us/step\n",
            "Final score : [0.7030379176139832, 0.6600193977355957]\n",
            "Epoch 00024: early stopping\n",
            "4/4 [==============================] - 0s 243us/step\n",
            "Final score : [0.6745328307151794, 0.7115403413772583]\n",
            "Epoch 00022: early stopping\n",
            "4/4 [==============================] - 0s 182us/step\n",
            "Final score : [1.211626410484314, 1.0135376453399658]\n",
            "Epoch 00011: early stopping\n",
            "4/4 [==============================] - 0s 254us/step\n",
            "Final score : [0.4626050889492035, 0.5751653909683228]\n",
            "Epoch 00015: early stopping\n",
            "4/4 [==============================] - 0s 225us/step\n",
            "Final score : [1.0405365228652954, 0.8145256042480469]\n",
            "Epoch 00011: early stopping\n",
            "4/4 [==============================] - 0s 257us/step\n",
            "Final score : [0.41310545802116394, 0.62394779920578]\n",
            "Epoch 00014: early stopping\n",
            "4/4 [==============================] - 0s 188us/step\n",
            "Final score : [1.2279276847839355, 0.9716480374336243]\n",
            "Score moyen : 0.7768264234066009\n",
            "83.01624438964666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMu8jCsoE8Ed",
        "colab_type": "text"
      },
      "source": [
        "###Dense neural network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwOZ70oiLm3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "1006a137-5a82-4f7d-8a94-d7bfdcff1999"
      },
      "source": [
        "\n",
        "updated_csv = pd.read_csv(\"/content/annotations_challenge_sid (2).csv\", sep=\",\" )\n",
        "# Get label dataset\n",
        "dataset = get_dataset(updated_csv)\n",
        "# Normalize dataset\n",
        "norm_dataset,data_max_list, data_min_list = normalisation_annot(dataset)\n",
        "data_max = data_max_list.mean()\n",
        "data_min = data_min_list.mean()\n",
        "# Transform code_doc to match on merge\n",
        "#norm_dataset[\"code_doc\"] = norm_dataset[\"code_doc\"].map(lambda x : x[:-6] )\n",
        "# Create dataset with feature and labels in one DataFrame\n",
        "model_dataset= create_model_dataset(norm_dataset,create_features_dataframe(\"Text_Features_6.csv\",\"silence_rolling_mean_new.csv\",\"feat_break.csv\") )\n",
        "# Remove Na row : unless error are raised  \n",
        "model_dataset = model_dataset.dropna()\n",
        "\n",
        "model=get_network(len(model_dataset.columns[1:-1]))\n",
        "\n",
        "\n",
        "mean_score = kfold_valid_keras(model, model_dataset,data_max,data_min)\n",
        "mean_score = 100*(mean_score - data_min)/(data_max - data_min)\n",
        "print(mean_score)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(169, 13)\n",
            "Epoch 00055: early stopping\n",
            "17/17 [==============================] - 0s 47us/step\n",
            "Final score : [1.070759892463684, 0.8041895627975464]\n",
            "Epoch 00011: early stopping\n",
            "17/17 [==============================] - 0s 55us/step\n",
            "Final score : [0.693495512008667, 0.6785553097724915]\n",
            "Epoch 00016: early stopping\n",
            "17/17 [==============================] - 0s 57us/step\n",
            "Final score : [0.45981287956237793, 0.5578392744064331]\n",
            "Epoch 00052: early stopping\n",
            "17/17 [==============================] - 0s 48us/step\n",
            "Final score : [0.7833932042121887, 0.6511227488517761]\n",
            "Epoch 00011: early stopping\n",
            "17/17 [==============================] - 0s 45us/step\n",
            "Final score : [0.7745643258094788, 0.6715742349624634]\n",
            "Epoch 00014: early stopping\n",
            "17/17 [==============================] - 0s 58us/step\n",
            "Final score : [0.5758650898933411, 0.6369807124137878]\n",
            "Epoch 00011: early stopping\n",
            "17/17 [==============================] - 0s 51us/step\n",
            "Final score : [0.5803644061088562, 0.6343424916267395]\n",
            "Epoch 00027: early stopping\n",
            "17/17 [==============================] - 0s 42us/step\n",
            "Final score : [0.3714321553707123, 0.4815148711204529]\n",
            "Epoch 00014: early stopping\n",
            "17/17 [==============================] - 0s 44us/step\n",
            "Final score : [0.4948594570159912, 0.6087021827697754]\n",
            "Epoch 00011: early stopping\n",
            "16/16 [==============================] - 0s 68us/step\n",
            "Final score : [0.2621060013771057, 0.4012904167175293]\n",
            "Score moyen : 0.6066652923822403\n",
            "69.19081818170463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUJUIsLYFTNa",
        "colab_type": "text"
      },
      "source": [
        "# 2nd approach : statistical modelisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J29ln4qF8he4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}