{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_feature_1-5_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6t6jWihX0aH",
        "colab_type": "text"
      },
      "source": [
        "# Challenge M2 SID : Partie Texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXPU1GuryU-9",
        "colab_type": "text"
      },
      "source": [
        "### 1 - Data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bMGx3icXxPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf challenge-m2-sid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJiWXJkOYRHg",
        "colab_type": "code",
        "outputId": "74b29f97-bb9f-4ca7-d583-daa43446041b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "# OS setup\n",
        "!cat /etc/os-release\n",
        "!apt-get install -qq bc tree sox\n",
        "\n",
        "# Liaison avec les données\n",
        "!git clone \"https://etudiantsid:etudiantsidPW;@gitlab.com/jeromefarinas/challenge-m2-sid.git\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.3 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.3 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/f/file/libmagic-mgc_5.32-2ubuntu0.2_amd64.deb  404  Not Found [IP: 91.189.88.162 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/f/file/libmagic1_5.32-2ubuntu0.2_amd64.deb  404  Not Found [IP: 91.189.88.162 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Cloning into 'challenge-m2-sid'...\n",
            "remote: Enumerating objects: 938, done.\u001b[K\n",
            "remote: Counting objects: 100% (938/938), done.\u001b[K\n",
            "remote: Compressing objects: 100% (930/930), done.\u001b[K\n",
            "remote: Total 938 (delta 5), reused 933 (delta 3)\u001b[K\n",
            "Receiving objects: 100% (938/938), 2.15 GiB | 36.92 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Checking out files: 100% (904/904), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cF5YGYHZVBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile\n",
        "import scipy.signal\n",
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re  \n",
        "from sklearn import preprocessing\n",
        "from google.colab import drive\n",
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "from google.colab import files\n",
        "from sklearn import preprocessing\n",
        "import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "import urllib.request\n",
        "import requests\n",
        "from getpass import getpass\n",
        "import os\n",
        "import spacy\n",
        "import gensim\n",
        "from stop_words import get_stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkizsQouBCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf Project-Archean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWFFTnA370tU",
        "colab_type": "code",
        "outputId": "997660ba-9dea-4dec-cd1d-b309476cbcc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "user = getpass('BitBucket user')\n",
        "password = getpass('BitBucket password')\n",
        "os.environ['GITHUB_AUTH'] = user + ':' + password\n",
        "\n",
        "!git clone https://$GITHUB_AUTH@github.com/vincentnam/Project-Archean.git\n",
        "!cd Project-Archean && git checkout Texte"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BitBucket user··········\n",
            "BitBucket password··········\n",
            "Cloning into 'Project-Archean'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 76 (delta 5), reused 5 (delta 3), pack-reused 67\u001b[K\n",
            "Unpacking objects: 100% (76/76), done.\n",
            "Branch 'Texte' set up to track remote branch 'Texte' from 'origin'.\n",
            "Switched to a new branch 'Texte'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhAwP14z4owp",
        "colab_type": "text"
      },
      "source": [
        "### 2 - Test code for every feature (RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q41-0kxrodo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = pd.read_csv(\"/content/challenge-m2-sid/annotations_challenge_sid.csv\", sep=\"\\t\" )\n",
        "\n",
        "# keep only lines that contains a number different from -1\n",
        "only_commented = []\n",
        "for index, row in csv_file.iterrows():\n",
        "    if row[\"il08_09\"] != -1 or row[\"vg04_05\"] != -1 \\\n",
        "            or row[\"fd03_04\"] != -1 or row[\"la09_10\"] != -1 \\\n",
        "        or row[\"cg13_14\"] != -1 or row[\"mb00_12\"] != -1 :\n",
        "        only_commented.append(row)\n",
        "# Transform it into a DataFrame\n",
        "only_commented = pd.DataFrame(only_commented)\n",
        "\n",
        "\n",
        "\n",
        "# Return a subset of informations limited to a communication medium \n",
        "# (audio : 100 , text : 001, audio and video : 110, audio and text : 101 \n",
        "# audio, video and text : 111)\n",
        "def get_medium(medium, df):\n",
        "    return df[df[\"code_doc\"].str.contains(\".*\"+medium+\"_[0-1]{1}\",regex=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3YJIB4S2V58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compar_anno(metric_fun, medium, only_commented):\n",
        "  list_max=list_max_docid(medium,only_commented)\n",
        "  #error_moy = np.array([])\n",
        "  list_doc = np.array([])\n",
        "  list_pred = np.array([])\n",
        "  list_annot = np.array([])\n",
        "  for doc in list_max:\n",
        "    list_pred = np.append(list_pred, metric_fun[doc])\n",
        "    list_doc = np.append(list_doc,doc)\n",
        "    list_annot = np.append(list_annot,list_max[doc])\n",
        "    #y_pred = metric_fun[doc]\n",
        "    #error = mean_squared_error(annot, y_pred)\n",
        "    #print(str(doc) + \" Annotation : \"+ str(list_max[doc]) + \" / pred :\" + str(y_pred) + \" / Error : \" + mean_squared_error(list_max[doc], y_pred))\n",
        "  print(\"Error(RMSE) : \" + str(np.sqrt(mean_squared_error(list_annot,list_pred))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p8Mvrb87OLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_max_docid(medium, only_commented):\n",
        "  # Get the list of annotated extracts for a medium\n",
        "  medium = get_medium(medium, only_commented)\n",
        "  # Get list of files identifiants\n",
        "  list_file = medium[\"code_doc\"]\n",
        "  # Return the list of couple (doc_id, evaluation max of complexity)\n",
        "  # Return [(doc_id, max(annot)),....]\n",
        "  return {(i[0][:-6]+\".xml\"):i[1:].max() for i in medium[medium.columns[-7:]].values }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHR2jU9vzQGJ",
        "colab_type": "text"
      },
      "source": [
        "### 3 - Getting clean words, sentences, lemmas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gUZgl3qT83pG",
        "colab": {}
      },
      "source": [
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtmX_015i0E3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_text = 'challenge-m2-sid/corpus/text/'\n",
        "List_txt = os.listdir(path_text)\n",
        "List_txt.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mdi9xoTgv4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentences(List_txt, path_text):\n",
        "  dic_docs = {}\n",
        "  for doc in List_txt:\n",
        "    tree = ET.parse(path_text + doc)\n",
        "    root = tree.getroot()\n",
        "    dic_docs[doc] = []\n",
        "    for s in root:\n",
        "      sentence = ''\n",
        "      for w in s:\n",
        "        word = w.text\n",
        "        if (word is not None):\n",
        "          sentence = sentence + word\n",
        "      dic_docs[doc].append(sentence)\n",
        "  return(dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CU5-6O7q3k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_sentences(dic_docs):  \n",
        "  for key in dic_docs.keys() : \n",
        "    list_new = []\n",
        "    for sentence in dic_docs[key]:\n",
        "      sentence = sentence.replace(\"'\", ' ').replace(\"’\", ' ')\n",
        "      sentence = re.sub(\"([^\\s\\w\\-])\", '',sentence)\n",
        "      list_new.append(sentence)\n",
        "    dic_docs[key] = list_new\n",
        "  return(dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvcRbvexwluh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_clean_words(dic_docs, mode = 'All'):\n",
        "  if mode == 'All':\n",
        "    for key in dic_docs.keys() : \n",
        "      list_words = []\n",
        "      for sentence in dic_docs[key]:\n",
        "        for word in sentence.split():\n",
        "            w = word.replace(' ', '')\n",
        "            if len(w) != 0:\n",
        "              list_words.append(w.lower())\n",
        "      dic_docs[key] = list_words\n",
        "  if mode == 'Sentences':\n",
        "    for key in dic_docs.keys() : \n",
        "        list_words = []\n",
        "        for sentence in dic_docs[key]:\n",
        "          list_words_sent = []\n",
        "          for word in sentence.split():\n",
        "              w = word.replace(' ', '')\n",
        "              if len(w) > 1:\n",
        "                list_words_sent.append(w.lower())\n",
        "          list_words.append(list_words_sent)\n",
        "        dic_docs[key] = list_words\n",
        "  return(dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qFaa2xt7LR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('fr_core_news_md')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXZYSjs7Ogj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lemmatize(dic_docs):\n",
        "  stop_words = get_stop_words('fr')\n",
        "  dic_lemma = {}\n",
        "  i = 1\n",
        "  N = len(dic_docs_sent)\n",
        "  for doc in dic_docs_sent.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    list_doc = []\n",
        "    for sentence in dic_docs_sent[doc]:\n",
        "      list_sent = []\n",
        "      filtered_sentence = [w for w in sentence.split() if not w in stop_words] \n",
        "      sentence_clean = ' '.join(w for w in filtered_sentence)\n",
        "      sentence_nlp = nlp(sentence_clean)\n",
        "      for token in sentence_nlp:\n",
        "          list_sent.append(token.lemma_)\n",
        "      list_doc.append(list_sent)\n",
        "    dic_lemma[doc] = list_doc\n",
        "    i = i + 1\n",
        "  return(dic_lemma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45_Zfg7WiDSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_docs = get_clean_words(clean_sentences(get_sentences(List_txt, path_text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwAIgs3n-lkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edb5dd55-1524-4b74-d087-f43b19d30ac7"
      },
      "source": [
        "dic_lemma = get_lemmatize(get_clean_words(clean_sentences(get_sentences(List_txt, path_text)), 'Sentences'))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 300\n",
            "2 / 300\n",
            "3 / 300\n",
            "4 / 300\n",
            "5 / 300\n",
            "6 / 300\n",
            "7 / 300\n",
            "8 / 300\n",
            "9 / 300\n",
            "10 / 300\n",
            "11 / 300\n",
            "12 / 300\n",
            "13 / 300\n",
            "14 / 300\n",
            "15 / 300\n",
            "16 / 300\n",
            "17 / 300\n",
            "18 / 300\n",
            "19 / 300\n",
            "20 / 300\n",
            "21 / 300\n",
            "22 / 300\n",
            "23 / 300\n",
            "24 / 300\n",
            "25 / 300\n",
            "26 / 300\n",
            "27 / 300\n",
            "28 / 300\n",
            "29 / 300\n",
            "30 / 300\n",
            "31 / 300\n",
            "32 / 300\n",
            "33 / 300\n",
            "34 / 300\n",
            "35 / 300\n",
            "36 / 300\n",
            "37 / 300\n",
            "38 / 300\n",
            "39 / 300\n",
            "40 / 300\n",
            "41 / 300\n",
            "42 / 300\n",
            "43 / 300\n",
            "44 / 300\n",
            "45 / 300\n",
            "46 / 300\n",
            "47 / 300\n",
            "48 / 300\n",
            "49 / 300\n",
            "50 / 300\n",
            "51 / 300\n",
            "52 / 300\n",
            "53 / 300\n",
            "54 / 300\n",
            "55 / 300\n",
            "56 / 300\n",
            "57 / 300\n",
            "58 / 300\n",
            "59 / 300\n",
            "60 / 300\n",
            "61 / 300\n",
            "62 / 300\n",
            "63 / 300\n",
            "64 / 300\n",
            "65 / 300\n",
            "66 / 300\n",
            "67 / 300\n",
            "68 / 300\n",
            "69 / 300\n",
            "70 / 300\n",
            "71 / 300\n",
            "72 / 300\n",
            "73 / 300\n",
            "74 / 300\n",
            "75 / 300\n",
            "76 / 300\n",
            "77 / 300\n",
            "78 / 300\n",
            "79 / 300\n",
            "80 / 300\n",
            "81 / 300\n",
            "82 / 300\n",
            "83 / 300\n",
            "84 / 300\n",
            "85 / 300\n",
            "86 / 300\n",
            "87 / 300\n",
            "88 / 300\n",
            "89 / 300\n",
            "90 / 300\n",
            "91 / 300\n",
            "92 / 300\n",
            "93 / 300\n",
            "94 / 300\n",
            "95 / 300\n",
            "96 / 300\n",
            "97 / 300\n",
            "98 / 300\n",
            "99 / 300\n",
            "100 / 300\n",
            "101 / 300\n",
            "102 / 300\n",
            "103 / 300\n",
            "104 / 300\n",
            "105 / 300\n",
            "106 / 300\n",
            "107 / 300\n",
            "108 / 300\n",
            "109 / 300\n",
            "110 / 300\n",
            "111 / 300\n",
            "112 / 300\n",
            "113 / 300\n",
            "114 / 300\n",
            "115 / 300\n",
            "116 / 300\n",
            "117 / 300\n",
            "118 / 300\n",
            "119 / 300\n",
            "120 / 300\n",
            "121 / 300\n",
            "122 / 300\n",
            "123 / 300\n",
            "124 / 300\n",
            "125 / 300\n",
            "126 / 300\n",
            "127 / 300\n",
            "128 / 300\n",
            "129 / 300\n",
            "130 / 300\n",
            "131 / 300\n",
            "132 / 300\n",
            "133 / 300\n",
            "134 / 300\n",
            "135 / 300\n",
            "136 / 300\n",
            "137 / 300\n",
            "138 / 300\n",
            "139 / 300\n",
            "140 / 300\n",
            "141 / 300\n",
            "142 / 300\n",
            "143 / 300\n",
            "144 / 300\n",
            "145 / 300\n",
            "146 / 300\n",
            "147 / 300\n",
            "148 / 300\n",
            "149 / 300\n",
            "150 / 300\n",
            "151 / 300\n",
            "152 / 300\n",
            "153 / 300\n",
            "154 / 300\n",
            "155 / 300\n",
            "156 / 300\n",
            "157 / 300\n",
            "158 / 300\n",
            "159 / 300\n",
            "160 / 300\n",
            "161 / 300\n",
            "162 / 300\n",
            "163 / 300\n",
            "164 / 300\n",
            "165 / 300\n",
            "166 / 300\n",
            "167 / 300\n",
            "168 / 300\n",
            "169 / 300\n",
            "170 / 300\n",
            "171 / 300\n",
            "172 / 300\n",
            "173 / 300\n",
            "174 / 300\n",
            "175 / 300\n",
            "176 / 300\n",
            "177 / 300\n",
            "178 / 300\n",
            "179 / 300\n",
            "180 / 300\n",
            "181 / 300\n",
            "182 / 300\n",
            "183 / 300\n",
            "184 / 300\n",
            "185 / 300\n",
            "186 / 300\n",
            "187 / 300\n",
            "188 / 300\n",
            "189 / 300\n",
            "190 / 300\n",
            "191 / 300\n",
            "192 / 300\n",
            "193 / 300\n",
            "194 / 300\n",
            "195 / 300\n",
            "196 / 300\n",
            "197 / 300\n",
            "198 / 300\n",
            "199 / 300\n",
            "200 / 300\n",
            "201 / 300\n",
            "202 / 300\n",
            "203 / 300\n",
            "204 / 300\n",
            "205 / 300\n",
            "206 / 300\n",
            "207 / 300\n",
            "208 / 300\n",
            "209 / 300\n",
            "210 / 300\n",
            "211 / 300\n",
            "212 / 300\n",
            "213 / 300\n",
            "214 / 300\n",
            "215 / 300\n",
            "216 / 300\n",
            "217 / 300\n",
            "218 / 300\n",
            "219 / 300\n",
            "220 / 300\n",
            "221 / 300\n",
            "222 / 300\n",
            "223 / 300\n",
            "224 / 300\n",
            "225 / 300\n",
            "226 / 300\n",
            "227 / 300\n",
            "228 / 300\n",
            "229 / 300\n",
            "230 / 300\n",
            "231 / 300\n",
            "232 / 300\n",
            "233 / 300\n",
            "234 / 300\n",
            "235 / 300\n",
            "236 / 300\n",
            "237 / 300\n",
            "238 / 300\n",
            "239 / 300\n",
            "240 / 300\n",
            "241 / 300\n",
            "242 / 300\n",
            "243 / 300\n",
            "244 / 300\n",
            "245 / 300\n",
            "246 / 300\n",
            "247 / 300\n",
            "248 / 300\n",
            "249 / 300\n",
            "250 / 300\n",
            "251 / 300\n",
            "252 / 300\n",
            "253 / 300\n",
            "254 / 300\n",
            "255 / 300\n",
            "256 / 300\n",
            "257 / 300\n",
            "258 / 300\n",
            "259 / 300\n",
            "260 / 300\n",
            "261 / 300\n",
            "262 / 300\n",
            "263 / 300\n",
            "264 / 300\n",
            "265 / 300\n",
            "266 / 300\n",
            "267 / 300\n",
            "268 / 300\n",
            "269 / 300\n",
            "270 / 300\n",
            "271 / 300\n",
            "272 / 300\n",
            "273 / 300\n",
            "274 / 300\n",
            "275 / 300\n",
            "276 / 300\n",
            "277 / 300\n",
            "278 / 300\n",
            "279 / 300\n",
            "280 / 300\n",
            "281 / 300\n",
            "282 / 300\n",
            "283 / 300\n",
            "284 / 300\n",
            "285 / 300\n",
            "286 / 300\n",
            "287 / 300\n",
            "288 / 300\n",
            "289 / 300\n",
            "290 / 300\n",
            "291 / 300\n",
            "292 / 300\n",
            "293 / 300\n",
            "294 / 300\n",
            "295 / 300\n",
            "296 / 300\n",
            "297 / 300\n",
            "298 / 300\n",
            "299 / 300\n",
            "300 / 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFaXktLP3Ca1",
        "colab_type": "text"
      },
      "source": [
        "### Feature 1 : number of low frequencie words per video "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hcNgsHHiY4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_excel(file):\n",
        "  dfs = pd.ExcelFile(file)\n",
        "  sh = dfs.sheet_names[0]\n",
        "  df_lex = dfs.parse('Sheet1')\n",
        "  df_lex = df_lex[['ortho', 'lemme', 'cgram', 'freqfilms2', 'nbsyll']]\n",
        "  serie = df_lex['freqfilms2']\n",
        "  normalized_serie=(serie)/max(serie)\n",
        "  df_lex['freqfilms2_norm'] = normalized_serie\n",
        "  return(df_lex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH1p2o6uiy_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lex = read_excel('Project-Archean/Lexique-query.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcH1yHkO8bwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_lex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pba9u33B7Yve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_complexity_doc(doc, df_lex, dic_docs):\n",
        "  cplxty = 0\n",
        "  for word in list(set(dic_docs[doc])):\n",
        "    try : \n",
        "      freq = max(df_lex[df_lex['ortho']==word]['freqlemfilms2_norm'])\n",
        "    except :\n",
        "      if len(word) >= 3:\n",
        "        freq = 0\n",
        "      else : \n",
        "        freq = 1\n",
        "    if freq < 0.0001:\n",
        "      cplxty = cplxty + 1\n",
        "#      print(word)\n",
        "#      print(freq)\n",
        "  cplxty = cplxty/(len(list(set(dic_docs[doc]))))\n",
        "  return(cplxty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_WQRJpCj2fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_cplx(df_lex, dic_docs):\n",
        "  dic_cplx = {}\n",
        "  i = 1\n",
        "  N = len(dic_docs.keys())\n",
        "  for doc in dic_docs.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    dic_cplx[doc] = get_complexity_doc(doc, df_lex, dic_docs)\n",
        "    i = i + 1\n",
        "  return(dic_cplx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlRalZfTjvj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_cplx = get_all_cplx(df_lex, dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLJ1k7tIBWYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ma_cplx = max(dic_cplx.values())\n",
        "mi_cplx = min(dic_cplx.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clTk_3V8kYOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_results(dic, ma, mi):\n",
        "  dic_N = {}\n",
        "  for doc in dic.keys():\n",
        "    score = dic[doc]\n",
        "    score =  100 * (score - mi) / (ma - mi)\n",
        "    dic_N[doc] = score\n",
        "  return(dic_N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KWKZyKhASdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cplx_1_doc(ma, doc, df_lex, dic_docs):\n",
        "  return(normalize_results({doc:get_complexity_doc(doc, df_lex, dic_docs)}, ma)[doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5HMBqsvlC-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_cplx_N = normalize_results(dic_cplx, ma_cplx, mi_cplx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr913odv6-0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_cplx_N, \"001\", only_commented)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10uxGAm93KqG",
        "colab_type": "text"
      },
      "source": [
        "### Feature 2 - 3 : Number of sentences per minute and mean length of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNAby9i09oN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_docs_sent = clean_sentences(get_sentences(List_txt, path_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSgyxSc23jIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_length_sentences_nb(dic_docs_sent, dic_doc_len_video):\n",
        "  dic_len_sentence = {}\n",
        "  dic_nb_sentence =  {}\n",
        "  for doc in dic_docs_sent.keys():\n",
        "    time = dic_doc_len_video[doc]\n",
        "    time_s = time.total_seconds()\n",
        "    nb_s = len(dic_docs_sent[doc])\n",
        "    s_per_min = 60*nb_s/time_s\n",
        "    mean_len_s = np.mean([len(s.split()) for s in dic_docs_sent[doc]])\n",
        "    dic_len_sentence[doc] = mean_len_s\n",
        "    dic_nb_sentence[doc] = s_per_min\n",
        "  return(dic_len_sentence, dic_nb_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFLo754_BP0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_len_video(List_txt, path_text):\n",
        "  dic_doc_len_video = {}\n",
        "  for doc in List_txt:\n",
        "    tree = ET.parse(path_text + doc)\n",
        "    root = tree.getroot()\n",
        "    ma = int(max([root[i].attrib['id'] for i in range(len(root))]))\n",
        "    start = root[0][0].attrib['value'][:8]\n",
        "    end = root[ma-1][-1].attrib['value'][:8]\n",
        "    format_ = '%H:%M:%S'\n",
        "    startDateTime = datetime.datetime.strptime(start, format_)\n",
        "    endDateTime = datetime.datetime.strptime(end, format_)\n",
        "    diff = endDateTime - startDateTime\n",
        "    dic_doc_len_video[doc] = diff\n",
        "  return(dic_doc_len_video)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkgK9wgVDPLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_doc_len_video = get_len_video(List_txt, path_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpxSNs_L5DkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_len_sentence, dic_nb_sentence = get_length_sentences_nb(dic_docs_sent, dic_doc_len_video)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op1v_aBr5cON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ma_nb_s = max(dic_nb_sentence.values())\n",
        "mi_nb_s = min(dic_nb_sentence.values())\n",
        "ma_len_s = max(dic_len_sentence.values())\n",
        "mi_len_s = min(dic_len_sentence.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ie1HnIB73d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_len_sentence_N = normalize_results(dic_len_sentence, ma_len_s, mi_len_s)\n",
        "dic_nb_sentence_N = normalize_results(dic_nb_sentence, ma_nb_s, mi_nb_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjpgrR4C8GWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_len_sentence_N, \"001\", only_commented)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmBOouJx8VmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_nb_sentence_N, \"001\", only_commented)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTMLtBiK-58E",
        "colab_type": "text"
      },
      "source": [
        "### Feature 4 : word repetition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVS4fTpvLfQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_repetition_ratio_doc(doc, dic_docs):\n",
        "  list_words_dif = list(set(dic_docs[doc]))\n",
        "  list_words = dic_docs[doc]\n",
        "  rep = 100*(len(list_words_dif)/len(list_words))\n",
        "  return(rep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGuhlx9Mm4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_rep (dic_docs):\n",
        "  dic_repetition = {}\n",
        "  for doc in dic_docs.keys():\n",
        "    dic_repetition[doc] = get_repetition_ratio_doc(doc, dic_docs)\n",
        "  return(dic_repetition)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHA_Cg02M07F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_repetition = get_all_rep (dic_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymXjTxlAIjKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ma_repet = max(dic_repetition.values())\n",
        "mi_repet = min(dic_repetition.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezQfPVcnIs2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_repetition_N = normalize_results(dic_repetition, ma_repet, mi_repet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYgphZu7M5k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_repetition_N, \"001\", only_commented)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE7clVBBO8PB",
        "colab_type": "text"
      },
      "source": [
        "### Feature 5 : Syllable per second"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOp9PnHPSinY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_time_sentence(List_txt, path_text):\n",
        "  dic_doc_time_sentences = {}\n",
        "  for doc in List_txt:\n",
        "    tree = ET.parse(path_text + doc)\n",
        "    root = tree.getroot()\n",
        "    list_times_sec = []\n",
        "    for child in root : \n",
        "      TS = child[0].attrib['value'][:8]\n",
        "      TE = child[-1].attrib['value'][:8]\n",
        "      format_ = '%H:%M:%S'\n",
        "      startDateTime = datetime.datetime.strptime(TS, format_)\n",
        "      endDateTime = datetime.datetime.strptime(TE, format_)\n",
        "      diff = endDateTime - startDateTime\n",
        "      list_times_sec.append(diff.total_seconds())\n",
        "    dic_doc_time_sentences[doc] = list_times_sec\n",
        "  return(dic_doc_time_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmPZNoJuTrYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_doc_time_sentences =  get_time_sentence(List_txt, path_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g90mW8leTyrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nb_syll_sec (dic_doc_time_sentences, dic_docs, df_lex):\n",
        "  dic_syll_per_sec = {}\n",
        "  i = 1\n",
        "  N = len(dic_docs)\n",
        "  m = np.mean(df_lex['nbsyll'])\n",
        "  list_words = list(set(list(df_lex['ortho'])))\n",
        "  for doc in dic_doc_time_sentences.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    time = 0\n",
        "    syll = 0\n",
        "    time = sum(dic_doc_time_sentences[doc])\n",
        "    syll = sum([df_lex[df_lex['ortho']==w]['nbsyll'] for w in dic_docs[doc] if w in list_words])\n",
        "    syll = syll + sum([m for w in dic_docs[doc] if w not in list_words])\n",
        "    ratio = syll / time \n",
        "    dic_syll_per_sec[doc] = ratio\n",
        "    i = i + 1\n",
        "  return(dic_syll_per_sec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9ClY2y3WmMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_syll_per_sec = nb_syll_sec (dic_doc_time_sentences, dic_docs_sent, df_lex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJFJvf0qXzO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ma_sy = max(dic_syll_per_sec.values())\n",
        "mi_sy = min(dic_syll_per_sec.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKZp3tum2qfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_syll_per_sec_N = normalize_results(dic_syll_per_sec, ma_sy, mi_sy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QKJsTzd2xdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compar_anno(dic_syll_per_sec_N, \"001\", only_commented)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJ7QwFiorOA",
        "colab_type": "text"
      },
      "source": [
        "### Feature 6 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAlOZ2LAxfh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "10e8dc61-f2f1-4063-e6ca-bc5d14330995"
      },
      "source": [
        "! python -m spacy download fr_core_news_md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fr_core_news_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-2.1.0/fr_core_news_md-2.1.0.tar.gz (85.7MB)\n",
            "\u001b[K     |████████████████████████████████| 85.7MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fr-core-news-md\n",
            "  Building wheel for fr-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-md: filename=fr_core_news_md-2.1.0-cp36-none-any.whl size=87463873 sha256=2f9e01ccc1bc1474ddb1a6c0c34e78c72d04b275ff5dc769f61d9859185533b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c3c41522/wheels/7e/91/64/f61e597321455d6e42a76abac5736d919a265c31be451cc1ba\n",
            "Successfully built fr-core-news-md\n",
            "Installing collected packages: fr-core-news-md\n",
            "Successfully installed fr-core-news-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdPVD6bFyzvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "d28d9e75-9c1e-4fc4-fe9f-d28d6a88e321"
      },
      "source": [
        "! pip install stop-words"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop-words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=5059da400773087d6c5e17d41b73c892aeb0acb7fb0ed449983fac27e2fa00fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Z-tsfCosxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_topics (dic_docs_sent_lemma):\n",
        "  i=1\n",
        "  N = len(dic_docs_sent_lemma)\n",
        "  for doc in dic_docs_sent_lemma.keys():\n",
        "    print(str(i) + ' / ' + str(N))\n",
        "    dictionary = gensim.corpora.Dictionary(dic_docs_sent_lemma[doc])\n",
        "    bow_corpus = [dictionary.doc2bow(s) for s in dic_docs_sent_lemma[doc]]\n",
        "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 1, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)\n",
        "    print(lda_model.print_topics())\n",
        "    i = i + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsRjjz7CzRxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f2ae114-efa8-4452-e3bd-a55494faddfb"
      },
      "source": [
        "get_topics (dic_docs_sent_lemma)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 300\n",
            "[(0, '0.033*\"-\" + 0.024*\"tu\" + 0.021*\"pas\" + 0.021*\"t\" + 0.018*\"est\" + 0.015*\"je\" + 0.015*\"de\" + 0.015*\"j\" + 0.015*\"le\" + 0.015*\"c\"')]\n",
            "2 / 300\n",
            "[(0, '0.032*\"ils\" + 0.032*\"sont\" + 0.032*\"patrouille\" + 0.032*\"vous\" + 0.024*\"mais\" + 0.024*\"mes\" + 0.024*\"pas\" + 0.024*\"les\" + 0.024*\"aussi\" + 0.024*\"la\"')]\n",
            "3 / 300\n",
            "[(0, '0.035*\"ben\" + 0.030*\"il\" + 0.030*\"-\" + 0.026*\"que\" + 0.026*\"vous\" + 0.026*\"pas\" + 0.026*\"est\" + 0.022*\"non\" + 0.022*\"là\" + 0.017*\"je\"')]\n",
            "4 / 300\n",
            "[(0, '0.027*\"est\" + 0.021*\"ça\" + 0.021*\"c\" + 0.021*\"je\" + 0.018*\"à\" + 0.018*\"de\" + 0.015*\"bien\" + 0.012*\"-\" + 0.012*\"se\" + 0.012*\"les\"')]\n",
            "5 / 300\n",
            "[(0, '0.044*\"-\" + 0.021*\"je\" + 0.021*\"de\" + 0.018*\"tu\" + 0.015*\"est\" + 0.014*\"à\" + 0.012*\"me\" + 0.012*\"un\" + 0.012*\"mais\" + 0.012*\"l\"')]\n",
            "6 / 300\n",
            "[(0, '0.032*\"vous\" + 0.032*\"de\" + 0.032*\"je\" + 0.025*\"à\" + 0.025*\"que\" + 0.025*\"oui\" + 0.025*\"vais\" + 0.019*\"beaucoup\" + 0.019*\"ce\" + 0.019*\"j\"')]\n",
            "7 / 300\n",
            "[(0, '0.077*\"-\" + 0.037*\"non\" + 0.028*\"oui\" + 0.024*\"en\" + 0.024*\"est\" + 0.020*\"fait\" + 0.020*\"j\" + 0.020*\"ai\" + 0.020*\"merci\" + 0.020*\"on\"')]\n",
            "8 / 300\n",
            "[(0, '0.046*\"-\" + 0.044*\"vous\" + 0.023*\"est\" + 0.019*\"je\" + 0.019*\"de\" + 0.017*\"en\" + 0.017*\"c\" + 0.017*\"ça\" + 0.015*\"et\" + 0.015*\"pas\"')]\n",
            "9 / 300\n",
            "[(0, '0.036*\"vous\" + 0.027*\"le\" + 0.022*\"je\" + 0.018*\"monsieur\" + 0.018*\"directeur\" + 0.018*\"une\" + 0.018*\"oui\" + 0.018*\"de\" + 0.013*\"moi\" + 0.013*\"mais\"')]\n",
            "10 / 300\n",
            "[(0, '0.035*\"je\" + 0.023*\"que\" + 0.023*\"il\" + 0.018*\"un\" + 0.018*\"ça\" + 0.018*\"s\" + 0.018*\"moi\" + 0.018*\"qu\" + 0.018*\"l\" + 0.018*\"va\"')]\n",
            "11 / 300\n",
            "[(0, '0.038*\"de\" + 0.025*\"je\" + 0.021*\"j\" + 0.021*\"la\" + 0.017*\"il\" + 0.017*\"a\" + 0.017*\"le\" + 0.017*\"l\" + 0.017*\"y\" + 0.013*\"mais\"')]\n",
            "12 / 300\n",
            "[(0, '0.041*\"de\" + 0.029*\"a\" + 0.029*\"un\" + 0.024*\"la\" + 0.018*\"fiancé\" + 0.018*\"votre\" + 0.018*\"mon\" + 0.018*\"günter\" + 0.018*\"d\" + 0.018*\"vous\"')]\n",
            "13 / 300\n",
            "[(0, '0.062*\"vous\" + 0.039*\"je\" + 0.028*\"de\" + 0.028*\"-\" + 0.028*\"des\" + 0.022*\"un\" + 0.022*\"que\" + 0.022*\"en\" + 0.022*\"pas\" + 0.022*\"l\"')]\n",
            "14 / 300\n",
            "[(0, '0.033*\"de\" + 0.033*\"à\" + 0.026*\"la\" + 0.018*\"le\" + 0.018*\"un\" + 0.018*\"d\" + 0.015*\"poulain\" + 0.015*\"du\" + 0.015*\"même\" + 0.011*\"une\"')]\n",
            "15 / 300\n",
            "[(0, '0.038*\"l\" + 0.038*\"on\" + 0.031*\"fait\" + 0.031*\"a\" + 0.031*\"pas\" + 0.025*\"à\" + 0.025*\"de\" + 0.025*\"c\" + 0.025*\"est\" + 0.019*\"campana\"')]\n",
            "16 / 300\n",
            "[(0, '0.043*\"-\" + 0.026*\"ce\" + 0.026*\"a\" + 0.026*\"d\" + 0.026*\"toi\" + 0.026*\"m\" + 0.026*\"le\" + 0.017*\"hein\" + 0.017*\"si\" + 0.017*\"on\"')]\n",
            "17 / 300\n",
            "[(0, '0.042*\"des\" + 0.042*\"pommes\" + 0.030*\"tu\" + 0.030*\"à\" + 0.024*\"pas\" + 0.024*\"bien\" + 0.024*\"que\" + 0.018*\"ton\" + 0.018*\"moi\" + 0.018*\"ces\"')]\n",
            "18 / 300\n",
            "[(0, '0.041*\"était\" + 0.034*\"c\" + 0.034*\"le\" + 0.034*\"oui\" + 0.028*\"pour\" + 0.028*\"jacques\" + 0.021*\"que\" + 0.021*\"maladie\" + 0.021*\"une\" + 0.021*\"sa\"')]\n",
            "19 / 300\n",
            "[(0, '0.035*\"de\" + 0.032*\"je\" + 0.025*\"l\" + 0.022*\"que\" + 0.019*\"et\" + 0.016*\"le\" + 0.016*\"les\" + 0.013*\"tous\" + 0.013*\"d\" + 0.013*\"dans\"')]\n",
            "20 / 300\n",
            "[(0, '0.027*\"on\" + 0.018*\"est\" + 0.018*\"c\" + 0.018*\"le\" + 0.017*\"les\" + 0.016*\"-\" + 0.016*\"pas\" + 0.013*\"que\" + 0.013*\"vous\" + 0.013*\"de\"')]\n",
            "21 / 300\n",
            "[(0, '0.054*\"elle\" + 0.054*\"est\" + 0.031*\"un\" + 0.031*\"de\" + 0.031*\"oui\" + 0.031*\"que\" + 0.023*\"crois\" + 0.023*\"ça\" + 0.023*\"pour\" + 0.023*\"je\"')]\n",
            "22 / 300\n",
            "[(0, '0.032*\"je\" + 0.032*\"suis\" + 0.032*\"ont\" + 0.032*\"il\" + 0.021*\"désolée\" + 0.021*\"avec\" + 0.021*\"venue\" + 0.021*\"voir\" + 0.021*\"le\" + 0.021*\"sa\"')]\n",
            "23 / 300\n",
            "[(0, '0.052*\"-\" + 0.031*\"vous\" + 0.031*\"attend\" + 0.031*\"général\" + 0.021*\"mien\" + 0.021*\"excusez\" + 0.021*\"mon\" + 0.021*\"le\" + 0.021*\"mari\" + 0.021*\"cuisine\"')]\n",
            "24 / 300\n",
            "[(0, '0.042*\"-\" + 0.036*\"vous\" + 0.030*\"qu\" + 0.030*\"est\" + 0.024*\"on\" + 0.024*\"a\" + 0.024*\"pas\" + 0.018*\"les\" + 0.018*\"à\" + 0.018*\"tous\"')]\n",
            "25 / 300\n",
            "[(0, '0.039*\"de\" + 0.029*\"qu\" + 0.029*\"hôpital\" + 0.029*\"avec\" + 0.029*\"tu\" + 0.029*\"à\" + 0.029*\"des\" + 0.029*\"caporal\" + 0.029*\"l\" + 0.029*\"le\"')]\n",
            "26 / 300\n",
            "[(0, '0.046*\"elle\" + 0.046*\"se\" + 0.037*\"à\" + 0.028*\"pas\" + 0.028*\"est\" + 0.028*\"ne\" + 0.028*\"son\" + 0.028*\"fil\" + 0.028*\"le\" + 0.028*\"si\"')]\n",
            "27 / 300\n",
            "[(0, '0.042*\"-\" + 0.032*\"il\" + 0.027*\"est\" + 0.023*\"mais\" + 0.020*\"ça\" + 0.018*\"je\" + 0.018*\"vous\" + 0.018*\"oui\" + 0.015*\"et\" + 0.015*\"l\"')]\n",
            "28 / 300\n",
            "[(0, '0.034*\"le\" + 0.028*\"est\" + 0.028*\"de\" + 0.023*\"a\" + 0.023*\"elle\" + 0.023*\"la\" + 0.023*\"et\" + 0.023*\"c\" + 0.023*\"que\" + 0.017*\"les\"')]\n",
            "29 / 300\n",
            "[(0, '0.032*\"est\" + 0.028*\"-\" + 0.028*\"c\" + 0.023*\"de\" + 0.023*\"un\" + 0.018*\"pas\" + 0.018*\"le\" + 0.018*\"on\" + 0.018*\"qu\" + 0.018*\"les\"')]\n",
            "30 / 300\n",
            "[(0, '0.037*\"est\" + 0.026*\"la\" + 0.026*\"en\" + 0.021*\"que\" + 0.021*\"c\" + 0.016*\"ai\" + 0.016*\"d\" + 0.016*\"au\" + 0.016*\"à\" + 0.016*\"de\"')]\n",
            "31 / 300\n",
            "[(0, '0.022*\"pas\" + 0.022*\"de\" + 0.022*\"est\" + 0.022*\"le\" + 0.019*\"c\" + 0.015*\"la\" + 0.015*\"tu\" + 0.015*\"ça\" + 0.015*\"au\" + 0.011*\"je\"')]\n",
            "32 / 300\n",
            "[(0, '0.038*\"pas\" + 0.029*\"-\" + 0.025*\"est\" + 0.025*\"je\" + 0.021*\"et\" + 0.021*\"te\" + 0.021*\"tu\" + 0.021*\"c\" + 0.017*\"mais\" + 0.017*\"a\"')]\n",
            "33 / 300\n",
            "[(0, '0.031*\"pas\" + 0.025*\"ne\" + 0.025*\"je\" + 0.025*\"les\" + 0.025*\"ils\" + 0.019*\"j\" + 0.019*\"blancs\" + 0.019*\"de\" + 0.019*\"tu\" + 0.019*\"m\"')]\n",
            "34 / 300\n",
            "[(0, '0.040*\"anatole\" + 0.040*\"je\" + 0.040*\"la\" + 0.030*\"pas\" + 0.030*\"chaleur\" + 0.030*\"t\" + 0.030*\"que\" + 0.030*\"avais\" + 0.030*\"à\" + 0.030*\"quelle\"')]\n",
            "35 / 300\n",
            "[(0, '0.031*\"c\" + 0.028*\"est\" + 0.017*\"le\" + 0.017*\"je\" + 0.017*\"de\" + 0.017*\"pas\" + 0.017*\"un\" + 0.017*\"que\" + 0.014*\"t\" + 0.014*\"a\"')]\n",
            "36 / 300\n",
            "[(0, '0.078*\"monsieur\" + 0.059*\"de\" + 0.039*\"action-réaction\" + 0.039*\"a\" + 0.039*\"il\" + 0.039*\"écrit\" + 0.039*\"par\" + 0.039*\"cachot\" + 0.039*\"au\" + 0.039*\"non\"')]\n",
            "37 / 300\n",
            "[(0, '0.035*\"de\" + 0.031*\"vous\" + 0.021*\"monsieur\" + 0.017*\"à\" + 0.014*\"n\" + 0.014*\"pas\" + 0.014*\"je\" + 0.014*\"maxence\" + 0.014*\"la\" + 0.012*\"qu\"')]\n",
            "38 / 300\n",
            "[(0, '0.044*\"je\" + 0.034*\"vous\" + 0.029*\"pas\" + 0.029*\"et\" + 0.029*\"-\" + 0.024*\"veux\" + 0.019*\"comprendre\" + 0.019*\"est\" + 0.019*\"à\" + 0.019*\"c\"')]\n",
            "39 / 300\n",
            "[(0, '0.027*\"biscotte\" + 0.027*\"la\" + 0.027*\"est\" + 0.027*\"c\" + 0.020*\"un\" + 0.020*\"son\" + 0.020*\"qui\" + 0.020*\"de\" + 0.020*\"bastoche\" + 0.020*\"pour\"')]\n",
            "40 / 300\n",
            "[(0, '0.028*\"pas\" + 0.028*\"est\" + 0.028*\"je\" + 0.028*\"non\" + 0.022*\"pour\" + 0.022*\"mais\" + 0.022*\"de\" + 0.022*\"le\" + 0.022*\"annonce\" + 0.017*\"c\"')]\n",
            "41 / 300\n",
            "[(0, '0.050*\"les\" + 0.038*\"soixante-huit\" + 0.038*\"je\" + 0.038*\"j\" + 0.038*\"filmais\" + 0.038*\"en\" + 0.038*\"et\" + 0.025*\"ils\" + 0.025*\"contre\" + 0.025*\"manifestaient\"')]\n",
            "42 / 300\n",
            "[(0, '0.057*\"de\" + 0.032*\"un\" + 0.025*\"une\" + 0.025*\"le\" + 0.025*\"robinet\" + 0.025*\"l\" + 0.025*\"était\" + 0.025*\"la\" + 0.019*\"comme\" + 0.019*\"ici\"')]\n",
            "43 / 300\n",
            "[(0, '0.041*\"-\" + 0.033*\"ah\" + 0.033*\"je\" + 0.033*\"vous\" + 0.033*\"ce\" + 0.024*\"une\" + 0.024*\"pas\" + 0.024*\"est\" + 0.024*\"ça\" + 0.024*\"n\"')]\n",
            "44 / 300\n",
            "[(0, '0.034*\"de\" + 0.029*\"à\" + 0.029*\"la\" + 0.023*\"fouine\" + 0.023*\"en\" + 0.023*\"vous\" + 0.017*\"non\" + 0.017*\"elle\" + 0.017*\"qu\" + 0.017*\"une\"')]\n",
            "45 / 300\n",
            "[(0, '0.079*\"de\" + 0.053*\"pas\" + 0.053*\"il\" + 0.053*\"y\" + 0.053*\"a\" + 0.053*\"jacques\" + 0.039*\"ne\" + 0.039*\"n\" + 0.039*\"morts\" + 0.039*\"je\"')]\n",
            "46 / 300\n",
            "[(0, '0.027*\"je\" + 0.027*\"a\" + 0.022*\"vous\" + 0.022*\"à\" + 0.022*\"de\" + 0.022*\"ça\" + 0.016*\"votre\" + 0.016*\"mon\" + 0.016*\"est\" + 0.016*\"lui\"')]\n",
            "47 / 300\n",
            "[(0, '0.051*\"des\" + 0.040*\"et\" + 0.040*\"de\" + 0.030*\"je\" + 0.030*\"boulets\" + 0.030*\"avec\" + 0.030*\"un\" + 0.020*\"chercher\" + 0.020*\"recharge\" + 0.020*\"seau\"')]\n",
            "48 / 300\n",
            "[(0, '0.052*\"-\" + 0.031*\"de\" + 0.026*\"un\" + 0.021*\"non\" + 0.021*\"pas\" + 0.021*\"est\" + 0.021*\"me\" + 0.016*\"les\" + 0.016*\"ce\" + 0.016*\"ah\"')]\n",
            "49 / 300\n",
            "[(0, '0.056*\"vous\" + 0.031*\"pas\" + 0.028*\"on\" + 0.024*\"la\" + 0.024*\"c\" + 0.024*\"est\" + 0.024*\"de\" + 0.021*\"je\" + 0.017*\"-\" + 0.017*\"un\"')]\n",
            "50 / 300\n",
            "[(0, '0.042*\"je\" + 0.035*\"le\" + 0.035*\"a\" + 0.028*\"te\" + 0.028*\"tu\" + 0.028*\"personne\" + 0.028*\"dis\" + 0.028*\"pas\" + 0.021*\"me\" + 0.021*\"est\"')]\n",
            "51 / 300\n",
            "[(0, '0.036*\"est\" + 0.031*\"pas\" + 0.027*\"que\" + 0.027*\"de\" + 0.022*\"c\" + 0.018*\"qui\" + 0.018*\"le\" + 0.018*\"si\" + 0.018*\"j\" + 0.013*\"une\"')]\n",
            "52 / 300\n",
            "[(0, '0.032*\"-\" + 0.032*\"c\" + 0.025*\"était\" + 0.025*\"de\" + 0.025*\"je\" + 0.021*\"tu\" + 0.021*\"ça\" + 0.018*\"plan\" + 0.018*\"me\" + 0.014*\"quoi\"')]\n",
            "53 / 300\n",
            "[(0, '0.038*\"la\" + 0.031*\"carte\" + 0.031*\"vous\" + 0.031*\"qui\" + 0.023*\"sur\" + 0.023*\"mademoiselle\" + 0.023*\"silence\" + 0.023*\"accrocher\" + 0.023*\"mais\" + 0.023*\"-\"')]\n",
            "54 / 300\n",
            "[(0, '0.041*\"vous\" + 0.041*\"-\" + 0.035*\"le\" + 0.029*\"on\" + 0.029*\"de\" + 0.023*\"a\" + 0.023*\"sur\" + 0.023*\"pont\" + 0.018*\"un\" + 0.018*\"une\"')]\n",
            "55 / 300\n",
            "[(0, '0.045*\"les\" + 0.028*\"de\" + 0.023*\"on\" + 0.023*\"paris\" + 0.023*\"et\" + 0.023*\"dans\" + 0.017*\"était\" + 0.017*\"la\" + 0.017*\"il\" + 0.017*\"y\"')]\n",
            "56 / 300\n",
            "[(0, '0.061*\"est\" + 0.040*\"et\" + 0.030*\"neuf\" + 0.030*\"cette\" + 0.030*\"que\" + 0.030*\"ici\" + 0.030*\"mathieu\" + 0.030*\"rosalie\" + 0.030*\"dans\" + 0.030*\"qui\"')]\n",
            "57 / 300\n",
            "[(0, '0.050*\"-\" + 0.041*\"je\" + 0.037*\"c\" + 0.033*\"est\" + 0.025*\"de\" + 0.025*\"pas\" + 0.021*\"l\" + 0.021*\"mais\" + 0.017*\"le\" + 0.017*\"vous\"')]\n",
            "58 / 300\n",
            "[(0, '0.030*\"est\" + 0.022*\"-\" + 0.020*\"en\" + 0.020*\"un\" + 0.020*\"je\" + 0.017*\"elle\" + 0.017*\"vous\" + 0.015*\"lui\" + 0.015*\"d\" + 0.015*\"le\"')]\n",
            "59 / 300\n",
            "[(0, '0.043*\"c\" + 0.043*\"-\" + 0.043*\"est\" + 0.026*\"ont\" + 0.026*\"foutu\" + 0.017*\"retrouver\" + 0.017*\"des\" + 0.017*\"tropicale\" + 0.017*\"fille\" + 0.017*\"perrin\"')]\n",
            "60 / 300\n",
            "[(0, '0.064*\"vous\" + 0.029*\"mais\" + 0.029*\"un\" + 0.029*\"que\" + 0.021*\"ça\" + 0.021*\"qu\" + 0.021*\"votre\" + 0.021*\"la\" + 0.021*\"cent\" + 0.021*\"de\"')]\n",
            "61 / 300\n",
            "[(0, '0.032*\"-\" + 0.025*\"est\" + 0.025*\"qu\" + 0.019*\"que\" + 0.019*\"d\" + 0.019*\"me\" + 0.019*\"mais\" + 0.019*\"vous\" + 0.019*\"ce\" + 0.019*\"de\"')]\n",
            "62 / 300\n",
            "[(0, '0.043*\"à\" + 0.032*\"on\" + 0.032*\"de\" + 0.021*\"crois\" + 0.021*\"dire\" + 0.021*\"deux\" + 0.021*\"les\" + 0.021*\"beaucoup\" + 0.021*\"choses\" + 0.021*\"a\"')]\n",
            "63 / 300\n",
            "[(0, '0.031*\"et\" + 0.027*\"était\" + 0.027*\"de\" + 0.022*\"il\" + 0.022*\"mon\" + 0.018*\"c\" + 0.018*\"un\" + 0.018*\"père\" + 0.018*\"l\" + 0.018*\"s\"')]\n",
            "64 / 300\n",
            "[(0, '0.035*\"où\" + 0.035*\"tout\" + 0.026*\"sûr\" + 0.026*\"bien\" + 0.026*\"pas\" + 0.026*\"avec\" + 0.026*\"bingo\" + 0.026*\"se\" + 0.026*\"commence\" + 0.026*\"-\"')]\n",
            "65 / 300\n",
            "[(0, '0.031*\"tu\" + 0.024*\"la\" + 0.021*\"on\" + 0.021*\"est\" + 0.021*\"et\" + 0.017*\"c\" + 0.017*\"l\" + 0.017*\"une\" + 0.017*\"oublie\" + 0.014*\"non\"')]\n",
            "66 / 300\n",
            "[(0, '0.037*\"vous\" + 0.032*\"que\" + 0.032*\"le\" + 0.027*\"-\" + 0.027*\"c\" + 0.027*\"qu\" + 0.027*\"est\" + 0.027*\"on\" + 0.021*\"prépuce\" + 0.021*\"hein\"')]\n",
            "67 / 300\n",
            "[(0, '0.046*\"vous\" + 0.034*\"est\" + 0.031*\"je\" + 0.025*\"c\" + 0.022*\"francs\" + 0.022*\"pas\" + 0.019*\"le\" + 0.015*\"êtes\" + 0.015*\"-\" + 0.012*\"midi\"')]\n",
            "68 / 300\n",
            "[(0, '0.047*\"vous\" + 0.034*\"que\" + 0.034*\"-\" + 0.025*\"le\" + 0.025*\"je\" + 0.016*\"à\" + 0.016*\"coupable\" + 0.016*\"l\" + 0.016*\"ai\" + 0.016*\"m\"')]\n",
            "69 / 300\n",
            "[(0, '0.027*\"-\" + 0.026*\"de\" + 0.022*\"un\" + 0.015*\"vous\" + 0.013*\"je\" + 0.013*\"le\" + 0.013*\"est\" + 0.013*\"que\" + 0.012*\"d\" + 0.011*\"qu\"')]\n",
            "70 / 300\n",
            "[(0, '0.049*\"qui\" + 0.039*\"alors\" + 0.039*\"voilà\" + 0.029*\"se\" + 0.029*\"ah\" + 0.029*\"hein\" + 0.029*\"croc\" + 0.029*\"est-ce\" + 0.029*\"enfants\" + 0.029*\"asseyez-vous\"')]\n",
            "71 / 300\n",
            "[(0, '0.035*\"le\" + 0.027*\"-\" + 0.019*\"mouton\" + 0.019*\"et\" + 0.019*\"est\" + 0.019*\"de\" + 0.015*\"pour\" + 0.015*\"querrec\" + 0.015*\"l\" + 0.015*\"vous\"')]\n",
            "72 / 300\n",
            "[(0, '0.039*\"vous\" + 0.025*\"de\" + 0.022*\"une\" + 0.018*\"c\" + 0.018*\"est\" + 0.018*\"l\" + 0.018*\"a\" + 0.018*\"la\" + 0.014*\"je\" + 0.014*\"il\"')]\n",
            "73 / 300\n",
            "[(0, '0.040*\"-\" + 0.032*\"que\" + 0.032*\"je\" + 0.032*\"j\" + 0.024*\"non\" + 0.024*\"de\" + 0.024*\"tu\" + 0.024*\"ai\" + 0.024*\"voulais\" + 0.024*\"la\"')]\n",
            "74 / 300\n",
            "[(0, '0.024*\"et\" + 0.022*\"un\" + 0.020*\"il\" + 0.020*\"que\" + 0.020*\"je\" + 0.017*\"on\" + 0.017*\"est\" + 0.015*\"à\" + 0.013*\"c\" + 0.013*\"mon\"')]\n",
            "75 / 300\n",
            "[(0, '0.041*\"-\" + 0.030*\"y\" + 0.030*\"il\" + 0.024*\"pyjama\" + 0.024*\"de\" + 0.024*\"en\" + 0.018*\"d\" + 0.018*\"a\" + 0.018*\"pas\" + 0.018*\"moi\"')]\n",
            "76 / 300\n",
            "[(0, '0.028*\"-\" + 0.021*\"mais\" + 0.021*\"vous\" + 0.021*\"est\" + 0.017*\"c\" + 0.017*\"un\" + 0.013*\"quoi\" + 0.013*\"non\" + 0.013*\"pas\" + 0.013*\"on\"')]\n",
            "77 / 300\n",
            "[(0, '0.051*\"les\" + 0.040*\"femmes\" + 0.040*\"des\" + 0.030*\"j\" + 0.030*\"jeunes\" + 0.030*\"un\" + 0.030*\"faire\" + 0.030*\"qui\" + 0.030*\"en\" + 0.020*\"à\"')]\n",
            "78 / 300\n",
            "[(0, '0.055*\"-\" + 0.044*\"est\" + 0.036*\"c\" + 0.025*\"bonjour\" + 0.022*\"ça\" + 0.018*\"un\" + 0.018*\"pas\" + 0.015*\"à\" + 0.015*\"on\" + 0.015*\"la\"')]\n",
            "79 / 300\n",
            "[(0, '0.049*\"vous\" + 0.039*\"oui\" + 0.039*\"de\" + 0.029*\"-\" + 0.029*\"un\" + 0.029*\"lui\" + 0.029*\"peut-être\" + 0.019*\"grave\" + 0.019*\"opérer\" + 0.019*\"ne\"')]\n",
            "80 / 300\n",
            "[(0, '0.055*\"-\" + 0.035*\"tu\" + 0.025*\"qu\" + 0.022*\"t\" + 0.020*\"de\" + 0.017*\"mais\" + 0.017*\"est-ce\" + 0.017*\"a\" + 0.015*\"as\" + 0.015*\"ça\"')]\n",
            "81 / 300\n",
            "[(0, '0.063*\"-\" + 0.044*\"je\" + 0.025*\"c\" + 0.025*\"est\" + 0.025*\"pas\" + 0.025*\"vous\" + 0.025*\"vraiment\" + 0.019*\"promis\" + 0.019*\"à\" + 0.019*\"l\"')]\n",
            "82 / 300\n",
            "[(0, '0.035*\"deux\" + 0.035*\"d\" + 0.035*\"en\" + 0.024*\"pouvant\" + 0.024*\"dans\" + 0.024*\"finement\" + 0.024*\"une\" + 0.024*\"aller\" + 0.024*\"feu\" + 0.024*\"plein\"')]\n",
            "83 / 300\n",
            "[(0, '0.039*\"à\" + 0.039*\"de\" + 0.032*\"la\" + 0.026*\"a\" + 0.019*\"fille\" + 0.019*\"l\" + 0.019*\"piste\" + 0.019*\"tina\" + 0.019*\"lombardi\" + 0.019*\"et\"')]\n",
            "84 / 300\n",
            "[(0, '0.041*\"il\" + 0.036*\"de\" + 0.031*\"je\" + 0.026*\"qu\" + 0.021*\"un\" + 0.021*\"est\" + 0.016*\"métier\" + 0.016*\"ce\" + 0.016*\"vous\" + 0.016*\"musique\"')]\n",
            "85 / 300\n",
            "[(0, '0.039*\"je\" + 0.028*\"il\" + 0.022*\"et\" + 0.022*\"un\" + 0.022*\"qu\" + 0.022*\"est\" + 0.022*\"merci\" + 0.022*\"de\" + 0.017*\"pour\" + 0.017*\"fier\"')]\n",
            "86 / 300\n",
            "[(0, '0.033*\"a\" + 0.033*\"le\" + 0.025*\"elle\" + 0.025*\"était\" + 0.025*\"c\" + 0.025*\"je\" + 0.025*\"vous\" + 0.025*\"une\" + 0.016*\"la\" + 0.016*\"quelqu\"')]\n",
            "87 / 300\n",
            "[(0, '0.032*\"ma\" + 0.026*\"soeur\" + 0.026*\"j\" + 0.026*\"je\" + 0.026*\"ai\" + 0.026*\"vous\" + 0.019*\"a\" + 0.019*\"pas\" + 0.019*\"moi\" + 0.019*\"au\"')]\n",
            "88 / 300\n",
            "[(0, '0.080*\"-\" + 0.046*\"monsieur\" + 0.046*\"en\" + 0.046*\"de\" + 0.046*\"vous\" + 0.034*\"non\" + 0.034*\"ai\" + 0.034*\"bonjour\" + 0.034*\"me\" + 0.034*\"tomates\"')]\n",
            "89 / 300\n",
            "[(0, '0.035*\"c\" + 0.035*\"est\" + 0.031*\"je\" + 0.031*\"-\" + 0.027*\"qu\" + 0.022*\"pour\" + 0.022*\"tout\" + 0.022*\"il\" + 0.022*\"oui\" + 0.022*\"un\"')]\n",
            "90 / 300\n",
            "[(0, '0.044*\"-\" + 0.037*\"cent\" + 0.030*\"mille\" + 0.022*\"casher\" + 0.022*\"est\" + 0.018*\"vous\" + 0.018*\"pour\" + 0.018*\"six\" + 0.018*\"pas\" + 0.015*\"du\"')]\n",
            "91 / 300\n",
            "[(0, '0.029*\"marre\" + 0.024*\"pas\" + 0.019*\"je\" + 0.019*\"en\" + 0.019*\"te\" + 0.019*\"le\" + 0.019*\"j\" + 0.019*\"vous\" + 0.014*\"tu\" + 0.014*\"ai\"')]\n",
            "92 / 300\n",
            "[(0, '0.037*\"est\" + 0.037*\"je\" + 0.025*\"vous\" + 0.025*\"de\" + 0.025*\"il\" + 0.025*\"a\" + 0.025*\"c\" + 0.019*\"ils\" + 0.019*\"avion\" + 0.019*\"ont\"')]\n",
            "93 / 300\n",
            "[(0, '0.035*\"de\" + 0.029*\"est\" + 0.029*\"la\" + 0.023*\"on\" + 0.023*\"des\" + 0.023*\"dans\" + 0.023*\"un\" + 0.017*\"je\" + 0.017*\"première\" + 0.017*\"et\"')]\n",
            "94 / 300\n",
            "[(0, '0.035*\"vous\" + 0.035*\"-\" + 0.033*\"je\" + 0.030*\"pas\" + 0.021*\"est\" + 0.021*\"c\" + 0.016*\"en\" + 0.015*\"non\" + 0.013*\"là\" + 0.013*\"que\"')]\n",
            "95 / 300\n",
            "[(0, '0.039*\"que\" + 0.029*\"est\" + 0.024*\"il\" + 0.024*\"je\" + 0.024*\"c\" + 0.024*\"le\" + 0.024*\"pas\" + 0.019*\"plus\" + 0.019*\"tu\" + 0.019*\"la\"')]\n",
            "96 / 300\n",
            "[(0, '0.043*\"vous\" + 0.026*\"le\" + 0.026*\"-\" + 0.026*\"je\" + 0.026*\"pas\" + 0.017*\"téléphoner\" + 0.017*\"obligez\" + 0.017*\"à\" + 0.017*\"pourtant\" + 0.017*\"alors\"')]\n",
            "97 / 300\n",
            "[(0, '0.060*\"la\" + 0.032*\"pan\" + 0.028*\"fraise\" + 0.028*\"le\" + 0.023*\"que\" + 0.023*\"de\" + 0.023*\"d\" + 0.023*\"je\" + 0.023*\"est\" + 0.023*\"-\"')]\n",
            "98 / 300\n",
            "[(0, '0.024*\"qu\" + 0.022*\"il\" + 0.019*\"est\" + 0.019*\"à\" + 0.019*\"la\" + 0.019*\"vous\" + 0.017*\"-\" + 0.017*\"c\" + 0.014*\"alors\" + 0.014*\"des\"')]\n",
            "99 / 300\n",
            "[(0, '0.043*\"-\" + 0.031*\"et\" + 0.031*\"nez\" + 0.027*\"j\" + 0.023*\"je\" + 0.020*\"le\" + 0.020*\"à\" + 0.020*\"l\" + 0.016*\"m\" + 0.016*\"un\"')]\n",
            "100 / 300\n",
            "[(0, '0.038*\"tu\" + 0.027*\"je\" + 0.027*\"que\" + 0.019*\"pas\" + 0.019*\"j\" + 0.019*\"en\" + 0.019*\"-\" + 0.019*\"ici\" + 0.019*\"t\" + 0.015*\"tes\"')]\n",
            "101 / 300\n",
            "[(0, '0.029*\"que\" + 0.025*\"je\" + 0.023*\"de\" + 0.023*\"pas\" + 0.019*\"une\" + 0.016*\"ne\" + 0.014*\"moucheboume\" + 0.014*\"-\" + 0.014*\"tu\" + 0.012*\"non\"')]\n",
            "102 / 300\n",
            "[(0, '0.051*\"dix\" + 0.041*\"francs\" + 0.041*\"le\" + 0.041*\"et\" + 0.041*\"quatre\" + 0.031*\"voilà\" + 0.031*\"pour\" + 0.031*\"miser\" + 0.031*\"vous\" + 0.031*\"sur\"')]\n",
            "103 / 300\n",
            "[(0, '0.026*\"de\" + 0.022*\"les\" + 0.022*\"et\" + 0.022*\"la\" + 0.019*\"un\" + 0.019*\"plus\" + 0.015*\"oui\" + 0.015*\"ce\" + 0.015*\"qui\" + 0.015*\"siècle\"')]\n",
            "104 / 300\n",
            "[(0, '0.028*\"est\" + 0.028*\"je\" + 0.021*\"pas\" + 0.021*\"c\" + 0.021*\"ça\" + 0.021*\"ne\" + 0.021*\"que\" + 0.017*\"il\" + 0.017*\"mais\" + 0.017*\"de\"')]\n",
            "105 / 300\n",
            "[(0, '0.032*\"les\" + 0.032*\"ˆ\" + 0.032*\"et\" + 0.032*\"des\" + 0.024*\"le\" + 0.024*\"a\" + 0.024*\"nous\" + 0.024*\"dans\" + 0.024*\"de\" + 0.024*\"maman\"')]\n",
            "106 / 300\n",
            "[(0, '0.039*\"vous\" + 0.018*\"hein\" + 0.018*\"pas\" + 0.018*\"je\" + 0.015*\"bien\" + 0.015*\"du\" + 0.015*\"de\" + 0.015*\"le\" + 0.015*\"ça\" + 0.012*\"-\"')]\n",
            "107 / 300\n",
            "[(0, '0.059*\"c\" + 0.059*\"vous\" + 0.046*\"était\" + 0.039*\"pas\" + 0.033*\"est\" + 0.033*\"bien\" + 0.033*\"de\" + 0.033*\"très\" + 0.026*\"n\" + 0.020*\"bon\"')]\n",
            "108 / 300\n",
            "[(0, '0.031*\"de\" + 0.026*\"un\" + 0.026*\"d\" + 0.021*\"la\" + 0.021*\"et\" + 0.021*\"rustique\" + 0.015*\"des\" + 0.015*\"pas\" + 0.015*\"comme\" + 0.015*\"ai\"')]\n",
            "109 / 300\n",
            "[(0, '0.046*\"voyons\" + 0.046*\"pas\" + 0.037*\"-\" + 0.037*\"tu\" + 0.037*\"a\" + 0.028*\"t\" + 0.028*\"puni\" + 0.028*\"c\" + 0.028*\"n\" + 0.028*\"qui\"')]\n",
            "110 / 300\n",
            "[(0, '0.029*\"le\" + 0.026*\"pas\" + 0.019*\"a\" + 0.019*\"c\" + 0.019*\"à\" + 0.019*\"est\" + 0.016*\"t\" + 0.016*\"de\" + 0.016*\"il\" + 0.013*\"on\"')]\n",
            "111 / 300\n",
            "[(0, '0.045*\"vous\" + 0.036*\"un\" + 0.036*\"je\" + 0.027*\"ai\" + 0.027*\"spécialiste\" + 0.027*\"la\" + 0.027*\"avez\" + 0.027*\"les\" + 0.027*\"est\" + 0.027*\"d\"')]\n",
            "112 / 300\n",
            "[(0, '0.049*\"elle\" + 0.024*\"a\" + 0.020*\"de\" + 0.020*\"pas\" + 0.020*\"on\" + 0.020*\"des\" + 0.020*\"et\" + 0.020*\"est\" + 0.016*\"toute\" + 0.016*\"qu\"')]\n",
            "113 / 300\n",
            "[(0, '0.030*\"est\" + 0.030*\"je\" + 0.027*\"-\" + 0.027*\"points\" + 0.027*\"trois\" + 0.024*\"ça\" + 0.024*\"elle\" + 0.024*\"c\" + 0.020*\"mais\" + 0.020*\"bon\"')]\n",
            "114 / 300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-922f3db34373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_topics\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdic_docs_sent_lemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-65-ed73c1efabbd>\u001b[0m in \u001b[0;36mget_topics\u001b[0;34m(dic_docs_sent_lemma)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                    \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                    \u001b[0mpasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                    workers = 2)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training LDA model using %i processes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_e_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mqueue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreallen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36mPool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         return Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[0;32m--> 119\u001b[0;31m                     context=self.get_context())\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRawValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypecode_or_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repopulate_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         self._worker_handler = threading.Thread(\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_repopulate_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PoolWorker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'added worker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh-Foc-BqJk3",
        "colab_type": "text"
      },
      "source": [
        "## 4 - Features agregation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bNPYhIJ3Nno",
        "colab": {}
      },
      "source": [
        "def create_DF_agreg(dic_nb_sentence_N,\n",
        "                    dic_len_sentence_N,\n",
        "                    dic_cplx_N,\n",
        "                    dic_syll_per_sec_N,\n",
        "                    dic_repetition):\n",
        "  col = ['doc', 'nb_sentence', 'len_sentence', 'cplx_words', 'syll_sec','different_words']\n",
        "  list_DF = []\n",
        "  for doc in dic_nb_sentence_N.keys():\n",
        "    list_DF_doc = [doc,\n",
        "                   dic_nb_sentence_N[doc],\n",
        "                   dic_len_sentence_N[doc],\n",
        "                   dic_cplx_N[doc],\n",
        "                   dic_syll_per_sec_N[doc],\n",
        "                   dic_repetition[doc]]\n",
        "    list_DF.append(list_DF_doc)\n",
        "  DF = pd.DataFrame(list_DF, columns=col)\n",
        "  return(DF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k9-NVtsuXLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DF_aggreg = create_DF_agreg(dic_nb_sentence_N,\n",
        "                    dic_len_sentence_N,\n",
        "                    dic_cplx_N,\n",
        "                    dic_syll_per_sec_N,\n",
        "                    dic_repetition_N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qka8vfAubTV",
        "colab_type": "code",
        "outputId": "260fbce2-458a-4865-ad9e-dcd908591722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "source": [
        "DF_aggreg[DF_aggreg['doc'] == '221_7.xml']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>nb_sentence</th>\n",
              "      <th>len_sentence</th>\n",
              "      <th>cplx_words</th>\n",
              "      <th>syll_sec</th>\n",
              "      <th>different_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>221_7.xml</td>\n",
              "      <td>36.94281</td>\n",
              "      <td>29.096045</td>\n",
              "      <td>49.361208</td>\n",
              "      <td>47.567247</td>\n",
              "      <td>12.081019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           doc  nb_sentence  ...   syll_sec  different_words\n",
              "134  221_7.xml     36.94281  ...  47.567247        12.081019\n",
              "\n",
              "[1 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    }
  ]
}